[
  {
    "objectID": "practice.html",
    "href": "practice.html",
    "title": "Practice 1",
    "section": "",
    "text": "Practice 1\n\n\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\nLetsPlot.setup_html(no_js=True)\nplt.style.use(\n    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n)\n\n\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\ndf\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n140\n2020\n1.58\n1.69\n1.66\n1.39\n1.26\n1.14\n1.10\n1.12\n1.19\n1.20\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.32\n\n\n141\n2021\n1.25\n0.95\n1.20\n1.12\n1.04\n1.20\n1.07\n1.02\n1.04\n1.29\n1.29\n1.16\n1.14\n1.14\n1.13\n1.12\n1.10\n1.21\n\n\n142\n2022\n1.24\n1.16\n1.41\n1.08\n1.02\n1.12\n1.06\n1.16\n1.14\n1.31\n1.09\n1.06\n1.15\n1.16\n1.19\n1.17\n1.11\n1.18\n\n\n143\n2023\n1.29\n1.29\n1.63\n1.01\n1.12\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.49\n1.43\n1.21\n1.26\n1.40\n1.84\n\n\n144\n2024\n1.66\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.57\nNaN\nNaN\nNaN\nNaN\nNaN\n1.81\n1.67\n1.46\nNaN\n\n\n\n\n145 rows × 19 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n\n\n\n\n\n\ndf.info()\nna_values=\"***\"\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     144 non-null    float64\n 11  Nov     144 non-null    float64\n 12  Dec     144 non-null    float64\n 13  J-D     144 non-null    float64\n 14  D-N     143 non-null    float64\n 15  DJF     144 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     144 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.7 KB\n\n\n\ndf = df.set_index(\"Year\")\ndf.head()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020\n1.58\n1.69\n1.66\n1.39\n1.26\n1.14\n1.10\n1.12\n1.19\n1.20\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.32\n\n\n2021\n1.25\n0.95\n1.20\n1.12\n1.04\n1.20\n1.07\n1.02\n1.04\n1.29\n1.29\n1.16\n1.14\n1.14\n1.13\n1.12\n1.10\n1.21\n\n\n2022\n1.24\n1.16\n1.41\n1.08\n1.02\n1.12\n1.06\n1.16\n1.14\n1.31\n1.09\n1.06\n1.15\n1.16\n1.19\n1.17\n1.11\n1.18\n\n\n2023\n1.29\n1.29\n1.63\n1.01\n1.12\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.49\n1.43\n1.21\n1.26\n1.40\n1.84\n\n\n2024\n1.66\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.57\nNaN\nNaN\nNaN\nNaN\nNaN\n1.81\n1.67\n1.46\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\ndf[\"Jan\"].plot(ax=ax)\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"Jan\"])\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\nmonth = \"J-D\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\ndf[\"Period\"] = pd.cut(\n    df.index,\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\ndf[\"Period\"].tail(20)\n\nYear\n2005    1981—2010\n2006    1981—2010\n2007    1981—2010\n2008    1981—2010\n2009    1981—2010\n2010    1981—2010\n2011          NaN\n2012          NaN\n2013          NaN\n2014          NaN\n2015          NaN\n2016          NaN\n2017          NaN\n2018          NaN\n2019          NaN\n2020          NaN\n2021          NaN\n2022          NaN\n2023          NaN\n2024          NaN\nName: Period, dtype: category\nCategories (3, object): ['1921—1950' &lt; '1951—1980' &lt; '1981—2010']\n\n\n\nlist_of_months = [\"Jun\", \"Jul\", \"Aug\"]\ndf[list_of_months].stack().head()\n\nYear     \n1880  Jun   -0.18\n      Jul   -0.22\n      Aug   -0.26\n1881  Jun   -0.34\n      Jul    0.09\ndtype: float64\n\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\nfor ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n    ax.set_title(period)\nplt.suptitle(\"Histogram of temperature anomalies\")\naxes[1].set_xlabel(\"Summer temperature distribution\")\nplt.tight_layout();\n\n\n\n\n\n\n\n\n\n# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1951) & (df.index &lt;= 1980), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at this data:\ntemp_all_months\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1951\nJan\n-0.36\n\n\n1\n1951\nFeb\n-0.51\n\n\n2\n1951\nMar\n-0.19\n\n\n3\n1951\nApr\n0.07\n\n\n4\n1951\nMay\n0.17\n\n\n...\n...\n...\n...\n\n\n355\n1980\nAug\n0.09\n\n\n356\n1980\nSep\n0.10\n\n\n357\n1980\nOct\n0.12\n\n\n358\n1980\nNov\n0.20\n\n\n359\n1980\nDec\n0.09\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nquantiles = [0.3, 0.7]\nlist_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n\nprint(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\nprint(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")\n\nThe cold threshold of 30.0% is -0.1\nThe hot threshold of 70.0% is 0.1\n\n\n\n# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1981) & (df.index &lt;= 2010), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at the start of this data data:\ntemp_all_months.head()\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1981\nJan\n0.79\n\n\n1\n1981\nFeb\n0.62\n\n\n2\n1981\nMar\n0.68\n\n\n3\n1981\nApr\n0.39\n\n\n4\n1981\nMay\n0.18\n\n\n\n\n\n\n\n\nentries_less_than_q30 = temp_all_months[\"values\"] &lt; list_of_percentiles[0]\nproportion_under_q30 = entries_less_than_q30.mean()\nprint(\n    f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\"\n)\n\nThe proportion under -0.1 is 1.94%\n\n\n\nproportion_over_q70 = (temp_all_months[\"values\"] &gt; list_of_percentiles[1]).mean()\nprint(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")\n\nThe proportion over 0.1 is 84.72%\n\n\n\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"Season\", 0: \"Values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nSeason\nValues\nPeriod\n\n\n\n\n443\n1991\nDJF\n0.51\n1981—2010\n\n\n444\n1991\nMAM\n0.45\n1981—2010\n\n\n445\n1991\nJJA\n0.42\n1981—2010\n\n\n446\n1991\nSON\n0.32\n1981—2010\n\n\n447\n1992\nDJF\n0.43\n1981—2010\n\n\n448\n1992\nMAM\n0.30\n1981—2010\n\n\n449\n1992\nJJA\n-0.04\n1981—2010\n\n\n450\n1992\nSON\n-0.15\n1981—2010\n\n\n451\n1993\nDJF\n0.37\n1981—2010\n\n\n452\n1993\nMAM\n0.31\n1981—2010\n\n\n\n\n\n\n\n\ngrp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\n    [np.mean, np.var]\n)\ngrp_mean_var\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\nSeason\nPeriod\n\n\n\n\n\n\nDJF\n1921—1950\n-0.027931\n0.057703\n\n\n1951—1980\n-0.003333\n0.050375\n\n\n1981—2010\n0.522000\n0.078644\n\n\nJJA\n1921—1950\n-0.054483\n0.021611\n\n\n1951—1980\n0.001333\n0.014640\n\n\n1981—2010\n0.399000\n0.067775\n\n\nMAM\n1921—1950\n-0.041724\n0.031136\n\n\n1951—1980\n0.000333\n0.025272\n\n\n1981—2010\n0.507667\n0.075812\n\n\nSON\n1921—1950\n0.081379\n0.027798\n\n\n1951—1980\n-0.001333\n0.026384\n\n\n1981—2010\n0.427000\n0.110739\n\n\n\n\n\n\n\n\nmin_year = 1880\n(\n    ggplot(temp_all_months, aes(x=\"Year\", y=\"Values\", color=\"Season\"))\n    + geom_abline(slope=0, color=\"black\", size=1)\n    + geom_line(size=1)\n    + labs(\n        title=f\"Average annual temperature anomaly in \\n in the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})\",\n        y=\"Annual temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n    + geom_text(\n        x=min_year, y=0.1, label=\"1951—1980 average\", hjust=\"left\", color=\"black\"\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n              \n            \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                    1951—1980 average\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1880\n              \n            \n          \n          \n            \n            \n            \n              \n                1900\n              \n            \n          \n          \n            \n            \n            \n              \n                1920\n              \n            \n          \n          \n            \n            \n            \n              \n                1940\n              \n            \n          \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2020\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -1.0\n              \n            \n          \n          \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.5\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n          \n            \n              \n                1.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Average annual temperature anomaly in \n      \n      \n         in the northern hemisphere (1880—2024)\n      \n    \n    \n      \n        Annual temperature anomalies\n      \n    \n    \n      \n        Year\n      \n    \n    \n      \n      \n      \n        \n          \n            Season\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                MAM\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                JJA\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                SON\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                DJF\n              \n            \n          \n        \n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_co2 = pd.read_csv(\"data2.csv\")\ndf_co2.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n1\n1958\n4\n317.45\n317.45\n315.29\n\n\n2\n1958\n5\n317.50\n317.50\n314.71\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n4\n1958\n7\n315.86\n315.86\n314.98\n\n\n\n\n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 6]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n15\n1959\n6\n318.15\n318.15\n315.92\n\n\n27\n1960\n6\n319.59\n319.59\n317.36\n\n\n39\n1961\n6\n319.77\n319.77\n317.48\n\n\n51\n1962\n6\n320.55\n320.55\n318.27\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Jun\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nJun\nTrend\n\n\n\n\n0\n1958\n0.05\n314.85\n\n\n1\n1959\n0.14\n315.92\n\n\n2\n1960\n0.18\n317.36\n\n\n3\n1961\n0.18\n317.48\n\n\n4\n1962\n-0.13\n318.27\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Jun\", y=\"Trend\"))\n    + geom_point(color=\"black\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_temp_co2[[\"Jun\", \"Trend\"]].corr(method=\"pearson\")\n\n\n\n\n\n\n\n\nJun\nTrend\n\n\n\n\nJun\n1.000000\n0.914371\n\n\nTrend\n0.914371\n1.000000\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Year\", y=\"Jun\"))\n    + geom_line(size=1)\n    + labs(\n        title=\"June temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1970\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                1990\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2010\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.6\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n        \n      \n    \n    \n      \n        June temperature anomalies\n      \n    \n    \n      \n        Jun\n      \n    \n    \n      \n        Year\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\nbase_plot = ggplot(df_temp_co2) + scale_x_continuous(format=\"d\")\nplot_p = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Jun\"), size=1)\n    + labs(title=\"June temperature anomalies\")\n)\nplot_q = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Trend\"), size=1)\n    + labs(title=\"Carbon dioxide emissions\")\n)\ngggrid([plot_p, plot_q], ncol=2)\n\n\n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  0.0\n                \n              \n            \n            \n              \n                \n                  0.2\n                \n              \n            \n            \n              \n                \n                  0.4\n                \n              \n            \n            \n              \n                \n                  0.6\n                \n              \n            \n            \n              \n                \n                  0.8\n                \n              \n            \n            \n              \n                \n                  1.0\n                \n              \n            \n          \n        \n      \n      \n        \n          June temperature anomalies\n        \n      \n      \n        \n          Jun\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  320\n                \n              \n            \n            \n              \n                \n                  340\n                \n              \n            \n            \n              \n                \n                  360\n                \n              \n            \n            \n              \n                \n                  380\n                \n              \n            \n            \n              \n                \n                  400\n                \n              \n            \n          \n        \n      \n      \n        \n          Carbon dioxide emissions\n        \n      \n      \n        \n          Trend\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n\n\n\nSource: Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\n\n\nPractice 2-1\n\n\n%pip install openpyxl\n\nCollecting openpyxlNote: you may need to restart the kernel to use updated packages.\n\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\nCollecting et-xmlfile (from openpyxl)\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\nDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n   - -------------------------------------- 10.2/250.9 kB ? eta -:--:--\n   ------ -------------------------------- 41.0/250.9 kB 653.6 kB/s eta 0:00:01\n   ------------------- -------------------- 122.9/250.9 kB 1.2 MB/s eta 0:00:01\n   ---------------------------------------  245.8/250.9 kB 1.5 MB/s eta 0:00:01\n   ---------------------------------------- 250.9/250.9 kB 1.5 MB/s eta 0:00:00\nDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n\n\n\nimport pandas as pd\ndata_np = pd.read_excel(\n    \"doing-economics-datafile-working-in-excel-project-2.xlsx\",\n    usecols=\"A:Q\",\n    header=1,\n    index_col=\"Period\",\n)\ndata_n = data_np.iloc[:10, :].copy()\ndata_p = data_np.iloc[14:24, :].copy()\n\n\ntest_data = {\n    \"City A\": [14.1, 14.1, 13.7],\n    \"City B\": [11.0, 12.6, 12.1],\n}\n\n\n# Original dataframe\ntest_df = pd.DataFrame.from_dict(test_data)\n# A copy of the dataframe\ntest_copy = test_df.copy()\n# A pointer to the dataframe\ntest_pointer = test_df\n\n\ntest_pointer.iloc[1, 1] = 99\n\n\nprint(\"test_df=\")\nprint(f\"{test_df}\\n\")\nprint(\"test_copy=\")\nprint(f\"{test_copy}\\n\")\n\ntest_df=\n   City A  City B\n0    14.1    11.0\n1    14.1    99.0\n2    13.7    12.1\n\ntest_copy=\n   City A  City B\n0    14.1    11.0\n1    14.1    12.6\n2    13.7    12.1\n\n\n\n\ndata_n.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 1 to 10\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Copenhagen       10 non-null     object\n 1   Dnipropetrovs’k  10 non-null     object\n 2   Minsk            10 non-null     object\n 3   St. Gallen       10 non-null     object\n 4   Muscat           10 non-null     object\n 5   Samara           10 non-null     object\n 6   Zurich           10 non-null     object\n 7   Boston           10 non-null     object\n 8   Bonn             10 non-null     object\n 9   Chengdu          10 non-null     object\n 10  Seoul            10 non-null     object\n 11  Riyadh           10 non-null     object\n 12  Nottingham       10 non-null     object\n 13  Athens           10 non-null     object\n 14  Istanbul         10 non-null     object\n 15  Melbourne        10 non-null     object\ndtypes: object(16)\nmemory usage: 1.3+ KB\n\n\n\ndata_n = data_n.astype(\"double\")\ndata_p = data_p.astype(\"double\")\n\n\nimport numpy as np\nmean_n_c = data_n.mean(axis=1)\nmean_p_c = data_p.agg(np.mean, axis=1)\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nmean_n_c.plot(ax=ax, label=\"Without punishment\")\nmean_p_c.plot(ax=ax, label=\"With punishment\")\nax.set_title(\"Average contributions to the public goods game\")\nax.set_ylabel(\"Average contribution\")\nax.legend();\n\n\n\n\n\n\n\n\nquestion:Describe any differences and similarities you see in the mean contribution over time in both experiments answer:From the chart,we can see that the trend of the blue line(the experiment without punishment)goes down obviously during each period.However the trend of orange line(the experiment with punishment)goes up slowly.They start from the same point.But they goes differently.\n\npartial_names_list = [\"F. Kennedy\", \"Lennon\", \"Maynard Keynes\", \"Wayne\"]\n[\"John \" + name for name in partial_names_list]\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n# Create new dataframe with bars in\ncompare_grps = pd.DataFrame(\n    [mean_n_c.loc[[1, 10]], mean_p_c.loc[[1, 10]]],\n    index=[\"Without punishment\", \"With punishment\"],\n)\n# Rename columns to have 'round' in them\ncompare_grps.columns = [\"Round \" + str(i) for i in compare_grps.columns]\n# Swap the column and index variables around with the transpose function, ready for plotting (.T is transpose)\ncompare_grps = compare_grps.T\n# Make a bar chart\ncompare_grps.plot.bar(rot=0);\n\n\n\n\n\n\n\n\nquestion:explain whether this means that the two sets of data are the same. answer:No,although the mean contribution for both experiments was 10.6 in Period 1,but in round 10,the mean contribution is totally different.The data from experiment with punishment is much higher than that from experiment without punishment.\n\nn_c = data_n.agg([\"std\", \"var\", \"mean\"], 1)\nn_c\n\n\n\n\n\n\n\n\nstd\nvar\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n1\n2.020724\n4.083325\n10.578313\n\n\n2\n2.238129\n5.009220\n10.628398\n\n\n3\n2.329569\n5.426891\n10.407079\n\n\n4\n2.068213\n4.277504\n9.813033\n\n\n5\n2.108329\n4.445049\n9.305433\n\n\n6\n2.240881\n5.021549\n8.454844\n\n\n7\n2.136614\n4.565117\n7.837568\n\n\n8\n2.349442\n5.519880\n7.376388\n\n\n9\n2.413845\n5.826645\n6.392985\n\n\n10\n2.187126\n4.783520\n4.383769\n\n\n\n\n\n\n\n\np_c = data_p.agg([\"std\", \"var\", \"mean\"], 1)\n\n\nfig, ax = plt.subplots()\nn_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 standard deviations\n(n_c[\"mean\"] + 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 standard deviations\n(n_c[\"mean\"] - 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_n.columns)):\n    ax.scatter(x=data_n.index, y=data_n.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game without punishment\")\nplt.show();\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\np_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 sd\n(p_c[\"mean\"] + 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 sd\n(p_c[\"mean\"] - 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_p.columns)):\n    ax.scatter(x=data_p.index, y=data_p.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game with punishment\")\nplt.show();\n\n\n\n\n\n\n\n\n\ndata_p.apply(lambda x: x.max() - x.min(), axis=1)\n\nPeriod\n1     10.199675\n2     12.185065\n3     12.689935\n4     12.625000\n5     12.140375\n6     12.827541\n7     13.098931\n8     13.482621\n9     13.496754\n10    11.307360\ndtype: float64\n\n\n\n# A lambda function accepting three inputs, a, b, and c, and calculating the sum of the squares\ntest_function = lambda a, b, c: a**2 + b**2 + c**2\n\n\n# Now we apply the function by handing over (in parenthesis) the following inputs: a=3, b=4 and c=5\ntest_function(3, 4, 5)\n\n50\n\n\n\nrange_function = lambda x: x.max() - x.min()\nrange_p = data_p.apply(range_function, axis=1)\nrange_n = data_n.apply(range_function, axis=1)\n\n\nfig, ax = plt.subplots()\nrange_p.plot(ax=ax, label=\"With punishment\")\nrange_n.plot(ax=ax, label=\"Without punishment\")\nax.set_ylim(0, None)\nax.legend()\nax.set_title(\"Range of contributions to the public goods game\")\nplt.show();\n\n\n\n\n\n\n\n\n\nfuncs_to_apply = [range_function, \"max\", \"min\", \"std\", \"mean\"]\nsumm_p = data_p.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\nsumm_n = data_n.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\n\n\nsumm_n.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n6.14\n14.10\n7.96\n2.02\n10.58\n\n\n10\n7.38\n8.68\n1.30\n2.19\n4.38\n\n\n\n\n\n\n\n\nsumm_p.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n10.20\n16.02\n5.82\n3.21\n10.64\n\n\n10\n11.31\n17.51\n6.20\n3.90\n12.87\n\n\n\n\n\n\n\nquestion:Comment on any similarities and differences in the distributions, both across time and across experiments answer:similarities:the statistics vary in different periods and different experiments differences:in experiment with punishment,the data is always going up,data from petiod 10 is always higher than that in period 1 but in experiment without punishment,the data from petiod 10 is not always higher than that in period 1\n\nimport pingouin as pg\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :])\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.063782\n30\ntwo-sided\n0.949567\n[-2.0, 1.87]\n0.02255\n0.337\n0.050437\n\n\n\n\n\n\n\n\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :], paired=True)\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.149959\n15\ntwo-sided\n0.882795\n[-0.92, 0.8]\n0.02255\n0.258\n0.05082\n\n\n\n\n\n\n\nquestion:What does this p-value tell us about the difference in means in Period 1? answer:This result of the previous one delivers a p-value of 0.9496. This means it is very likely that the assumption that there are no differences in the populations is likely to be true.In the latter one,The p-value becomes smaller as we can attribute more of the differences to the ‘with punishment’ treatment, but the p-value is still very large (0.8828), so we still conclude that the differences in Period 1 are likely to be due to chance.\nSource: Original dataframe\n\n\nPractice 2-2\n\n\n%pip install lets-plot\n\nRequirement already satisfied: lets-plot in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.5.1)\nRequirement already satisfied: pypng in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lets-plot) (0.20220715.0)\nRequirement already satisfied: palettable in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lets-plot) (3.3.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom lets_plot import *\n\n\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom lets_plot import *\n\ndata = {\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 4, 1, 3, 5]\n}\ndf = pd.DataFrame(data)\n\nggplot(df, aes(x='x', y='y')) + geom_line()\n\n   \n   \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom lets_plot import *\n\ndata = {\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 4, 1, 3, 5]\n}\ndf = pd.DataFrame(data)\n\nggplot(df, aes(x='x', y='y')) + geom_bar(stat='identity')\n\n   \n   \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.bar(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\nplt.hist(x, bins=5)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\nplt.boxplot(x)\nplt.show()\n\n\n\n\n\n\n\n\nSource: practice 2_2.ipynb\n\n\nPractice 3-1\n\n\n%pip install pandas-datareader\n\nRequirement already satisfied: pandas-datareader in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.10.0)\nRequirement already satisfied: lxml in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (5.3.0)\nRequirement already satisfied: pandas&gt;=0.23 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (2.2.3)\nRequirement already satisfied: requests&gt;=2.19.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (2.32.3)\nRequirement already satisfied: numpy&gt;=1.23.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (1.26.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2024.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (2024.8.30)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.23-&gt;pandas-datareader) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install BeautifulSoup\n\nCollecting BeautifulSoup\n  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'error'\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install bs4\n\nCollecting bs4\n  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nCollecting beautifulsoup4 (from bs4)\n  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\nCollecting soupsieve&gt;1.2 (from beautifulsoup4-&gt;bs4)\n  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\nDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\nDownloading soupsieve-2.6-py3-none-any.whl (36 kB)\nInstalling collected packages: soupsieve, beautifulsoup4, bs4\nSuccessfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\n\n\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)\n\n\n\n\n\n\n\n\nrownames\nname\nyear\nmonth\nday\nhour\nlat\nlong\nstatus\ncategory\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\n\n0\n1\nAmy\n1975\n6\n27\n0\n27.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n1\n2\nAmy\n1975\n6\n27\n6\n28.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n2\n3\nAmy\n1975\n6\n27\n12\n29.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n3\n4\nAmy\n1975\n6\n27\n18\n30.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n4\n5\nAmy\n1975\n6\n28\n0\n31.5\n-78.8\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n5\n6\nAmy\n1975\n6\n28\n6\n32.4\n-78.7\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n6\n7\nAmy\n1975\n6\n28\n12\n33.3\n-78.0\ntropical depression\nNaN\n25\n1011\nNaN\nNaN\n\n\n7\n8\nAmy\n1975\n6\n28\n18\n34.0\n-77.0\ntropical depression\nNaN\n30\n1006\nNaN\nNaN\n\n\n8\n9\nAmy\n1975\n6\n29\n0\n34.4\n-75.8\ntropical storm\nNaN\n35\n1004\nNaN\nNaN\n\n\n9\n10\nAmy\n1975\n6\n29\n6\n34.0\n-74.8\ntropical storm\nNaN\n40\n1002\nNaN\nNaN\n\n\n\n\n\n\n\n\nurl = \"http://aeturrell.com/research\"\npage = requests.get(url)\npage.text[:300]\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.5.56\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n&lt;meta name=\"author\" content=\"Arthur Turrell\"&gt;\\n'\n\n\n\nsoup = BeautifulSoup(page.text, \"html.parser\")\nprint(soup.prettify()[60000:60500])\n\n       &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=gender pay gap\"&gt;\n            gender pay gap\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=labour\"&gt;\n            labour\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=text analysis\"&gt;\n            text analysis\n           &lt;/a&gt;\n          &lt;/div&gt;\n         &lt;/div&gt;\n         &lt;div class=\"project-details-listing\n\n\n\n# Get all paragraphs\nall_paras = soup.find_all(\"p\")\n# Just show one of the paras\nall_paras[1]\n\n&lt;p&gt;Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" &lt;i&gt;Environment and Planning B: Urban Analytics and City Science&lt;/i&gt; (2024): 23998083241267331. doi: &lt;a href=\"https://doi.org/10.1177/23998083241267331\"&gt;&lt;code&gt;10.1177/23998083241267331&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n\n\nall_paras[1].text\n\n'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331'\n\n\n\nprojects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\nprojects = [x.text.strip() for x in projects]\nprojects[:4]\n\n['Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013',\n 'Haldane, Andrew G., and Arthur E. Turrell. \"Drawing on different disciplines: macroeconomic agent-based models.\" Journal of Evolutionary Economics 29 (2019): 39-66. doi: 10.1007/s00191-018-0557-5']\n\n\n\ndf_list = pd.read_html(\n    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n)\n# Retrieve first and only entry from list of dataframes\ndf = df_list[0]\ndf.head()\n\n\n\n\n\n\n\n\nYears\nHosts\nWinners\nScore\nRunner's-up\nThird place\nScore.1\nFourth place\n\n\n\n\n0\n1930 Details\nUruguay\nUruguay\n4 - 2\nArgentina\nUnited States\n[note 1]\nYugoslavia\n\n\n1\n1934 Details\nItaly\nItaly\n2 - 1\nCzechoslovakia\nGermany\n3 - 2\nAustria\n\n\n2\n1938 Details\nFrance\nItaly\n4 - 2\nHungary\nBrazil\n4 - 2\nSweden\n\n\n3\n1950 Details\nBrazil\nUruguay\n2 - 1\nBrazil\nSweden\n[note 2]\nSpain\n\n\n4\n1954 Details\nSwitzerland\nWest Germany\n3 - 2\nHungary\nAustria\n3 - 1\nUruguay\n\n\n\n\n\n\n\n\n%pip install pdftotext\n\nCollecting pdftotext\n  Using cached pdftotext-2.2.2.tar.gz (113 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nBuilding wheels for collected packages: pdftotext\n  Building wheel for pdftotext (setup.py): started\n  Building wheel for pdftotext (setup.py): finished with status 'error'\n  Running setup.py clean for pdftotext\nFailed to build pdftotext\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n#saving the list as dataframe\n#then converting into .csv file\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n# printing movie details with its rating.\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n##.......##\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\n\n%pip install requests\n\nRequirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install beautifulsoup4\n\nRequirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.12.3)\nRequirement already satisfied: soupsieve&gt;1.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.6)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport requests\n \n# 定义请求的 URL 和 headers\nurl = \"https://movie.douban.com/top250\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n}\n \n# 发送 GET 请求\nresponse = requests.get(url, headers=headers)\nresponse.encoding = 'utf-8'  # 设置编码方式\nhtml_content = response.text  # 获取网页的 HTML 内容\nprint(\"网页内容加载成功！\")\n\n网页内容加载成功！\n\n\n\nfrom bs4 import BeautifulSoup\n \n# 使用 Beautiful Soup 解析 HTML\nsoup = BeautifulSoup(html_content, 'html.parser')\n \n# 提取电影名称、描述、评分和评价人数\nmovies = []\nfor item in soup.find_all('div', class_='item'):\n    title = item.find('span', class_='title').get_text()  # 电影名称\n    description = item.find('span', class_='inq')  # 电影描述\n    rating = item.find('span', class_='rating_num').get_text()  # 评分\n    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n    \n    # 如果没有描述，将其置为空字符串\n    if description:\n        description = description.get_text()\n    else:\n        description = ''\n    \n    movie = {\n        \"title\": title,\n        \"description\": description,\n        \"rating\": rating,\n        \"votes\": votes.replace('人评价', '').strip()\n    }\n    movies.append(movie)\n \nprint(\"数据提取成功！\")\n\n数据提取成功！\n\n\n\nimport csv\n \n# 将数据保存到 CSV 文件\nwith open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = ['title', 'description', 'rating', 'votes']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n \n    writer.writeheader()  # 写入表头\n    for movie in movies:\n        writer.writerow(movie)  # 写入每一行数据\n \nprint(\"数据已成功保存到 douban_top250.csv\")\n\n数据已成功保存到 douban_top250.csv\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n \n# 定义请求的 URL 和 headers\nurl = \"https://movie.douban.com/top250\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n}\n \n# 发送 GET 请求\nresponse = requests.get(url, headers=headers)\nresponse.encoding = 'utf-8'  # 设置编码方式\nhtml_content = response.text  # 获取网页的 HTML 内容\n \n# 使用 Beautiful Soup 解析 HTML\nsoup = BeautifulSoup(html_content, 'html.parser')\n \n# 提取电影名称、描述、评分和评价人数\nmovies = []\nfor item in soup.find_all('div', class_='item'):\n    title = item.find('span', class_='title').get_text()  # 电影名称\n    description = item.find('span', class_='inq')  # 电影描述\n    rating = item.find('span', class_='rating_num').get_text()  # 评分\n    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n    \n    # 如果没有描述，将其置为空字符串\n    if description:\n        description = description.get_text()\n    else:\n        description = ''\n    \n    movie = {\n        \"title\": title,\n        \"description\": description,\n        \"rating\": rating,\n        \"votes\": votes.replace('人评价', '').strip()\n    }\n    movies.append(movie)\n \n# 将数据保存到 CSV 文件\nwith open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = ['title', 'description', 'rating', 'votes']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n \n    writer.writeheader()  # 写入表头\n    for movie in movies:\n        writer.writerow(movie)  # 写入每一行数据\n \nprint(\"数据已成功保存到 douban_top250.csv\")\n\n数据已成功保存到 douban_top250.csv\n\n\nSource: Get all paragraphs\n\n\nPractice 3-2\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\n\n\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)\n\n\n\n\n\n\n\n\nrownames\nname\nyear\nmonth\nday\nhour\nlat\nlong\nstatus\ncategory\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\n\n0\n1\nAmy\n1975\n6\n27\n0\n27.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n1\n2\nAmy\n1975\n6\n27\n6\n28.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n2\n3\nAmy\n1975\n6\n27\n12\n29.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n3\n4\nAmy\n1975\n6\n27\n18\n30.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n4\n5\nAmy\n1975\n6\n28\n0\n31.5\n-78.8\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n5\n6\nAmy\n1975\n6\n28\n6\n32.4\n-78.7\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n6\n7\nAmy\n1975\n6\n28\n12\n33.3\n-78.0\ntropical depression\nNaN\n25\n1011\nNaN\nNaN\n\n\n7\n8\nAmy\n1975\n6\n28\n18\n34.0\n-77.0\ntropical depression\nNaN\n30\n1006\nNaN\nNaN\n\n\n8\n9\nAmy\n1975\n6\n29\n0\n34.4\n-75.8\ntropical storm\nNaN\n35\n1004\nNaN\nNaN\n\n\n9\n10\nAmy\n1975\n6\n29\n6\n34.0\n-74.8\ntropical storm\nNaN\n40\n1002\nNaN\nNaN\n\n\n\n\n\n\n\n\nimport requests\n\nurl = \"https://api.ons.gov.uk/timeseries/JP9Z/dataset/UNEM/data\"\n\n# Get the data from the ONS API:\njson_data = requests.get(url).json()\n\n# Prep the data for a quick plot\ntitle = json_data[\"description\"][\"title\"]\ndf = (\n    pd.DataFrame(pd.json_normalize(json_data[\"months\"]))\n    .assign(\n        date=lambda x: pd.to_datetime(x[\"date\"]),\n        value=lambda x: pd.to_numeric(x[\"value\"]),\n    )\n    .set_index(\"date\")\n)\n\ndf[\"value\"].plot(title=title, ylim=(0, df[\"value\"].max() * 1.2), lw=3.0);\n\n\n---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\nFile d:\\anaconda3\\Lib\\site-packages\\requests\\models.py:974, in Response.json(self, **kwargs)\n    973 try:\n--&gt; 974     return complexjson.loads(self.text, **kwargs)\n    975 except JSONDecodeError as e:\n    976     # Catch JSON-related errors and raise as requests.JSONDecodeError\n    977     # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n\nFile d:\\anaconda3\\Lib\\json\\__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    343 if (cls is None and object_hook is None and\n    344         parse_int is None and parse_float is None and\n    345         parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 346     return _default_decoder.decode(s)\n    347 if cls is None:\n\nFile d:\\anaconda3\\Lib\\json\\decoder.py:337, in JSONDecoder.decode(self, s, _w)\n    333 \"\"\"Return the Python representation of ``s`` (a ``str`` instance\n    334 containing a JSON document).\n    335 \n    336 \"\"\"\n--&gt; 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    338 end = _w(s, end).end()\n\nFile d:\\anaconda3\\Lib\\json\\decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)\n    354 except StopIteration as err:\n--&gt; 355     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    356 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nJSONDecodeError                           Traceback (most recent call last)\nCell In[1], line 6\n      3 url = \"https://api.ons.gov.uk/timeseries/JP9Z/dataset/UNEM/data\"\n      5 # Get the data from the ONS API:\n----&gt; 6 json_data = requests.get(url).json()\n      8 # Prep the data for a quick plot\n      9 title = json_data[\"description\"][\"title\"]\n\nFile d:\\anaconda3\\Lib\\site-packages\\requests\\models.py:978, in Response.json(self, **kwargs)\n    974     return complexjson.loads(self.text, **kwargs)\n    975 except JSONDecodeError as e:\n    976     # Catch JSON-related errors and raise as requests.JSONDecodeError\n    977     # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n--&gt; 978     raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n\n\n\nurl = \"http://aeturrell.com/research\"\npage = requests.get(url)\npage.text[:300]\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.5.56\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n&lt;meta name=\"author\" content=\"Arthur Turrell\"&gt;\\n'\n\n\n\nsoup = BeautifulSoup(page.text, \"html.parser\")\nprint(soup.prettify()[60000:60500])\n\n       &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=gender pay gap\"&gt;\n            gender pay gap\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=labour\"&gt;\n            labour\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=text analysis\"&gt;\n            text analysis\n           &lt;/a&gt;\n          &lt;/div&gt;\n         &lt;/div&gt;\n         &lt;div class=\"project-details-listing\n\n\n\n# Get all paragraphs\nall_paras = soup.find_all(\"p\")\n# Just show one of the paras\nall_paras[1]\n\n&lt;p&gt;Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" &lt;i&gt;Environment and Planning B: Urban Analytics and City Science&lt;/i&gt; (2024): 23998083241267331. doi: &lt;a href=\"https://doi.org/10.1177/23998083241267331\"&gt;&lt;code&gt;10.1177/23998083241267331&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n\n\nall_paras[1].text\n\n'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331'\n\n\n\nprojects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\nprojects = [x.text.strip() for x in projects]\nprojects[:4]\n\n['Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013',\n 'Haldane, Andrew G., and Arthur E. Turrell. \"Drawing on different disciplines: macroeconomic agent-based models.\" Journal of Evolutionary Economics 29 (2019): 39-66. doi: 10.1007/s00191-018-0557-5']\n\n\n\nstart, stop = 0, 50\nroot_url = \"www.codingforeconomists.com/page=\"\ninfo_on_pages = [scraper(root_url + str(i)) for i in range(start, stop)]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 3\n      1 start, stop = 0, 50\n      2 root_url = \"www.codingforeconomists.com/page=\"\n----&gt; 3 info_on_pages = [scraper(root_url + str(i)) for i in range(start, stop)]\n\nNameError: name 'scraper' is not defined\n\n\n\n\ndf_list = pd.read_html(\n    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n)\n# Retrieve first and only entry from list of dataframes\ndf = df_list[0]\ndf.head()\n\n\n\n\n\n\n\n\nYears\nHosts\nWinners\nScore\nRunner's-up\nThird place\nScore.1\nFourth place\n\n\n\n\n0\n1930 Details\nUruguay\nUruguay\n4 - 2\nArgentina\nUnited States\n[note 1]\nYugoslavia\n\n\n1\n1934 Details\nItaly\nItaly\n2 - 1\nCzechoslovakia\nGermany\n3 - 2\nAustria\n\n\n2\n1938 Details\nFrance\nItaly\n4 - 2\nHungary\nBrazil\n4 - 2\nSweden\n\n\n3\n1950 Details\nBrazil\nUruguay\n2 - 1\nBrazil\nSweden\n[note 2]\nSpain\n\n\n4\n1954 Details\nSwitzerland\nWest Germany\n3 - 2\nHungary\nAustria\n3 - 1\nUruguay\n\n\n\n\n\n\n\n\nimport pdftotext\nfrom pathlib import Path\n\n# Download the pdf_with_table.pdf file from\n# https://github.com/aeturrell/coding-for-economists/blob/main/data/pdf_with_table.pdf\n# and put it in a subfolder called data before running the next line\n\n# Load the PDF\nwith open(Path(\"data/pdf_with_table.pdf\"), \"rb\") as f:\n    pdf = pdftotext.PDF(f)\n\n# Read all the text into one string; print a chunk of the string\nprint(\"\\n\\n\".join(pdf)[:220])\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 import pdftotext\n      2 from pathlib import Path\n      4 # Download the pdf_with_table.pdf file from\n      5 # https://github.com/aeturrell/coding-for-economists/blob/main/data/pdf_with_table.pdf\n      6 # and put it in a subfolder called data before running the next line\n      7 \n      8 # Load the PDF\n\nModuleNotFoundError: No module named 'pdftotext'\n\n\n\n\n%pip install pdftotext\n\nCollecting pdftotextNote: you may need to restart the kernel to use updated packages.\n\n  Downloading pdftotext-2.2.2.tar.gz (113 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nBuilding wheels for collected packages: pdftotext\n  Building wheel for pdftotext (setup.py): started\n  Building wheel for pdftotext (setup.py): finished with status 'error'\n  Running setup.py clean for pdftotext\nFailed to build pdftotext\n\n\nSource: Get the data from the ONS API:\n\n\nPractice 3-3\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n#saving the list as dataframe\n#then converting into .csv file\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n# printing movie details with its rating.\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n##.......##\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\nSource: Downloading imdb top 250 movie's data\n\n\nPractice 4\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimdb_data = pd.read_csv('IMDB_Top250.csv')  # Replace with actual file path\ndouban_data = pd.read_csv('douban_top250.csv')  # Replace with actual file path\n\n\nfrom bs4 import BeautifulSoup\nimport re\nimport urllib.request, urllib.error  # for URL requests\nimport csv  # for saving as CSV\n\n\n# Regular expressions to extract information\nfindLink = re.compile(r'&lt;a href=\"(.*?)\"&gt;')  # detail link\nfindImgSrc = re.compile(r'&lt;img.*src=\"(.*?)\"', re.S)  # image link\nfindTitle = re.compile(r'&lt;span class=\"title\"&gt;(.*)&lt;/span&gt;')  # movie title\nfindRating = re.compile(r'&lt;span class=\"rating_num\" property=\"v:average\"&gt;(.*)&lt;/span&gt;')  # rating\nfindJudge = re.compile(r'&lt;span&gt;(\\d*)人评价&lt;/span&gt;')  # number of reviews\nfindInq = re.compile(r'&lt;span class=\"inq\"&gt;(.*)&lt;/span&gt;')  # summary\nfindBd = re.compile(r'&lt;p class=\"\"&gt;(.*?)&lt;/p&gt;', re.S)  # additional info\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load datasets\ndouban_file_path = 'douban_top250.csv'  \nimdb_file_path = 'IMDB_Top250.csv'      \n\ndouban_data = pd.read_csv(douban_file_path, encoding='utf-8', on_bad_lines='skip')\nimdb_data = pd.read_csv(imdb_file_path, encoding='utf-8', on_bad_lines='skip')\n\n# Renaming columns for clarity and merging compatibility\ndouban_data.rename(columns={\n    '影片中文名': 'Title',\n    '评分': 'Douban_Score',\n    '评价数': 'Douban_Reviews',\n    '相关信息': 'Douban_Info'\n}, inplace=True)\n\n\nimdb_data.rename(columns={\n    'Name': 'Title',\n    'Year': 'Release_Year',\n    'IMDB Ranking': 'IMDB_Score',\n    'Genre': 'IMDB_Genre',\n    'Director': 'IMDB_Director'\n}, inplace=True)\n\n\n# Calculate average scores for both platforms\ndouban_avg_score = douban_data['Douban_Score'].mean()\nimdb_avg_score = imdb_data['IMDB_Score'].mean()\n\n# Find overlapping movies by title\noverlap_movies = pd.merge(douban_data, imdb_data, on='Title')\n\n# Visualize average scores\nplt.figure(figsize=(8, 5))\nplt.bar(['Douban', 'IMDb'], [douban_avg_score, imdb_avg_score], alpha=0.7)\nplt.title('Average Scores: Douban vs IMDb')\nplt.ylabel('Average Score')\nplt.show()\n\n# Analyze release year distribution\nplt.figure(figsize=(10, 5))\ndouban_data['Douban_Info'] = douban_data['Douban_Info'].astype(str)\ndouban_years = douban_data['Douban_Info'].str.extract(r'(\\d{4})').dropna()\ndouban_years = douban_years[0].astype(int).value_counts().sort_index()\n\nimdb_years = imdb_data['Release_Year'].value_counts().sort_index()\n\ndouban_years.plot(kind='bar', alpha=0.7, label='Douban', figsize=(10, 5))\nimdb_years.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Release Year Distribution')\nplt.xlabel('Year')\nplt.ylabel('Number of Movies')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Analyze genre distribution\nimdb_genres = imdb_data['IMDB_Genre'].str.split(',').explode().str.strip().value_counts()\nplt.figure(figsize=(10, 5))\nimdb_genres.head(10).plot(kind='bar', alpha=0.7, color='orange')\nplt.title('Top 10 IMDb Genres')\nplt.xlabel('Genre')\nplt.ylabel('Count')\nplt.show()\n\n# Top directors by movie count\ndouban_directors = douban_data['Douban_Info'].str.extract(r'导演: (.+?) ').dropna()\ndouban_top_directors = douban_directors[0].value_counts().head(10)\n\nimdb_top_directors = imdb_data['IMDB_Director'].value_counts().head(10)\n\nplt.figure(figsize=(10, 5))\ndouban_top_directors.plot(kind='bar', alpha=0.7, label='Douban', color='blue')\nplt.title('Top 10 Douban Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\nplt.figure(figsize=(10, 5))\nimdb_top_directors.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Top 10 IMDb Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\n# Save overlapping movies to a CSV file\noverlap_movies.to_csv('overlap_movies.csv', index=False)\n\n# Print results\nprint(f\"豆瓣平均评分: {douban_avg_score}\")\nprint(f\"IMDb平均评分: {imdb_avg_score}\")\nprint(f\"重叠电影数量: {len(overlap_movies)}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n豆瓣平均评分: 8.9396\nIMDb平均评分: 8.254\n重叠电影数量: 0\n\n\nSource: Regular expressions to extract information"
  },
  {
    "objectID": "practice/practice 3_3.html",
    "href": "practice/practice 3_3.html",
    "title": "",
    "section": "",
    "text": "from bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n#saving the list as dataframe\n#then converting into .csv file\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n# printing movie details with its rating.\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n##.......##\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)"
  },
  {
    "objectID": "practice/practice 3.html",
    "href": "practice/practice 3.html",
    "title": "",
    "section": "",
    "text": "%pip install pandas-datareader\n\nRequirement already satisfied: pandas-datareader in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.10.0)\nRequirement already satisfied: lxml in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (5.3.0)\nRequirement already satisfied: pandas&gt;=0.23 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (2.2.3)\nRequirement already satisfied: requests&gt;=2.19.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (2.32.3)\nRequirement already satisfied: numpy&gt;=1.23.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (1.26.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2024.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (2024.8.30)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.23-&gt;pandas-datareader) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install BeautifulSoup\n\nCollecting BeautifulSoup\n  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'error'\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install bs4\n\nCollecting bs4\n  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nCollecting beautifulsoup4 (from bs4)\n  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\nCollecting soupsieve&gt;1.2 (from beautifulsoup4-&gt;bs4)\n  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\nDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\nDownloading soupsieve-2.6-py3-none-any.whl (36 kB)\nInstalling collected packages: soupsieve, beautifulsoup4, bs4\nSuccessfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\n\n\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)\n\n\n\n\n\n\n\n\nrownames\nname\nyear\nmonth\nday\nhour\nlat\nlong\nstatus\ncategory\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\n\n0\n1\nAmy\n1975\n6\n27\n0\n27.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n1\n2\nAmy\n1975\n6\n27\n6\n28.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n2\n3\nAmy\n1975\n6\n27\n12\n29.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n3\n4\nAmy\n1975\n6\n27\n18\n30.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n4\n5\nAmy\n1975\n6\n28\n0\n31.5\n-78.8\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n5\n6\nAmy\n1975\n6\n28\n6\n32.4\n-78.7\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n6\n7\nAmy\n1975\n6\n28\n12\n33.3\n-78.0\ntropical depression\nNaN\n25\n1011\nNaN\nNaN\n\n\n7\n8\nAmy\n1975\n6\n28\n18\n34.0\n-77.0\ntropical depression\nNaN\n30\n1006\nNaN\nNaN\n\n\n8\n9\nAmy\n1975\n6\n29\n0\n34.4\n-75.8\ntropical storm\nNaN\n35\n1004\nNaN\nNaN\n\n\n9\n10\nAmy\n1975\n6\n29\n6\n34.0\n-74.8\ntropical storm\nNaN\n40\n1002\nNaN\nNaN\n\n\n\n\n\n\n\n\nurl = \"http://aeturrell.com/research\"\npage = requests.get(url)\npage.text[:300]\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.5.56\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n&lt;meta name=\"author\" content=\"Arthur Turrell\"&gt;\\n'\n\n\n\nsoup = BeautifulSoup(page.text, \"html.parser\")\nprint(soup.prettify()[60000:60500])\n\n       &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=gender pay gap\"&gt;\n            gender pay gap\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=labour\"&gt;\n            labour\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=text analysis\"&gt;\n            text analysis\n           &lt;/a&gt;\n          &lt;/div&gt;\n         &lt;/div&gt;\n         &lt;div class=\"project-details-listing\n\n\n\n# Get all paragraphs\nall_paras = soup.find_all(\"p\")\n# Just show one of the paras\nall_paras[1]\n\n&lt;p&gt;Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" &lt;i&gt;Environment and Planning B: Urban Analytics and City Science&lt;/i&gt; (2024): 23998083241267331. doi: &lt;a href=\"https://doi.org/10.1177/23998083241267331\"&gt;&lt;code&gt;10.1177/23998083241267331&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n\n\nall_paras[1].text\n\n'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331'\n\n\n\nprojects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\nprojects = [x.text.strip() for x in projects]\nprojects[:4]\n\n['Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013',\n 'Haldane, Andrew G., and Arthur E. Turrell. \"Drawing on different disciplines: macroeconomic agent-based models.\" Journal of Evolutionary Economics 29 (2019): 39-66. doi: 10.1007/s00191-018-0557-5']\n\n\n\ndf_list = pd.read_html(\n    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n)\n# Retrieve first and only entry from list of dataframes\ndf = df_list[0]\ndf.head()\n\n\n\n\n\n\n\n\nYears\nHosts\nWinners\nScore\nRunner's-up\nThird place\nScore.1\nFourth place\n\n\n\n\n0\n1930 Details\nUruguay\nUruguay\n4 - 2\nArgentina\nUnited States\n[note 1]\nYugoslavia\n\n\n1\n1934 Details\nItaly\nItaly\n2 - 1\nCzechoslovakia\nGermany\n3 - 2\nAustria\n\n\n2\n1938 Details\nFrance\nItaly\n4 - 2\nHungary\nBrazil\n4 - 2\nSweden\n\n\n3\n1950 Details\nBrazil\nUruguay\n2 - 1\nBrazil\nSweden\n[note 2]\nSpain\n\n\n4\n1954 Details\nSwitzerland\nWest Germany\n3 - 2\nHungary\nAustria\n3 - 1\nUruguay\n\n\n\n\n\n\n\n\n%pip install pdftotext\n\nCollecting pdftotext\n  Using cached pdftotext-2.2.2.tar.gz (113 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nBuilding wheels for collected packages: pdftotext\n  Building wheel for pdftotext (setup.py): started\n  Building wheel for pdftotext (setup.py): finished with status 'error'\n  Running setup.py clean for pdftotext\nFailed to build pdftotext\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n#saving the list as dataframe\n#then converting into .csv file\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n# printing movie details with its rating.\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n##.......##\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\n\n%pip install requests\n\nRequirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2024.8.30)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install beautifulsoup4\n\nRequirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.12.3)\nRequirement already satisfied: soupsieve&gt;1.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.6)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport requests\n \n# 定义请求的 URL 和 headers\nurl = \"https://movie.douban.com/top250\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n}\n \n# 发送 GET 请求\nresponse = requests.get(url, headers=headers)\nresponse.encoding = 'utf-8'  # 设置编码方式\nhtml_content = response.text  # 获取网页的 HTML 内容\nprint(\"网页内容加载成功！\")\n\n网页内容加载成功！\n\n\n\nfrom bs4 import BeautifulSoup\n \n# 使用 Beautiful Soup 解析 HTML\nsoup = BeautifulSoup(html_content, 'html.parser')\n \n# 提取电影名称、描述、评分和评价人数\nmovies = []\nfor item in soup.find_all('div', class_='item'):\n    title = item.find('span', class_='title').get_text()  # 电影名称\n    description = item.find('span', class_='inq')  # 电影描述\n    rating = item.find('span', class_='rating_num').get_text()  # 评分\n    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n    \n    # 如果没有描述，将其置为空字符串\n    if description:\n        description = description.get_text()\n    else:\n        description = ''\n    \n    movie = {\n        \"title\": title,\n        \"description\": description,\n        \"rating\": rating,\n        \"votes\": votes.replace('人评价', '').strip()\n    }\n    movies.append(movie)\n \nprint(\"数据提取成功！\")\n\n数据提取成功！\n\n\n\nimport csv\n \n# 将数据保存到 CSV 文件\nwith open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = ['title', 'description', 'rating', 'votes']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n \n    writer.writeheader()  # 写入表头\n    for movie in movies:\n        writer.writerow(movie)  # 写入每一行数据\n \nprint(\"数据已成功保存到 douban_top250.csv\")\n\n数据已成功保存到 douban_top250.csv\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\n \n# 定义请求的 URL 和 headers\nurl = \"https://movie.douban.com/top250\"\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n}\n \n# 发送 GET 请求\nresponse = requests.get(url, headers=headers)\nresponse.encoding = 'utf-8'  # 设置编码方式\nhtml_content = response.text  # 获取网页的 HTML 内容\n \n# 使用 Beautiful Soup 解析 HTML\nsoup = BeautifulSoup(html_content, 'html.parser')\n \n# 提取电影名称、描述、评分和评价人数\nmovies = []\nfor item in soup.find_all('div', class_='item'):\n    title = item.find('span', class_='title').get_text()  # 电影名称\n    description = item.find('span', class_='inq')  # 电影描述\n    rating = item.find('span', class_='rating_num').get_text()  # 评分\n    votes = item.find('div', class_='star').find_all('span')[3].get_text()  # 评价人数\n    \n    # 如果没有描述，将其置为空字符串\n    if description:\n        description = description.get_text()\n    else:\n        description = ''\n    \n    movie = {\n        \"title\": title,\n        \"description\": description,\n        \"rating\": rating,\n        \"votes\": votes.replace('人评价', '').strip()\n    }\n    movies.append(movie)\n \n# 将数据保存到 CSV 文件\nwith open('douban_top250.csv', 'w', newline='', encoding='utf-8') as csvfile:\n    fieldnames = ['title', 'description', 'rating', 'votes']\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n \n    writer.writeheader()  # 写入表头\n    for movie in movies:\n        writer.writerow(movie)  # 写入每一行数据\n \nprint(\"数据已成功保存到 douban_top250.csv\")\n\n数据已成功保存到 douban_top250.csv"
  },
  {
    "objectID": "practice/practice 2.html",
    "href": "practice/practice 2.html",
    "title": "",
    "section": "",
    "text": "%pip install openpyxl\n\nCollecting openpyxlNote: you may need to restart the kernel to use updated packages.\n\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\nCollecting et-xmlfile (from openpyxl)\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\nDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n   - -------------------------------------- 10.2/250.9 kB ? eta -:--:--\n   ------ -------------------------------- 41.0/250.9 kB 653.6 kB/s eta 0:00:01\n   ------------------- -------------------- 122.9/250.9 kB 1.2 MB/s eta 0:00:01\n   ---------------------------------------  245.8/250.9 kB 1.5 MB/s eta 0:00:01\n   ---------------------------------------- 250.9/250.9 kB 1.5 MB/s eta 0:00:00\nDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n\n\n\nimport pandas as pd\ndata_np = pd.read_excel(\n    \"doing-economics-datafile-working-in-excel-project-2.xlsx\",\n    usecols=\"A:Q\",\n    header=1,\n    index_col=\"Period\",\n)\ndata_n = data_np.iloc[:10, :].copy()\ndata_p = data_np.iloc[14:24, :].copy()\n\n\ntest_data = {\n    \"City A\": [14.1, 14.1, 13.7],\n    \"City B\": [11.0, 12.6, 12.1],\n}\n\n\n# Original dataframe\ntest_df = pd.DataFrame.from_dict(test_data)\n# A copy of the dataframe\ntest_copy = test_df.copy()\n# A pointer to the dataframe\ntest_pointer = test_df\n\n\ntest_pointer.iloc[1, 1] = 99\n\n\nprint(\"test_df=\")\nprint(f\"{test_df}\\n\")\nprint(\"test_copy=\")\nprint(f\"{test_copy}\\n\")\n\ntest_df=\n   City A  City B\n0    14.1    11.0\n1    14.1    99.0\n2    13.7    12.1\n\ntest_copy=\n   City A  City B\n0    14.1    11.0\n1    14.1    12.6\n2    13.7    12.1\n\n\n\n\ndata_n.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 1 to 10\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Copenhagen       10 non-null     object\n 1   Dnipropetrovs’k  10 non-null     object\n 2   Minsk            10 non-null     object\n 3   St. Gallen       10 non-null     object\n 4   Muscat           10 non-null     object\n 5   Samara           10 non-null     object\n 6   Zurich           10 non-null     object\n 7   Boston           10 non-null     object\n 8   Bonn             10 non-null     object\n 9   Chengdu          10 non-null     object\n 10  Seoul            10 non-null     object\n 11  Riyadh           10 non-null     object\n 12  Nottingham       10 non-null     object\n 13  Athens           10 non-null     object\n 14  Istanbul         10 non-null     object\n 15  Melbourne        10 non-null     object\ndtypes: object(16)\nmemory usage: 1.3+ KB\n\n\n\ndata_n = data_n.astype(\"double\")\ndata_p = data_p.astype(\"double\")\n\n\nimport numpy as np\nmean_n_c = data_n.mean(axis=1)\nmean_p_c = data_p.agg(np.mean, axis=1)\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nmean_n_c.plot(ax=ax, label=\"Without punishment\")\nmean_p_c.plot(ax=ax, label=\"With punishment\")\nax.set_title(\"Average contributions to the public goods game\")\nax.set_ylabel(\"Average contribution\")\nax.legend();\n\n\n\n\n\n\n\n\nquestion:Describe any differences and similarities you see in the mean contribution over time in both experiments answer:From the chart,we can see that the trend of the blue line(the experiment without punishment)goes down obviously during each period.However the trend of orange line(the experiment with punishment)goes up slowly.They start from the same point.But they goes differently.\n\npartial_names_list = [\"F. Kennedy\", \"Lennon\", \"Maynard Keynes\", \"Wayne\"]\n[\"John \" + name for name in partial_names_list]\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n# Create new dataframe with bars in\ncompare_grps = pd.DataFrame(\n    [mean_n_c.loc[[1, 10]], mean_p_c.loc[[1, 10]]],\n    index=[\"Without punishment\", \"With punishment\"],\n)\n# Rename columns to have 'round' in them\ncompare_grps.columns = [\"Round \" + str(i) for i in compare_grps.columns]\n# Swap the column and index variables around with the transpose function, ready for plotting (.T is transpose)\ncompare_grps = compare_grps.T\n# Make a bar chart\ncompare_grps.plot.bar(rot=0);\n\n\n\n\n\n\n\n\nquestion:explain whether this means that the two sets of data are the same. answer:No,although the mean contribution for both experiments was 10.6 in Period 1,but in round 10,the mean contribution is totally different.The data from experiment with punishment is much higher than that from experiment without punishment.\n\nn_c = data_n.agg([\"std\", \"var\", \"mean\"], 1)\nn_c\n\n\n\n\n\n\n\n\nstd\nvar\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n1\n2.020724\n4.083325\n10.578313\n\n\n2\n2.238129\n5.009220\n10.628398\n\n\n3\n2.329569\n5.426891\n10.407079\n\n\n4\n2.068213\n4.277504\n9.813033\n\n\n5\n2.108329\n4.445049\n9.305433\n\n\n6\n2.240881\n5.021549\n8.454844\n\n\n7\n2.136614\n4.565117\n7.837568\n\n\n8\n2.349442\n5.519880\n7.376388\n\n\n9\n2.413845\n5.826645\n6.392985\n\n\n10\n2.187126\n4.783520\n4.383769\n\n\n\n\n\n\n\n\np_c = data_p.agg([\"std\", \"var\", \"mean\"], 1)\n\n\nfig, ax = plt.subplots()\nn_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 standard deviations\n(n_c[\"mean\"] + 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 standard deviations\n(n_c[\"mean\"] - 2 * n_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_n.columns)):\n    ax.scatter(x=data_n.index, y=data_n.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game without punishment\")\nplt.show();\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\np_c[\"mean\"].plot(ax=ax, label=\"mean\")\n# mean + 2 sd\n(p_c[\"mean\"] + 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"±2 s.d.\")\n# mean - 2 sd\n(p_c[\"mean\"] - 2 * p_c[\"std\"]).plot(ax=ax, ylim=(0, None), color=\"red\", label=\"\")\nfor i in range(len(data_p.columns)):\n    ax.scatter(x=data_p.index, y=data_p.iloc[:, i], color=\"k\", alpha=0.3)\nax.legend()\nax.set_ylabel(\"Average contribution\")\nax.set_title(\"Contribution to public goods game with punishment\")\nplt.show();\n\n\n\n\n\n\n\n\n\ndata_p.apply(lambda x: x.max() - x.min(), axis=1)\n\nPeriod\n1     10.199675\n2     12.185065\n3     12.689935\n4     12.625000\n5     12.140375\n6     12.827541\n7     13.098931\n8     13.482621\n9     13.496754\n10    11.307360\ndtype: float64\n\n\n\n# A lambda function accepting three inputs, a, b, and c, and calculating the sum of the squares\ntest_function = lambda a, b, c: a**2 + b**2 + c**2\n\n\n# Now we apply the function by handing over (in parenthesis) the following inputs: a=3, b=4 and c=5\ntest_function(3, 4, 5)\n\n50\n\n\n\nrange_function = lambda x: x.max() - x.min()\nrange_p = data_p.apply(range_function, axis=1)\nrange_n = data_n.apply(range_function, axis=1)\n\n\nfig, ax = plt.subplots()\nrange_p.plot(ax=ax, label=\"With punishment\")\nrange_n.plot(ax=ax, label=\"Without punishment\")\nax.set_ylim(0, None)\nax.legend()\nax.set_title(\"Range of contributions to the public goods game\")\nplt.show();\n\n\n\n\n\n\n\n\n\nfuncs_to_apply = [range_function, \"max\", \"min\", \"std\", \"mean\"]\nsumm_p = data_p.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\nsumm_n = data_n.apply(funcs_to_apply, axis=1).rename(columns={\"&lt;lambda&gt;\": \"range\"})\n\n\nsumm_n.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n6.14\n14.10\n7.96\n2.02\n10.58\n\n\n10\n7.38\n8.68\n1.30\n2.19\n4.38\n\n\n\n\n\n\n\n\nsumm_p.loc[[1, 10], :].round(2)\n\n\n\n\n\n\n\n\nrange\nmax\nmin\nstd\nmean\n\n\nPeriod\n\n\n\n\n\n\n\n\n\n1\n10.20\n16.02\n5.82\n3.21\n10.64\n\n\n10\n11.31\n17.51\n6.20\n3.90\n12.87\n\n\n\n\n\n\n\nquestion:Comment on any similarities and differences in the distributions, both across time and across experiments answer:similarities:the statistics vary in different periods and different experiments differences:in experiment with punishment,the data is always going up,data from petiod 10 is always higher than that in period 1 but in experiment without punishment,the data from petiod 10 is not always higher than that in period 1\n\nimport pingouin as pg\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :])\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.063782\n30\ntwo-sided\n0.949567\n[-2.0, 1.87]\n0.02255\n0.337\n0.050437\n\n\n\n\n\n\n\n\npg.ttest(x=data_n.iloc[0, :], y=data_p.iloc[0, :], paired=True)\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n-0.149959\n15\ntwo-sided\n0.882795\n[-0.92, 0.8]\n0.02255\n0.258\n0.05082\n\n\n\n\n\n\n\nquestion:What does this p-value tell us about the difference in means in Period 1? answer:This result of the previous one delivers a p-value of 0.9496. This means it is very likely that the assumption that there are no differences in the populations is likely to be true.In the latter one,The p-value becomes smaller as we can attribute more of the differences to the ‘with punishment’ treatment, but the p-value is still very large (0.8828), so we still conclude that the differences in Period 1 are likely to be due to chance."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "lab.html",
    "href": "lab.html",
    "title": "01Chipotle-Exercises-with-solutions",
    "section": "",
    "text": "01Chipotle-Exercises-with-solutions\n\n\nEx2 - Getting and Knowing your Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 3. Assign it to a variable called chipo.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\n# Solution 1\n\nchipo.shape\n\n(4622, 5)\n\n\n\n# Solution 2\n\nchipo.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nchipo.shape[1]\n\n5\n\n\n\n\nStep 7. Print the name of all the columns.\n\nchipo.columns\n\nIndex(['order_id', 'quantity', 'item_name', 'choice_description',\n       'item_price'],\n      dtype='object')\n\n\n\n\nStep 8. How is the dataset indexed?\n\nchipo.index\n\nRangeIndex(start=0, stop=4622, step=1)\n\n\n\n\nStep 9. Which was the most-ordered item?\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\nStep 10. For the most-ordered item, how many items were ordered?\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\nStep 11. What was the most ordered item in the choice_description column?\n\nchipo.groupby(by=\"choice_description\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nitem_price\n\n\nchoice_description\n\n\n\n\n\n\n\n\n[Diet Coke]\n123455\n159\nCanned SodaCanned SodaCanned Soda6 Pack Soft D...\n$2.18 $1.09 $1.09 $6.49 $2.18 $1.25 $1.09 $6.4...\n\n\n\n\n\n\n\n\n\nStep 12. How many items were orderd in total?\n\nchipo.item_name.count()\n\n4622\n\n\n\n\nStep 13. Turn the item price into a float\n\nStep 13.a. Check the item price type\n\nchipo.item_price.dtype\n\ndtype('O')\n\n\n\n\nStep 13.b. Create a lambda function and change the type of item price\n\ndollarizer = lambda x: float(x[1:-1])\nchipo.item_price = chipo.item_price.apply(dollarizer)\n\n\n\nStep 13.c. Check the item price type\n\nchipo.item_price.dtype\n\ndtype('float64')\n\n\n\n\n\nStep 14. How much was the revenue for the period in the dataset?\n\nrevenue =  (chipo.item_price * chipo.quantity).sum()\nprint('Revenue is : $ '+ str(revenue))\n\nRevenue is : $ 39237.02\n\n\n\n\nStep 15. How many orders were made in the period?\n\nchipo.order_id.value_counts().count()\n\n1834\n\n\n\n\nStep 16. What is the average revenue amount per order?\n\n# Solution 1\nchipo['revenue'] = chipo['quantity'] * chipo['item_price']\norder_grouped = chipo.groupby(by=['order_id']).sum()\norder_grouped['revenue'].mean()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[8], line 4\n      2 chipo['revenue'] = chipo['quantity'] * chipo['item_price']\n      3 order_grouped = chipo.groupby(by=['order_id']).sum()\n----&gt; 4 order_grouped['revenue'].mean()\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6549, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n   6541 @doc(make_doc(\"mean\", ndim=1))\n   6542 def mean(\n   6543     self,\n   (...)\n   6547     **kwargs,\n   6548 ):\n-&gt; 6549     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n  12413 def mean(\n  12414     self,\n  12415     axis: Axis | None = 0,\n   (...)\n  12418     **kwargs,\n  12419 ) -&gt; Series | float:\n&gt; 12420     return self._stat_function(\n  12421         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n  12422     )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n  12373 nv.validate_func(name, (), kwargs)\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n&gt; 12377 return self._reduce(\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n  12379 )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6457, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   6452     # GH#47500 - change to TypeError to match other methods\n   6453     raise TypeError(\n   6454         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n   6455         \"with non-numeric dtypes.\"\n   6456     )\n-&gt; 6457 return op(delegate, skipna=skipna, **kwds)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147, in bottleneck_switch.__call__.&lt;locals&gt;.f(values, axis, skipna, **kwds)\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n    146 else:\n--&gt; 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n    149 return result\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404, in _datetimelike_compat.&lt;locals&gt;.new_func(values, axis, skipna, mask, **kwargs)\n    401 if datetimelike and mask is None:\n    402     mask = isna(values)\n--&gt; 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n    406 if datetimelike:\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:720, in nanmean(values, axis, skipna, mask)\n    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    719 the_sum = values.sum(axis, dtype=dtype_sum)\n--&gt; 720 the_sum = _ensure_numeric(the_sum)\n    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n    723     count = cast(np.ndarray, count)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1701, in _ensure_numeric(x)\n   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n   1699     if isinstance(x, str):\n   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n-&gt; 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n   1702     try:\n   1703         x = float(x)\n\nTypeError: Could not convert string '$2.39 $3.39 $3.39 $2.39 $16.98 $16.98 $10.98 $1.69 $11.75 $9.25 $9.25 $4.45 $8.75 $8.75 $11.25 $4.45 $2.39 $8.49 $8.49 $2.18 $2.18 $8.75 $4.45 $8.99 $3.39 $10.98 $3.39 $2.39 $8.49 $8.99 $1.09 $8.49 $2.39 $8.99 $1.69 $8.99 $1.09 $8.75 $8.75 $4.45 $2.95 $11.75 $2.15 $4.45 $11.25 $11.75 $8.75 $10.98 $8.99 $3.39 $8.99 $3.99 $8.99 $2.18 $2.18 $10.98 $1.09 $8.99 $2.39 $9.25 $11.25 $11.75 $2.15 $4.45 $9.25 $11.25 $8.75 $8.99 $8.99 $3.39 $8.99 $10.98 $8.99 $1.69 $8.99 $3.99 $8.75 $4.45 $8.75 $8.75 $2.15 $8.75 $11.25 $2.15 $9.25 $8.75 $8.75 $9.25 $8.49 $8.99 $1.09 $9.25 $2.95 $11.75 $11.75 $9.25 $11.75 $4.45 $9.25 $4.45 $11.75 $8.75 $8.75 $4.45 $8.99 $8.99 $3.99 $8.49 $3.39 $8.99 $1.09 $9.25 $4.45 $8.75 $2.95 $4.45 $2.39 $8.49 $8.99 $8.49 $1.09 $8.99 $3.99 $8.75 $9.25 $4.45 $11.25 $4.45 $8.99 $1.09 $9.25 $2.95 $4.45 $11.75 $4.45 $8.49 $2.39 $10.98 $22.50 $22.50 $11.75 $4.45 $11.25 $4.45 $11.25 $4.45 $11.25 $11.25 $11.75 $9.25 $4.45 $11.48 $17.98 $17.98 $1.69 $17.50 $17.50 $4.45 $8.49 $2.39 $17.50 $17.50 $4.45 $4.45 $11.25 $11.75 $10.98 $8.49 $10.98 $2.18 $2.18 $11.48 $8.49 $2.39 $4.45 $11.25 $11.75 $8.75 $8.49 $2.18 $2.18 $8.49 $3.39 $8.49 $8.99 $10.98 $11.48 $8.49 $1.09 $1.09 $9.25 $8.75 $2.95 $9.25 $4.45 $11.25 $11.48 $8.49 $8.49 $8.99 $2.39 $11.25 $8.75 $2.95 $1.09 $8.99 $8.49 $2.39 $10.98 $1.09 $3.99 $11.25 $8.75 $8.49 $3.39 $8.75 $9.25 $2.15 $11.25 $11.25 $11.25 $4.45 $22.50 $22.50 $4.45 $11.75 $8.75 $17.50 $17.50 $8.75 $9.25 $8.75 $2.15 $9.25 $4.30 $4.30 $8.75 $11.25 $2.15 $8.99 $1.09 $8.99 $3.99 $8.75 $2.95 $2.95 $11.75 $5.90 $5.90 $9.25 $9.25 $11.75 $9.25 $2.95 $17.50 $17.50 $8.75 $9.25 $10.98 $8.99 $1.09 $1.09 $1.09 $8.99 $10.98 $1.09 $8.75 $8.75 $9.25 $9.25 $8.75 $8.75 $8.99 $8.99 $8.99 $1.09 $11.75 $1.25 $8.99 $2.39 $9.25 $2.95 $8.99 $3.99 $8.49 $2.39 $8.49 $8.49 $8.49 $1.69 $8.49 $3.99 $8.99 $1.69 $1.09 $23.78 $23.78 $17.50 $17.50 $2.15 $8.75 $9.25 $9.25 $8.75 $4.45 $8.75 $11.25 $11.25 $1.25 $9.25 $4.45 $11.25 $11.75 $11.75 $6.49 $8.99 $2.39 $8.49 $2.39 $11.25 $8.75 $2.15 $8.99 $1.69 $8.75 $11.25 $2.15 $4.45 $8.75 $8.49 $8.99 $17.50 $17.50 $8.49 $1.09 $1.09 $8.75 $1.25 $2.15 $11.08 $8.49 $8.49 $8.99 $2.39 $8.75 $2.15 $1.50 $11.25 $2.15 $8.49 $8.49 $11.75 $9.25 $11.75 $1.25 $11.25 $8.75 $4.45 $6.49 $9.25 $2.95 $11.25 $4.45 $1.25 $1.25 $8.49 $2.39 $2.18 $2.18 $8.49 $2.18 $2.18 $22.16 $22.16 $17.50 $17.50 $8.75 $2.95 $6.49 $8.99 $3.39 $3.39 $8.99 $8.49 $11.25 $2.15 $11.25 $2.95 $11.25 $1.25 $8.99 $1.09 $8.75 $8.75 $9.25 $2.95 $11.75 $1.50 $8.99 $1.09 $11.25 $1.25 $1.25 $11.25 $11.75 $2.15 $8.99 $1.69 $11.75 $6.49 $8.75 $9.25 $11.25 $4.45 $1.25 $11.25 $4.45 $8.49 $8.99 $8.49 $8.99 $11.25 $1.25 $11.75 $1.25 $11.75 $9.25 $4.45 $11.25 $2.15 $32.94 $32.94 $32.94 $1.25 $11.25 $11.48 $1.69 $1.09 $17.50 $17.50 $4.45 $6.49 $9.25 $8.75 $9.25 $9.25 $8.75 $8.75 $2.15 $2.95 $17.50 $17.50 $10.98 $11.48 $11.48 $3.39 $8.99 $1.69 $8.99 $1.09 $10.98 $3.39 $8.99 $1.09 $9.25 $8.75 $11.25 $4.45 $2.95 $9.25 $22.20 $22.20 $22.20 $8.49 $8.99 $8.75 $8.75 $11.75 $8.75 $11.25 $9.25 $11.25 $11.25 $8.75 $11.25 $2.95 $1.25 $8.49 $1.69 $11.75 $11.25 $8.75 $8.75 $4.45 $8.49 $3.99 $8.49 $3.99 $11.48 $1.69 $1.09 $11.25 $1.50 $10.58 $1.69 $9.25 $11.25 $8.75 $9.25 $11.25 $11.25 $8.75 $11.75 $8.75 $8.75 $8.75 $2.15 $11.25 $11.75 $2.50 $2.50 $4.45 $9.25 $4.45 $11.25 $8.49 $3.99 $9.25 $9.25 $11.25 $9.25 $11.75 $11.25 $1.25 $23.50 $23.50 $1.25 $8.99 $8.49 $7.40 $7.40 $8.75 $1.25 $4.45 $8.75 $2.15 $8.75 $4.45 $7.40 $7.40 $7.40 $8.99 $3.99 $8.99 $1.69 $8.75 $8.75 $8.75 $8.75 $11.25 $11.25 $2.95 $8.75 $18.50 $18.50 $8.49 $3.99 $2.95 $9.25 $9.25 $3.00 $3.00 $1.25 $8.75 $9.25 $4.45 $8.75 $11.25 $4.45 $10.98 $22.16 $22.16 $4.45 $8.75 $9.25 $6.49 $9.25 $11.25 $8.75 $9.25 $2.15 $9.25 $4.45 $9.25 $2.95 $9.25 $8.75 $9.25 $1.25 $1.25 $8.75 $8.75 $9.25 $4.45 $11.75 $11.75 $11.75 $9.25 $9.25 $16.98 $16.98 $2.39 $3.39 $3.39 $9.25 $11.75 $11.25 $2.15 $8.75 $9.25 $4.45 $10.98 $11.25 $9.25 $22.50 $22.50 $9.25 $2.95 $1.50 $11.48 $8.49 $1.69 $8.49 $8.49 $8.49 $6.78 $6.78 $11.75 $4.45 $8.75 $4.45 $11.89 $9.39 $8.75 $2.95 $1.25 $9.25 $8.75 $23.78 $23.78 $8.75 $9.25 $2.15 $2.15 $1.25 $8.49 $3.99 $10.98 $1.09 $8.75 $4.45 $8.75 $11.75 $2.95 $4.45 $9.25 $8.75 $8.49 $3.99 $22.50 $22.50 $11.25 $1.25 $8.75 $8.75 $18.50 $18.50 $6.49 $8.75 $8.75 $4.45 $8.49 $3.99 $8.99 $1.09 $8.49 $2.39 $11.48 $1.69 $2.50 $2.50 $9.25 $1.50 $17.50 $17.50 $2.95 $8.75 $4.45 $11.75 $8.75 $8.49 $1.69 $8.49 $3.99 $8.99 $8.99 $3.99 $8.99 $11.25 $4.45 $1.25 $3.99 $10.98 $7.40 $3.00 $7.40 $4.00 $8.49 $3.99 $9.25 $4.45 $11.25 $1.25 $11.75 $1.25 $11.25 $2.15 $11.25 $4.45 $3.75 $3.75 $3.75 $11.75 $8.99 $2.39 $8.75 $4.45 $1.25 $8.99 $8.49 $2.18 $2.18 $8.49 $2.18 $2.18 $1.09 $8.75 $2.95 $1.25 $1.50 $11.25 $9.25 $2.95 $1.25 $8.49 $3.99 $11.48 $3.99 $8.49 $11.25 $1.25 $8.99 $1.69 $11.25 $1.25 $6.49 $8.75 $9.25 $8.75 $2.95 $8.75 $11.75 $8.69 $8.69 $2.29 $3.99 $8.49 $8.75 $8.75 $1.25 $11.75 $11.25 $11.25 $11.25 $1.25 $9.25 $11.75 $6.49 $3.99 $8.49 $11.25 $2.15 $11.25 $11.89 $8.99 $1.69 $8.99 $8.99 $3.99 $8.99 $9.25 $9.25 $2.15 $7.40 $7.40 $8.75 $8.75 $9.25 $4.45 $11.25 $1.25 $11.75 $11.25 $1.25 $3.99 $8.49 $8.49 $8.49 $8.99 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $8.75 $2.95 $1.25 $9.25 $1.25 $2.15 $11.25 $1.25 $4.45 $8.75 $2.50 $2.50 $8.90 $8.90 $8.75 $8.75 $8.75 $11.25 $11.25 $10.98 $3.99 $10.98 $3.99 $1.69 $8.99 $9.25 $8.75 $8.99 $1.09 $9.25 $2.95 $8.75 $9.25 $3.99 $8.49 $8.75 $8.75 $22.50 $22.50 $10.98 $3.27 $3.27 $3.27 $3.99 $8.99 $1.09 $11.08 $8.75 $4.45 $11.08 $3.99 $8.49 $4.30 $4.30 $9.25 $8.75 $11.25 $11.25 $9.25 $8.49 $8.99 $8.49 $8.75 $2.95 $4.45 $9.25 $2.95 $9.25 $8.75 $11.25 $4.45 $16.98 $16.98 $8.49 $2.39 $11.25 $3.75 $3.75 $3.75 $9.25 $4.45 $9.25 $9.25 $4.45 $8.75 $9.25 $8.75 $9.25 $9.25 $9.25 $11.48 $8.99 $22.50 $22.50 $11.75 $11.25 $1.25 $8.75 $2.15 $1.25 $11.25 $8.75 $1.25 $11.25 $1.50 $11.25 $11.25 $9.25 $6.49 $8.90 $8.90 $8.75 $4.45 $11.25 $1.25 $17.50 $17.50 $9.25 $8.75 $11.75 $3.00 $3.00 $8.49 $8.49 $10.98 $8.99 $3.99 $8.75 $4.45 $8.99 $1.69 $11.75 $8.75 $11.25 $4.45 $11.75 $1.25 $11.75 $2.95 $8.99 $8.99 $2.18 $2.18 $17.98 $17.98 $8.99 $8.49 $1.69 $11.75 $11.25 $2.95 $3.75 $3.75 $3.75 $9.25 $11.75 $8.75 $2.15 $1.50 $8.49 $8.49 $3.39 $8.69 $3.89 $8.75 $4.45 $8.75 $11.25 $2.15 $8.75 $8.49 $1.69 $8.49 $8.49 $1.25 $8.75 $11.75 $11.75 $8.99 $1.09 $8.75 $4.45 $8.75 $2.95 $8.75 $2.15 $3.99 $8.49 $8.99 $3.99 $8.49 $1.69 $1.09 $8.99 $1.09 $9.25 $8.75 $8.99 $2.39 $1.25 $1.25 $11.25 $11.25 $9.25 $9.25 $11.25 $1.50 $3.99 $8.49 $11.25 $9.25 $11.25 $17.50 $17.50 $8.75 $8.90 $8.90 $8.75 $8.75 $8.99 $2.39 $11.25 $9.25 $2.15 $11.25 $1.25 $11.75 $1.25 $11.25 $11.75 $1.25 $11.25 $11.25 $8.49 $10.98 $8.75 $1.25 $8.75 $8.49 $8.49 $1.50 $1.50 $8.75 $4.45 $11.25 $1.25 $11.75 $8.49 $2.39 $9.25 $4.45 $9.25 $8.75 $8.99 $1.69 $17.50 $17.50 $2.39 $8.99 $8.99 $11.25 $4.45 $8.75 $4.45 $9.25 $6.49 $10.98 $8.49 $8.49 $1.09 $1.69 $9.25 $4.45 $8.75 $1.25 $2.95 $3.99 $8.49 $11.75 $11.75 $2.15 $11.48 $8.75 $2.15 $1.25 $11.25 $2.15 $1.25 $8.75 $8.75 $6.49 $1.69 $8.99 $8.75 $11.75 $10.98 $1.09 $8.49 $3.39 $8.75 $2.15 $1.25 $11.48 $10.98 $10.98 $8.49 $2.95 $9.25 $9.25 $11.75 $4.45 $11.48 $11.25 $8.75 $4.45 $1.69 $8.99 $8.75 $4.45 $1.50 $11.75 $2.15 $8.99 $2.39 $8.75 $2.95 $1.25 $8.75 $2.15 $1.25 $2.18 $2.18 $2.18 $2.18 $11.48 $8.75 $2.95 $11.75 $11.75 $1.25 $10.58 $8.99 $2.39 $11.75 $4.45 $11.25 $11.25 $17.50 $17.50 $8.75 $8.75 $8.75 $22.50 $22.50 $9.25 $8.75 $4.45 $11.75 $1.25 $11.25 $11.25 $2.95 $8.99 $1.69 $11.25 $4.45 $8.75 $6.49 $8.75 $4.45 $9.25 $4.45 $11.75 $11.75 $4.45 $11.89 $11.75 $11.25 $2.95 $1.50 $4.45 $8.75 $8.99 $1.09 $8.99 $1.09 $3.99 $11.48 $8.49 $9.25 $4.45 $11.48 $9.25 $2.95 $9.25 $8.49 $8.99 $8.99 $8.49 $8.75 $2.95 $4.45 $11.89 $10.58 $8.19 $1.69 $8.75 $2.15 $1.25 $17.50 $17.50 $6.49 $9.25 $2.15 $8.75 $4.45 $8.75 $1.25 $11.48 $11.48 $8.99 $2.18 $2.18 $8.49 $8.99 $2.39 $2.39 $2.18 $2.18 $8.75 $4.45 $11.25 $9.25 $9.25 $11.25 $11.25 $4.45 $2.95 $11.75 $8.49 $8.49 $8.99 $1.69 $9.25 $11.25 $11.75 $9.25 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $10.98 $11.25 $4.45 $10.98 $8.49 $8.99 $3.39 $3.99 $8.99 $1.09 $1.09 $2.39 $17.50 $17.50 $4.45 $11.25 $11.25 $4.45 $9.25 $4.45 $8.75 $2.15 $1.25 $11.89 $2.95 $11.75 $1.25 $11.25 $4.45 $11.48 $11.48 $2.95 $9.25 $8.75 $9.25 $2.95 $11.25 $1.25 $11.75 $1.25 $8.99 $2.39 $1.25 $11.25 $1.25 $11.25 $8.49 $3.99 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $8.75 $11.80 $11.80 $11.80 $11.80 $8.90 $8.90 $5.90 $5.90 $6.49 $10.98 $17.98 $17.98 $2.39 $9.25 $8.75 $2.15 $8.75 $4.45 $8.49 $1.69 $8.19 $8.69 $10.98 $3.99 $11.48 $11.48 $4.45 $8.75 $6.49 $8.75 $8.75 $9.25 $1.25 $4.45 $8.49 $1.69 $9.25 $4.45 $8.99 $1.09 $11.25 $2.95 $11.08 $11.08 $3.89 $10.98 $11.25 $8.75 $11.25 $9.25 $4.30 $4.30 $8.75 $8.49 $3.99 $1.69 $8.99 $8.49 $1.69 $11.75 $11.25 $11.89 $9.25 $2.95 $9.25 $2.95 $8.75 $4.45 $4.45 $8.75 $10.98 $11.48 $8.49 $9.25 $4.45 $11.75 $11.89 $8.99 $8.49 $8.75 $9.25 $8.75 $8.75 $11.75 $11.75 $4.45 $11.25 $11.75 $2.50 $2.50 $8.99 $1.69 $11.75 $2.15 $1.25 $9.25 $8.75 $8.90 $8.90 $9.25 $2.95 $8.75 $11.25 $8.90 $8.90 $11.25 $11.75 $11.48 $1.69 $3.39 $9.25 $2.95 $8.99 $1.69 $8.49 $10.98 $11.25 $2.95 $8.99 $1.69 $8.75 $2.15 $1.25 $8.75 $2.95 $9.25 $2.50 $2.50 $11.25 $1.25 $11.75 $2.50 $2.50 $11.25 $1.50 $8.75 $1.25 $2.95 $11.48 $11.48 $8.75 $8.75 $2.15 $11.75 $1.25 $9.25 $9.25 $6.49 $11.75 $8.49 $8.49 $1.09 $10.98 $8.75 $1.25 $2.15 $11.25 $1.50 $11.25 $11.25 $8.49 $8.49 $8.75 $1.50 $1.25 $1.50 $8.75 $2.50 $2.50 $2.15 $7.40 $7.40 $4.00 $9.25 $9.39 $9.25 $9.25 $9.39 $11.25 $8.90 $8.90 $11.25 $6.00 $6.00 $6.00 $6.00 $11.25 $11.25 $11.25 $22.50 $22.50 $11.48 $1.09 $8.49 $8.49 $17.50 $17.50 $11.25 $1.50 $9.25 $8.75 $3.99 $8.49 $8.75 $8.75 $8.75 $8.75 $8.75 $11.75 $1.50 $11.25 $11.25 $2.95 $8.99 $10.98 $9.25 $8.75 $4.45 $8.49 $1.09 $2.39 $8.75 $8.75 $11.48 $8.99 $8.49 $8.49 $2.39 $10.98 $8.49 $3.99 $11.75 $4.45 $8.75 $2.15 $1.25 $10.98 $8.99 $11.25 $1.50 $8.75 $2.15 $1.25 $8.75 $9.25 $8.75 $11.25 $1.50 $8.75 $1.25 $4.45 $10.98 $8.75 $2.95 $1.25 $8.75 $2.95 $1.25 $8.49 $8.49 $2.39 $11.25 $1.25 $8.75 $8.75 $9.25 $8.75 $11.89 $1.25 $8.75 $2.15 $1.25 $8.99 $1.09 $8.75 $4.45 $26.25 $26.25 $26.25 $8.75 $4.45 $11.75 $2.95 $8.75 $8.75 $11.75 $8.75 $11.25 $11.25 $11.25 $4.45 $1.25 $8.49 $8.49 $8.49 $8.99 $8.99 $2.39 $2.39 $3.99 $8.75 $4.45 $2.15 $9.25 $1.25 $11.25 $11.75 $8.75 $4.45 $11.25 $2.15 $8.75 $4.45 $8.75 $8.75 $1.25 $11.25 $2.15 $8.75 $5.90 $5.90 $11.75 $1.25 $9.25 $3.75 $3.75 $3.75 $8.75 $1.25 $4.45 $11.75 $4.45 $8.75 $23.50 $23.50 $8.75 $2.95 $8.75 $8.75 $11.89 $4.45 $2.95 $1.25 $8.75 $4.45 $2.95 $1.25 $8.75 $2.15 $1.25 $11.75 $2.95 $8.99 $3.39 $9.25 $9.25 $17.50 $17.50 $2.95 $11.89 $1.50 $11.25 $2.95 $9.25 $11.25 $11.25 $2.95 $8.75 $9.25 $4.30 $4.30 $8.75 $8.75 $11.25 $8.75 $4.30 $4.30 $8.75 $1.25 $2.15 $8.49 $8.49 $3.39 $3.39 $10.98 $10.98 $2.39 $11.25 $11.75 $11.75 $1.25 $5.90 $5.90 $8.75 $11.25 $9.25 $4.45 $1.50 $3.39 $8.99 $2.39 $11.25 $2.15 $11.25 $11.75 $11.75 $4.45 $11.75 $4.45 $9.25 $8.75 $8.49 $8.99 $8.49 $8.99 $11.75 $8.75 $8.49 $3.99 $3.89 $11.08 $8.49 $8.99 $8.49 $8.49 $8.49 $11.25 $2.15 $17.50 $17.50 $8.75 $2.95 $8.49 $8.49 $10.98 $1.09 $11.25 $2.15 $2.95 $1.25 $8.75 $9.25 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $8.99 $3.99 $11.75 $2.15 $8.99 $3.39 $9.25 $8.75 $11.25 $11.25 $4.45 $8.75 $2.15 $1.25 $11.75 $4.45 $9.25 $2.95 $8.49 $8.49 $11.25 $8.75 $4.45 $11.25 $11.25 $11.25 $11.25 $4.45 $8.49 $1.69 $8.49 $3.39 $8.75 $11.25 $9.25 $8.75 $11.25 $11.25 $11.75 $11.25 $11.75 $11.25 $11.75 $21.96 $21.96 $10.98 $1.69 $11.48 $8.99 $8.49 $1.69 $9.25 $2.15 $1.50 $11.25 $1.50 $8.75 $8.75 $2.95 $8.49 $1.69 $8.75 $2.95 $1.25 $11.25 $2.15 $11.08 $8.49 $8.49 $8.49 $11.75 $1.25 $11.75 $8.75 $8.75 $8.75 $4.45 $11.25 $1.50 $23.50 $23.50 $11.75 $6.49 $8.75 $4.45 $6.49 $8.75 $2.50 $2.50 $2.15 $8.49 $2.39 $8.75 $11.75 $4.45 $8.99 $10.98 $9.25 $2.95 $9.25 $9.25 $11.75 $8.75 $8.75 $8.75 $10.98 $11.25 $9.25 $8.75 $8.75 $2.15 $11.25 $2.15 $4.45 $11.75 $8.49 $2.39 $9.25 $1.25 $1.25 $1.25 $1.25 $8.75 $2.15 $8.49 $1.69 $11.25 $1.50 $8.75 $8.75 $8.49 $3.99 $8.99 $1.09 $11.25 $1.25 $8.49 $2.39 $8.49 $8.75 $9.25 $11.25 $4.45 $11.25 $11.89 $8.99 $8.49 $8.75 $4.45 $8.75 $11.75 $11.75 $8.90 $8.90 $9.39 $2.95 $8.49 $3.99 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $8.75 $4.45 $8.49 $8.99 $8.49 $3.99 $8.75 $8.75 $2.95 $8.75 $17.50 $17.50 $9.25 $2.95 $8.75 $6.49 $4.30 $4.30 $8.75 $8.75 $2.15 $1.50 $8.49 $8.49 $2.39 $9.25 $4.45 $6.49 $11.75 $4.45 $10.98 $1.69 $9.39 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $11.25 $9.25 $8.75 $11.25 $8.75 $11.25 $2.50 $2.50 $2.50 $2.50 $6.00 $6.00 $6.00 $6.00 $8.90 $8.90 $5.90 $5.90 $11.25 $11.25 $8.49 $10.98 $8.75 $2.15 $1.50 $9.25 $1.25 $1.50 $2.15 $1.25 $8.75 $2.95 $8.49 $3.99 $11.25 $4.30 $4.30 $11.75 $2.15 $18.50 $18.50 $8.49 $2.39 $8.75 $4.45 $11.75 $8.99 $3.99 $9.25 $9.25 $1.50 $8.75 $2.95 $6.49 $11.75 $8.49 $8.99 $8.75 $4.45 $6.49 $22.50 $22.50 $9.25 $2.95 $8.49 $1.69 $10.98 $8.75 $4.45 $11.25 $2.95 $8.99 $8.49 $2.39 $11.75 $6.49 $11.25 $11.75 $2.95 $8.99 $1.69 $8.99 $2.18 $2.18 $1.09 $8.99 $8.99 $1.09 $8.99 $8.99 $8.49 $10.98 $1.09 $11.75 $9.25 $11.25 $11.25 $2.15 $11.25 $8.75 $4.45 $2.95 $11.75 $1.50 $8.99 $10.98 $2.39 $8.75 $2.15 $9.25 $1.50 $8.75 $2.15 $3.99 $8.99 $6.49 $8.75 $8.90 $8.90 $8.99 $3.99 $17.50 $17.50 $11.25 $1.25 $10.98 $9.25 $4.45 $1.25 $3.00 $3.00 $11.25 $4.45 $4.45 $2.95 $9.25 $11.25 $2.15 $11.25 $11.25 $4.45 $2.95 $9.25 $11.25 $1.25 $8.75 $2.95 $1.25 $8.75 $4.45 $11.48 $11.48 $8.49 $2.39 $11.25 $11.75 $2.15 $1.50 $2.15 $8.75 $11.25 $8.90 $8.90 $11.25 $11.25 $1.25 $4.45 $9.25 $9.25 $8.75 $9.25 $8.75 $8.75 $9.25 $8.75 $11.75 $11.75 $8.75 $8.75 $8.90 $8.90 $2.95 $10.98 $8.49 $8.49 $10.98 $8.99 $8.99 $11.75 $17.50 $17.50 $11.75 $3.99 $8.49 $10.98 $1.69 $17.50 $17.50 $8.99 $2.39 $8.99 $2.39 $1.25 $8.75 $2.95 $11.75 $11.25 $17.50 $17.50 $8.49 $8.49 $2.39 $11.25 $1.50 $8.75 $3.00 $3.00 $1.25 $8.75 $4.45 $11.75 $11.75 $4.45 $21.96 $21.96 $8.75 $4.45 $8.75 $11.25 $9.25 $8.99 $2.39 $9.25 $8.75 $10.98 $8.49 $3.99 $3.39 $11.75 $1.50 $4.45 $9.25 $8.75 $1.25 $11.75 $8.75 $1.50 $8.75 $8.75 $2.15 $1.50 $8.75 $2.95 $8.75 $8.75 $17.50 $17.50 $8.75 $6.49 $4.45 $11.25 $11.25 $4.30 $4.30 $8.75 $11.25 $4.45 $8.99 $2.39 $9.25 $9.25 $9.25 $4.45 $11.75 $11.25 $2.95 $2.15 $11.25 $11.25 $8.75 $2.15 $1.50 $9.25 $4.45 $10.98 $8.99 $2.18 $2.18 $8.75 $4.45 $1.25 $8.99 $2.39 $4.45 $8.75 $10.98 $11.75 $1.50 $10.98 $8.99 $8.49 $3.99 $8.99 $8.49 $3.99 $8.49 $8.49 $8.99 $11.25 $11.25 $10.98 $10.98 $10.98 $2.39 $3.39 $8.75 $1.25 $2.95 $11.75 $1.50 $10.98 $1.69 $4.45 $8.75 $8.75 $8.75 $8.75 $4.45 $9.25 $8.75 $11.25 $8.75 $3.99 $8.99 $8.49 $11.25 $11.25 $8.75 $4.45 $8.75 $4.45 $1.25 $8.75 $8.75 $1.50 $2.15 $11.75 $11.75 $11.75 $11.75 $11.75 $1.50 $8.75 $9.25 $1.25 $8.75 $2.15 $8.99 $1.09 $4.45 $11.25 $11.75 $2.15 $8.75 $8.75 $1.25 $9.25 $2.15 $11.75 $11.25 $8.75 $11.25 $4.45 $8.49 $1.69 $8.75 $8.75 $8.99 $8.49 $9.25 $11.25 $2.95 $4.45 $11.75 $6.49 $11.48 $8.99 $4.36 $4.36 $4.36 $4.36 $11.48 $8.99 $8.49 $11.48 $8.75 $2.15 $1.50 $8.99 $1.69 $11.25 $1.25 $9.25 $9.25 $8.75 $9.25 $8.90 $8.90 $2.15 $9.25 $10.98 $8.49 $8.75 $9.25 $4.30 $4.30 $9.25 $8.75 $8.75 $2.15 $1.25 $8.75 $1.25 $8.75 $5.90 $5.90 $9.25 $8.75 $9.25 $4.45 $9.25 $11.75 $2.50 $2.50 $9.25 $2.15 $9.25 $1.50 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $10.98 $8.75 $8.75 $8.75 $2.15 $1.25 $10.98 $8.75 $2.15 $1.50 $8.75 $2.95 $1.25 $9.25 $9.25 $8.49 $2.39 $8.75 $4.45 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.99 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $11.25 $11.25 $8.75 $1.50 $2.15 $16.98 $16.98 $11.75 $1.50 $8.75 $4.30 $4.30 $1.50 $8.75 $2.95 $1.25 $1.25 $9.25 $4.45 $11.25 $8.75 $4.45 $8.75 $2.15 $1.25 $10.98 $1.69 $8.75 $1.25 $8.75 $1.25 $11.25 $8.75 $8.75 $8.49 $1.69 $9.25 $11.75 $8.49 $2.39 $9.25 $2.95 $6.49 $8.75 $8.75 $9.25 $8.75 $6.78 $6.78 $17.98 $17.98 $3.39 $11.75 $11.25 $8.75 $4.45 $11.75 $9.25 $8.75 $6.49 $8.99 $2.39 $8.75 $11.25 $11.75 $4.45 $8.75 $2.15 $9.25 $9.25 $9.25 $11.89 $11.75 $11.25 $9.25 $9.25 $8.75 $8.75 $8.49 $1.69 $1.09 $11.25 $1.50 $11.25 $11.25 $11.75 $1.50 $8.49 $8.99 $22.50 $22.50 $8.75 $4.30 $4.30 $8.75 $11.25 $2.15 $11.25 $2.95 $4.45 $11.25 $8.49 $3.39 $2.39 $11.75 $2.15 $11.75 $8.99 $2.39 $8.75 $11.75 $11.89 $1.25 $7.50 $7.50 $7.50 $7.50 $7.50 $11.89 $1.09 $8.49 $2.39 $8.75 $8.75 $8.75 $8.75 $9.25 $11.25 $8.75 $8.90 $8.90 $9.25 $8.75 $8.75 $11.75 $3.00 $3.00 $1.50 $11.25 $11.75 $8.99 $10.98 $4.45 $8.75 $2.15 $9.25 $11.25 $4.45 $1.69 $10.98 $9.25 $11.75 $9.25 $4.45 $10.98 $3.99 $8.49 $1.25 $9.25 $4.45 $10.98 $8.75 $8.75 $11.75 $11.25 $8.49 $11.48 $4.45 $1.25 $11.25 $8.99 $1.09 $2.39 $11.25 $2.15 $8.75 $4.45 $8.49 $1.69 $10.98 $1.69 $9.25 $4.45 $11.25 $8.75 $11.25 $11.75 $11.25 $22.50 $22.50 $8.49 $2.39 $2.50 $2.50 $8.75 $8.75 $9.25 $9.25 $11.25 $8.99 $1.09 $8.99 $1.69 $11.75 $1.25 $21.96 $21.96 $8.75 $2.15 $1.25 $8.75 $11.25 $9.25 $11.25 $8.75 $8.75 $11.25 $2.15 $8.99 $1.09 $1.69 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $11.48 $8.49 $8.49 $4.78 $4.78 $9.25 $1.25 $1.25 $1.25 $11.25 $11.25 $11.75 $4.45 $11.25 $4.45 $8.99 $1.09 $11.25 $2.15 $11.25 $9.25 $11.75 $11.25 $11.25 $9.25 $2.95 $11.25 $4.45 $8.75 $2.95 $2.95 $11.25 $1.50 $10.98 $16.98 $16.98 $18.50 $18.50 $10.98 $3.99 $1.09 $9.25 $9.25 $8.75 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $1.25 $4.45 $9.25 $8.75 $8.75 $17.50 $17.50 $4.45 $9.39 $1.25 $2.95 $11.25 $8.75 $8.75 $11.25 $2.15 $8.90 $8.90 $11.25 $11.89 $10.98 $11.25 $4.45 $11.25 $11.25 $8.49 $10.98 $8.49 $3.39 $9.25 $8.75 $2.95 $3.00 $3.00 $9.39 $11.75 $2.95 $1.50 $11.25 $11.75 $8.75 $2.15 $1.50 $8.49 $3.39 $11.75 $1.25 $17.50 $17.50 $11.25 $1.25 $8.75 $2.95 $1.25 $11.25 $11.75 $13.35 $13.35 $13.35 $11.25 $11.75 $11.25 $11.25 $4.45 $11.25 $8.49 $3.39 $9.25 $2.95 $4.78 $4.78 $2.39 $3.99 $8.99 $8.99 $11.25 $11.25 $8.75 $11.25 $2.95 $4.45 $9.25 $8.75 $4.45 $8.49 $8.49 $10.98 $10.98 $3.99 $11.75 $8.75 $11.75 $4.45 $1.50 $1.25 $8.49 $8.49 $8.75 $8.75 $8.75 $9.25 $8.75 $2.95 $1.25 $11.25 $1.50 $11.25 $4.45 $9.25 $8.75 $8.75 $9.25 $8.75 $4.45 $1.50 $8.75 $8.75 $8.49 $1.69 $8.75 $2.15 $9.25 $2.15 $1.50 $11.25 $11.75 $2.15 $6.49 $9.25 $9.25 $11.25 $11.25 $11.75 $11.75 $11.75 $11.25 $8.75 $2.15 $1.25 $11.75 $9.25 $11.25 $8.75 $5.90 $5.90 $8.75 $4.45 $9.25 $9.25 $4.45 $11.25 $4.45 $11.25 $8.75 $2.15 $11.89 $11.25 $8.75 $2.95 $1.50 $8.75 $4.30 $4.30 $8.75 $11.25 $11.75 $11.75 $2.15 $11.25 $8.99 $1.09 $8.49 $8.49 $8.49 $3.39 $8.99 $10.98 $3.99 $11.75 $2.15 $8.75 $4.45 $2.50 $2.50 $11.48 $1.09 $8.49 $8.49 $16.98 $16.98 $3.99 $10.98 $1.09 $8.75 $2.95 $8.75 $8.75 $2.95 $9.25 $11.25 $2.15 $9.25 $4.45 $4.45 $9.25 $11.75 $11.75 $2.15 $9.25 $8.75 $11.25 $6.49 $8.75 $11.25 $2.95 $10.98 $3.99 $1.50 $9.25 $2.15 $8.75 $11.25 $11.89 $4.45 $1.50 $1.25 $8.75 $8.75 $4.45 $11.25 $11.75 $8.49 $1.09 $1.09 $1.69 $8.99 $3.39 $8.99 $1.69 $8.49 $8.99 $3.27 $3.27 $3.27 $8.99 $8.99 $1.09 $10.98 $1.69 $3.99 $8.49 $1.09 $8.75 $8.75 $11.75 $8.75 $9.25 $8.75 $3.39 $8.99 $8.99 $2.39 $9.25 $8.75 $9.25 $9.25 $8.99 $3.99 $2.39 $8.49 $1.09 $8.49 $8.99 $3.39 $11.25 $1.25 $8.99 $3.99 $8.75 $8.90 $8.90 $6.49 $8.75 $9.25 $11.25 $11.25 $11.25 $1.25 $8.75 $9.25 $4.45 $1.25 $8.75 $1.25 $2.15 $17.98 $17.98 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $1.50 $8.75 $8.75 $11.08 $8.99 $1.69 $8.99 $1.69 $10.98 $3.99 $3.39 $11.75 $2.15 $11.75 $2.95 $8.75 $8.75 $11.75 $11.25 $11.75 $11.25 $4.45 $11.25 $1.25 $2.18 $2.18 $2.18 $2.18 $2.39 $8.49 $8.99 $2.39 $11.25 $8.75 $11.75 $11.75 $11.25 $4.45 $2.15 $8.19 $10.58 $4.45 $9.25 $1.09 $8.99 $11.25 $1.50 $8.99 $3.99 $4.45 $11.75 $2.15 $11.25 $8.75 $4.45 $8.75 $9.25 $6.45 $6.45 $6.45 $8.75 $11.25 $11.25 $8.75 $11.75 $21.96 $21.96 $8.99 $5.07 $5.07 $5.07 $8.49 $9.25 $11.25 $4.45 $3.39 $8.49 $8.99 $8.49 $17.50 $17.50 $22.96 $22.96 $8.75 $11.25 $11.89 $11.25 $8.49 $1.69 $1.09 $8.99 $8.99 $9.25 $8.75 $9.25 $2.95 $8.49 $3.99 $8.99 $8.49 $7.17 $7.17 $7.17 $8.49 $8.99 $17.50 $17.50 $9.25 $9.25 $11.25 $1.25 $8.99 $1.09 $8.75 $4.45 $11.25 $2.15 $11.75 $11.25 $11.25 $8.75 $8.75 $4.45 $1.25 $11.75 $11.75 $2.50 $2.50 $8.49 $8.99 $2.18 $2.18 $11.25 $4.45 $11.25 $11.75 $8.49 $8.99 $1.69 $1.09 $8.99 $8.99 $11.25 $6.49 $11.25 $8.75 $4.45 $8.99 $1.69 $11.48 $11.75 $2.50 $2.50 $8.49 $1.09 $1.09 $1.69 $8.49 $2.39 $11.75 $1.25 $8.49 $1.69 $8.49 $1.69 $11.75 $4.45 $8.75 $8.75 $4.45 $8.75 $11.25 $11.25 $8.75 $7.98 $7.98 $8.49 $1.09 $8.49 $3.99 $8.49 $3.99 $8.99 $3.99 $11.25 $4.45 $8.49 $2.39 $8.49 $2.39 $3.99 $8.49 $1.25 $11.25 $4.45 $9.25 $4.45 $1.09 $8.99 $3.99 $11.25 $8.90 $8.90 $9.25 $11.25 $8.75 $11.25 $11.25 $11.25 $11.25 $11.25 $8.99 $8.49 $8.75 $8.75 $4.45 $16.98 $16.98 $11.75 $11.25 $9.25 $4.45 $9.25 $2.95 $8.49 $1.69 $3.75 $3.75 $3.75 $4.45 $9.25 $1.50 $11.25 $11.48 $11.25 $2.15 $8.75 $9.39 $8.49 $3.99 $8.19 $2.29 $11.48 $1.69 $11.48 $3.99 $8.49 $1.69 $9.25 $2.95 $8.49 $1.69 $11.25 $4.45 $9.39 $9.25 $8.75 $8.75 $4.45 $11.89 $4.45 $4.45 $8.75 $8.75 $8.75 $2.15 $8.75 $3.75 $3.75 $3.75 $9.25 $11.25 $4.45 $6.49 $16.98 $16.98 $18.50 $18.50 $2.50 $2.50 $2.95 $3.99 $8.49 $8.19 $11.08 $6.49 $11.75 $2.39 $8.99 $1.09 $11.25 $4.45 $11.25 $8.99 $1.69 $21.96 $21.96 $2.18 $2.18 $8.99 $8.99 $2.39 $8.69 $1.69 $8.90 $8.90 $2.50 $2.50 $8.75 $8.99 $1.09 $8.49 $8.49 $8.75 $4.45 $17.50 $17.50 $8.75 $9.25 $8.49 $2.39 $8.75 $4.45 $11.25 $11.25 $11.75 $8.75 $8.49 $8.49 $8.49 $8.99 $8.75 $4.45 $11.48 $8.75 $1.25 $2.15 $9.25 $4.45 $11.75 $2.15 $11.25 $8.99 $2.39 $8.69 $8.69 $11.75 $2.95 $11.75 $1.50 $9.25 $4.45 $1.50 $11.48 $8.99 $2.39 $11.25 $11.89 $2.15 $1.25 $11.75 $4.45 $8.75 $8.75 $11.25 $4.45 $11.25 $2.15 $4.45 $8.49 $1.09 $3.99 $11.25 $11.25 $8.49 $2.39 $8.99 $2.39 $11.25 $2.15 $8.75 $2.95 $1.25 $8.75 $11.25 $17.50 $17.50 $11.75 $11.75 $11.25 $11.25 $4.45 $2.50 $2.50 $8.75 $8.99 $8.99 $1.69 $8.99 $1.69 $11.25 $1.25 $11.08 $8.69 $8.99 $1.09 $11.25 $11.25 $2.95 $1.25 $8.75 $1.25 $8.75 $8.75 $2.15 $1.25 $8.49 $3.99 $8.49 $2.39 $8.49 $7.17 $7.17 $7.17 $8.75 $4.45 $11.48 $8.75 $8.75 $11.48 $8.75 $9.25 $8.49 $3.99 $1.50 $11.25 $11.25 $8.75 $8.75 $4.45 $9.25 $4.45 $8.75 $8.75 $4.30 $4.30 $2.95 $8.75 $4.50 $4.50 $4.50 $9.25 $11.25 $4.45 $11.25 $11.25 $8.75 $9.25 $8.75 $2.15 $1.25 $8.75 $2.15 $1.25 $8.99 $8.49 $8.75 $8.75 $1.25 $11.75 $4.50 $4.50 $4.50 $8.75 $8.75 $3.99 $3.39 $8.49 $2.39 $8.99 $1.50 $11.25 $11.25 $8.75 $8.75 $2.15 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $26.07 $26.07 $26.07 $11.75 $1.50 $8.99 $8.99 $11.48 $9.25 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $8.75 $2.15 $1.25 $11.89 $8.75 $11.75 $4.45 $18.50 $18.50 $9.25 $9.39 $8.49 $2.39 $8.49 $1.69 $1.09 $8.99 $8.49 $2.18 $2.18 $11.25 $1.25 $8.49 $3.39 $8.49 $3.99 $11.25 $8.75 $8.49 $1.69 $16.98 $16.98 $9.25 $9.25 $11.75 $1.25 $11.25 $8.75 $8.49 $8.49 $8.75 $1.25 $1.25 $1.25 $11.25 $12.98 $12.98 $11.75 $11.75 $4.45 $11.25 $11.75 $10.98 $8.49 $8.49 $2.39 $9.25 $11.25 $8.75 $2.95 $1.50 $11.25 $2.95 $9.25 $2.95 $9.25 $8.75 $11.25 $8.75 $8.75 $4.45 $11.25 $1.25 $8.75 $2.95 $2.50 $2.50 $9.25 $9.25 $9.25 $6.49 $17.50 $17.50 $8.49 $1.69 $8.49 $3.99 $8.75 $2.95 $2.95 $8.49 $8.99 $8.99 $1.09 $8.75 $4.45 $1.25 $11.25 $11.75 $11.25 $9.25 $11.25 $1.50 $11.25 $2.15 $10.98 $8.75 $11.75 $2.95 $11.25 $11.25 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $1.25 $11.25 $1.50 $2.15 $8.75 $8.75 $11.25 $8.75 $8.75 $11.25 $4.45 $8.49 $8.49 $8.49 $8.49 $8.99 $1.69 $2.39 $1.09 $1.09 $11.25 $2.95 $35.25 $35.25 $35.25 $8.75 $2.95 $1.25 $11.25 $2.15 $9.25 $4.45 $8.75 $8.75 $8.75 $4.45 $1.25 $11.89 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $8.69 $8.69 $9.25 $2.95 $10.98 $2.39 $22.50 $22.50 $21.96 $21.96 $10.98 $8.49 $1.69 $11.75 $2.95 $8.75 $11.25 $8.75 $9.25 $8.75 $8.19 $10.58 $8.75 $2.95 $1.50 $2.50 $2.50 $9.25 $4.45 $9.25 $11.25 $8.69 $8.69 $3.89 $8.69 $1.69 $4.45 $9.25 $11.25 $4.45 $2.15 $8.19 $8.69 $4.45 $11.25 $1.25 $11.25 $8.75 $11.89 $11.75 $11.75 $9.25 $8.75 $2.15 $1.50 $11.75 $4.45 $3.99 $8.99 $10.98 $2.39 $1.25 $2.95 $8.75 $2.95 $9.25 $9.25 $9.25 $8.75 $2.15 $9.25 $9.25 $3.39 $8.49 $8.49 $2.39 $10.98 $1.09 $1.09 $8.49 $11.25 $1.25 $8.99 $1.09 $8.75 $2.15 $1.50 $3.99 $8.49 $8.75 $2.15 $1.50 $8.49 $10.98 $2.18 $2.18 $8.75 $2.15 $1.50 $4.45 $9.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $6.49 $33.75 $33.75 $33.75 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $3.00 $3.00 $11.25 $11.75 $10.98 $2.39 $2.50 $2.50 $8.75 $4.45 $16.98 $16.98 $8.75 $6.49 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $16.98 $16.98 $8.99 $8.99 $8.49 $9.25 $8.75 $2.95 $11.48 $2.39 $8.99 $2.39 $11.25 $4.45 $8.75 $9.25 $6.49 $26.25 $26.25 $26.25 $8.75 $26.25 $26.25 $26.25 $8.75 $8.75 $11.25 $11.25 $2.15 $1.25 $11.75 $8.75 $2.15 $1.50 $11.25 $2.15 $8.99 $2.39 $11.25 $11.25 $2.15 $11.25 $11.25 $8.75 $4.78 $4.78 $21.96 $21.96 $8.49 $2.39 $9.25 $2.95 $16.98 $16.98 $8.19 $3.89 $8.99 $1.09 $8.99 $3.39 $9.25 $4.45 $10.98 $10.98 $17.50 $17.50 $11.25 $1.25 $8.75 $11.75 $4.45 $11.25 $11.25 $8.99 $1.09 $10.98 $8.49 $1.69 $11.25 $9.25 $16.98 $16.98 $8.75 $4.45 $11.25 $6.49 $11.75 $9.25 $9.25 $8.75 $4.45 $2.50 $2.50 $8.75 $11.75 $8.75 $9.25 $11.75 $9.25 $8.75 $9.25 $8.75 $11.25 $11.75 $9.25 $8.75 $11.75 $8.49 $1.09 $1.09 $8.49 $1.09 $1.69 $11.25 $1.25 $8.75 $2.15 $1.50 $8.49 $1.69 $1.25 $8.75 $2.95 $8.49 $3.99 $8.49 $8.49 $8.75 $11.25 $2.15 $1.50 $11.75 $8.99 $1.09 $10.98 $10.98 $11.25 $1.25 $8.75 $4.45 $4.45 $1.25 $11.89 $8.99 $8.99 $11.25 $4.45 $23.50 $23.50 $8.49 $3.99 $9.25 $4.45 $4.45 $9.25 $8.75 $4.45 $8.75 $8.75 $11.75 $6.49 $17.50 $17.50 $4.45 $8.75 $2.95 $1.50 $8.75 $8.75 $8.75 $4.45 $11.25 $11.25 $11.75 $11.25 $2.95 $11.25 $4.45 $3.00 $3.00 $1.25 $2.95 $9.25 $8.99 $2.39 $6.49 $8.75 $8.90 $8.90 $11.48 $1.09 $10.98 $9.25 $9.25 $11.25 $8.75 $11.75 $11.25 $11.25 $1.25 $9.25 $4.45 $9.25 $6.49 $11.75 $11.75 $8.99 $2.39 $8.49 $8.49 $9.25 $9.25 $1.25 $8.75 $2.95 $11.75 $2.15 $8.49 $8.49 $8.69 $16.38 $16.38 $8.19 $3.89 $2.29 $11.75 $8.75 $8.75 $8.75 $4.45 $8.49 $8.49 $9.25 $8.75 $6.49 $2.95 $11.25 $11.25 $2.15 $9.25 $11.75 $21.96 $21.96 $8.49 $3.39 $1.69 $8.49 $8.75 $4.45 $8.49 $3.99 $11.25 $8.75 $11.25 $2.15 $11.75 $4.45 $11.25 $9.25 $8.75 $18.50 $18.50 $1.50 $8.75 $2.15 $11.48 $2.18 $2.18 $3.99 $11.25 $1.50 $8.99 $2.39 $11.75 $1.50 $11.25 $6.49 $4.45 $11.25 $8.49 $3.99 $2.50 $2.50 $8.75 $9.25 $3.99 $8.99 $8.75 $6.49 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $16.98 $16.98 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $8.75 $8.75 $11.25 $11.25 $8.49 $1.09 $1.69 $1.25 $9.25 $2.95 $8.69 $8.19 $8.49 $2.39 $8.49 $2.39 $10.98 $8.99 $8.99 $1.69 $8.49 $8.75 $8.75 $11.25 $4.45 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $2.15 $1.50 $1.50 $8.99 $1.09 $8.75 $4.45 $8.75 $8.75 $11.25 $4.30 $4.30 $8.49 $1.69 $1.09 $1.09 $8.75 $2.15 $1.50 $8.99 $8.49 $3.99 $8.75 $2.15 $1.25 $9.25 $2.95 $11.25 $4.45 $9.25 $2.95 $3.99 $8.99 $8.49 $8.75 $8.75 $11.25 $9.25 $8.75 $4.45 $11.25 $1.25 $9.25 $11.25 $4.45 $2.95 $10.98 $8.75 $8.75 $18.50 $18.50 $9.25 $9.25 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $16.98 $16.98 $11.25 $2.95 $8.75 $2.15 $1.50 $8.49 $2.39 $9.25 $2.15 $1.25 $8.19 $8.69 $8.19 $8.19 $8.75 $2.95 $1.25 $9.25 $2.95 $11.25 $8.75 $11.25 $11.25 $8.99 $1.09 $9.25 $9.25 $4.45 $8.49 $3.99 $2.39 $1.09 $8.99 $8.49 $8.75 $8.75 $11.25 $11.75 $4.45 $2.50 $2.50 $8.75 $8.49 $3.39 $8.75 $9.25 $4.45 $1.25 $11.25 $2.15 $4.45 $2.50 $2.50 $8.99 $3.99 $8.75 $2.15 $11.75 $11.75 $1.25 $8.75 $9.39 $11.25 $9.25 $9.25 $2.95 $9.25 $4.45 $1.25 $9.25 $8.75 $11.75 $1.50 $8.75 $4.45 $8.99 $1.09 $9.25 $2.95 $8.99 $1.69 $8.69 $1.69 $11.25 $4.45 $8.75 $8.75 $4.45 $11.25 $8.75 $2.95 $1.50 $8.19 $8.69 $1.09 $1.69 $8.49 $8.75 $2.95 $1.25 $8.49 $3.99 $10.98 $3.39 $11.25 $11.25 $2.15 $18.50 $18.50 $8.49 $8.49 $11.25 $1.50 $8.49 $2.39 $8.99 $2.39 $11.75 $4.45 $17.50 $17.50 $9.25 $9.25 $8.75 $4.45 $3.75 $3.75 $3.75 $8.75 $4.45 $11.75 $2.95 $1.25 $4.45 $8.75 $1.25 $1.50 $9.25 $11.25 $11.25 $11.25 $11.25 $9.25 $11.25 $4.45 $8.75 $9.25 $8.75 $8.75 $4.45 $8.75 $1.25 $2.15 $8.75 $8.75 $4.30 $4.30 $8.75 $1.25 $2.95 $9.25 $2.95 $4.45 $11.25 $11.25 $9.25 $9.25 $4.50 $4.50 $4.50 $11.75 $1.25 $11.75 $11.75 $1.25 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $2.15 $1.25 $11.25 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $11.25 $4.45 $4.45 $2.95 $11.25 $2.15 $1.25 $1.50 $8.75 $11.25 $2.95 $11.25 $1.25 $2.15 $11.25 $9.25 $6.49 $1.25 $8.75 $2.15 $8.75 $6.49 $11.25 $1.50 $8.75 $4.45 $8.75 $4.45 $9.25 $9.25 $1.25 $1.25 $8.75 $4.50 $4.50 $4.50 $11.25 $1.25 $1.50 $9.25 $2.15 $11.25 $4.45 $11.25 $4.45 $8.75 $4.45 $9.25 $4.45 $1.25 $11.25 $4.45 $8.75 $4.45 $8.75 $2.15 $8.75 $4.45 $8.75 $11.75 $1.50 $11.25 $4.45 $8.75 $2.15 $1.50 $8.75 $4.45 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $1.50 $8.75 $2.15 $11.75 $2.15 $9.25 $2.95 $18.50 $18.50 $1.25 $4.45 $8.50 $8.50 $8.50 $8.50 $11.25 $11.89 $1.25 $9.39 $4.45 $8.75 $2.15 $1.50 $11.75 $8.75 $9.25 $9.25 $4.45 $1.25 $11.25 $9.25 $2.95 $8.75 $8.75 $2.15 $8.75 $11.25 $11.25 $11.25 $11.75 $11.25 $2.15 $11.25 $2.15 $8.99 $8.99 $8.75 $9.25 $9.25 $11.25 $8.75 $4.45 $8.75 $1.25 $4.45 $11.25 $1.25 $8.75 $8.75 $11.25 $8.75 $1.25 $1.25 $1.25 $9.25 $11.75 $2.15 $8.75 $4.45 $8.75 $4.45 $11.25 $9.25 $8.75 $9.25 $4.45 $11.75 $4.45 $1.25 $4.45 $11.75 $9.25 $11.25 $2.15 $23.50 $23.50 $9.25 $2.15 $18.50 $18.50 $8.75 $5.90 $5.90 $11.89 $4.45 $8.75 $4.45 $9.25 $2.95 $11.25 $2.95 $11.25 $11.25 $11.25 $2.95 $11.75 $9.25 $9.25 $2.95 $11.25 $2.15 $9.25 $8.90 $8.90 $8.75 $11.25 $11.25 $11.25 $8.75 $2.15 $1.25 $11.25 $1.25 $8.75 $4.45 $1.25 $11.25 $11.25 $11.75 $1.25 $11.25 $11.25 $11.25 $8.75 $8.75 $18.50 $18.50 $11.75 $1.25 $4.45 $9.25 $6.49 $4.45 $8.75 $11.25 $6.49 $11.75 $8.75 $9.25 $11.25 $2.15 $8.75 $4.45 $11.25 $4.45 $8.75 $9.25 $4.45 $1.25 $1.25 $8.75 $11.25 $11.75 $2.15 $11.75 $4.45 $11.75 $1.25 $11.75 $11.75 $11.25 $4.30 $4.30 $9.39 $9.39 $8.75 $1.25 $9.25 $4.45 $11.25 $1.25 $8.75 $1.25 $2.95 $11.25 $4.45 $8.75 $1.50 $4.45 $4.45 $9.25 $8.75 $2.95 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $8.75 $4.30 $4.30 $9.25 $9.39 $4.45 $9.25 $1.25 $22.50 $22.50 $4.45 $2.95 $2.15 $23.50 $23.50 $11.75 $2.15 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $8.75 $11.75 $11.25 $8.75 $4.45 $11.75 $1.50 $8.75 $8.75 $11.75 $9.25 $11.25 $4.45 $11.75 $9.25 $4.45 $11.25 $8.75 $8.75 $2.15 $1.50 $8.75 $4.45 $9.25 $8.75 $1.50 $1.25 $1.25 $1.25 $8.75 $2.95 $11.25 $11.25 $1.50 $11.75 $11.25 $2.15 $9.25 $8.75 $11.75 $2.95 $1.50 $8.75 $1.50 $1.25 $8.75 $11.75 $11.25 $11.25 $11.75 $11.25 $11.75 $8.75 $17.80 $17.80 $17.80 $17.80 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $1.25 $11.25 $1.25 $2.15 $11.25 $2.50 $2.50 $9.25 $1.25 $11.25 $2.95 $11.75 $2.15 $11.25 $1.50 $8.99 $1.99 $11.49 $8.75 $4.45 $1.25 $8.75 $4.45 $1.25 $1.50 $11.75 $8.75 $8.75 $11.25 $6.49 $11.75 $8.75 $2.15 $1.25 $6.49 $8.75 $4.45 $8.75 $4.45 $8.75 $11.25 $4.45 $6.49 $9.25 $8.75 $1.25 $4.45 $11.25 $8.75 $1.50 $8.75 $1.50 $1.25 $9.25 $9.39 $4.45 $9.25 $8.75 $4.45 $1.25 $11.25 $11.75 $8.75 $11.25 $9.25 $8.75 $11.25 $2.50 $2.50 $17.50 $17.50 $9.25 $4.45 $11.25 $1.25 $8.75 $4.45 $1.50 $8.75 $1.50 $1.25 $9.39 $8.75 $8.75 $4.45 $11.25 $1.25 $9.25 $4.45 $11.25 $8.75 $3.00 $3.00 $8.75 $2.15 $1.25 $11.25 $11.25 $4.45 $11.25 $11.25 $8.75 $11.75 $11.75 $11.75 $8.75 $4.45 $1.25 $1.50 $8.75 $4.45 $1.25 $9.25 $9.25 $8.75 $4.45 $1.25 $11.75 $11.25 $1.25 $11.75 $11.25 $9.25 $2.15 $1.50 $8.75 $4.45 $11.75 $11.75 $11.25 $8.75 $8.75 ' to numeric\n\n\n\n\n# Solution 2\n\nchipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 # Solution 2\n----&gt; 3 chipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6549, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n   6541 @doc(make_doc(\"mean\", ndim=1))\n   6542 def mean(\n   6543     self,\n   (...)\n   6547     **kwargs,\n   6548 ):\n-&gt; 6549     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n  12413 def mean(\n  12414     self,\n  12415     axis: Axis | None = 0,\n   (...)\n  12418     **kwargs,\n  12419 ) -&gt; Series | float:\n&gt; 12420     return self._stat_function(\n  12421         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n  12422     )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n  12373 nv.validate_func(name, (), kwargs)\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n&gt; 12377 return self._reduce(\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n  12379 )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6457, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   6452     # GH#47500 - change to TypeError to match other methods\n   6453     raise TypeError(\n   6454         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n   6455         \"with non-numeric dtypes.\"\n   6456     )\n-&gt; 6457 return op(delegate, skipna=skipna, **kwds)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147, in bottleneck_switch.__call__.&lt;locals&gt;.f(values, axis, skipna, **kwds)\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n    146 else:\n--&gt; 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n    149 return result\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404, in _datetimelike_compat.&lt;locals&gt;.new_func(values, axis, skipna, mask, **kwargs)\n    401 if datetimelike and mask is None:\n    402     mask = isna(values)\n--&gt; 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n    406 if datetimelike:\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:720, in nanmean(values, axis, skipna, mask)\n    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    719 the_sum = values.sum(axis, dtype=dtype_sum)\n--&gt; 720 the_sum = _ensure_numeric(the_sum)\n    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n    723     count = cast(np.ndarray, count)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1701, in _ensure_numeric(x)\n   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n   1699     if isinstance(x, str):\n   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n-&gt; 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n   1702     try:\n   1703         x = float(x)\n\nTypeError: Could not convert string '$2.39 $3.39 $3.39 $2.39 $16.98 $16.98 $10.98 $1.69 $11.75 $9.25 $9.25 $4.45 $8.75 $8.75 $11.25 $4.45 $2.39 $8.49 $8.49 $2.18 $2.18 $8.75 $4.45 $8.99 $3.39 $10.98 $3.39 $2.39 $8.49 $8.99 $1.09 $8.49 $2.39 $8.99 $1.69 $8.99 $1.09 $8.75 $8.75 $4.45 $2.95 $11.75 $2.15 $4.45 $11.25 $11.75 $8.75 $10.98 $8.99 $3.39 $8.99 $3.99 $8.99 $2.18 $2.18 $10.98 $1.09 $8.99 $2.39 $9.25 $11.25 $11.75 $2.15 $4.45 $9.25 $11.25 $8.75 $8.99 $8.99 $3.39 $8.99 $10.98 $8.99 $1.69 $8.99 $3.99 $8.75 $4.45 $8.75 $8.75 $2.15 $8.75 $11.25 $2.15 $9.25 $8.75 $8.75 $9.25 $8.49 $8.99 $1.09 $9.25 $2.95 $11.75 $11.75 $9.25 $11.75 $4.45 $9.25 $4.45 $11.75 $8.75 $8.75 $4.45 $8.99 $8.99 $3.99 $8.49 $3.39 $8.99 $1.09 $9.25 $4.45 $8.75 $2.95 $4.45 $2.39 $8.49 $8.99 $8.49 $1.09 $8.99 $3.99 $8.75 $9.25 $4.45 $11.25 $4.45 $8.99 $1.09 $9.25 $2.95 $4.45 $11.75 $4.45 $8.49 $2.39 $10.98 $22.50 $22.50 $11.75 $4.45 $11.25 $4.45 $11.25 $4.45 $11.25 $11.25 $11.75 $9.25 $4.45 $11.48 $17.98 $17.98 $1.69 $17.50 $17.50 $4.45 $8.49 $2.39 $17.50 $17.50 $4.45 $4.45 $11.25 $11.75 $10.98 $8.49 $10.98 $2.18 $2.18 $11.48 $8.49 $2.39 $4.45 $11.25 $11.75 $8.75 $8.49 $2.18 $2.18 $8.49 $3.39 $8.49 $8.99 $10.98 $11.48 $8.49 $1.09 $1.09 $9.25 $8.75 $2.95 $9.25 $4.45 $11.25 $11.48 $8.49 $8.49 $8.99 $2.39 $11.25 $8.75 $2.95 $1.09 $8.99 $8.49 $2.39 $10.98 $1.09 $3.99 $11.25 $8.75 $8.49 $3.39 $8.75 $9.25 $2.15 $11.25 $11.25 $11.25 $4.45 $22.50 $22.50 $4.45 $11.75 $8.75 $17.50 $17.50 $8.75 $9.25 $8.75 $2.15 $9.25 $4.30 $4.30 $8.75 $11.25 $2.15 $8.99 $1.09 $8.99 $3.99 $8.75 $2.95 $2.95 $11.75 $5.90 $5.90 $9.25 $9.25 $11.75 $9.25 $2.95 $17.50 $17.50 $8.75 $9.25 $10.98 $8.99 $1.09 $1.09 $1.09 $8.99 $10.98 $1.09 $8.75 $8.75 $9.25 $9.25 $8.75 $8.75 $8.99 $8.99 $8.99 $1.09 $11.75 $1.25 $8.99 $2.39 $9.25 $2.95 $8.99 $3.99 $8.49 $2.39 $8.49 $8.49 $8.49 $1.69 $8.49 $3.99 $8.99 $1.69 $1.09 $23.78 $23.78 $17.50 $17.50 $2.15 $8.75 $9.25 $9.25 $8.75 $4.45 $8.75 $11.25 $11.25 $1.25 $9.25 $4.45 $11.25 $11.75 $11.75 $6.49 $8.99 $2.39 $8.49 $2.39 $11.25 $8.75 $2.15 $8.99 $1.69 $8.75 $11.25 $2.15 $4.45 $8.75 $8.49 $8.99 $17.50 $17.50 $8.49 $1.09 $1.09 $8.75 $1.25 $2.15 $11.08 $8.49 $8.49 $8.99 $2.39 $8.75 $2.15 $1.50 $11.25 $2.15 $8.49 $8.49 $11.75 $9.25 $11.75 $1.25 $11.25 $8.75 $4.45 $6.49 $9.25 $2.95 $11.25 $4.45 $1.25 $1.25 $8.49 $2.39 $2.18 $2.18 $8.49 $2.18 $2.18 $22.16 $22.16 $17.50 $17.50 $8.75 $2.95 $6.49 $8.99 $3.39 $3.39 $8.99 $8.49 $11.25 $2.15 $11.25 $2.95 $11.25 $1.25 $8.99 $1.09 $8.75 $8.75 $9.25 $2.95 $11.75 $1.50 $8.99 $1.09 $11.25 $1.25 $1.25 $11.25 $11.75 $2.15 $8.99 $1.69 $11.75 $6.49 $8.75 $9.25 $11.25 $4.45 $1.25 $11.25 $4.45 $8.49 $8.99 $8.49 $8.99 $11.25 $1.25 $11.75 $1.25 $11.75 $9.25 $4.45 $11.25 $2.15 $32.94 $32.94 $32.94 $1.25 $11.25 $11.48 $1.69 $1.09 $17.50 $17.50 $4.45 $6.49 $9.25 $8.75 $9.25 $9.25 $8.75 $8.75 $2.15 $2.95 $17.50 $17.50 $10.98 $11.48 $11.48 $3.39 $8.99 $1.69 $8.99 $1.09 $10.98 $3.39 $8.99 $1.09 $9.25 $8.75 $11.25 $4.45 $2.95 $9.25 $22.20 $22.20 $22.20 $8.49 $8.99 $8.75 $8.75 $11.75 $8.75 $11.25 $9.25 $11.25 $11.25 $8.75 $11.25 $2.95 $1.25 $8.49 $1.69 $11.75 $11.25 $8.75 $8.75 $4.45 $8.49 $3.99 $8.49 $3.99 $11.48 $1.69 $1.09 $11.25 $1.50 $10.58 $1.69 $9.25 $11.25 $8.75 $9.25 $11.25 $11.25 $8.75 $11.75 $8.75 $8.75 $8.75 $2.15 $11.25 $11.75 $2.50 $2.50 $4.45 $9.25 $4.45 $11.25 $8.49 $3.99 $9.25 $9.25 $11.25 $9.25 $11.75 $11.25 $1.25 $23.50 $23.50 $1.25 $8.99 $8.49 $7.40 $7.40 $8.75 $1.25 $4.45 $8.75 $2.15 $8.75 $4.45 $7.40 $7.40 $7.40 $8.99 $3.99 $8.99 $1.69 $8.75 $8.75 $8.75 $8.75 $11.25 $11.25 $2.95 $8.75 $18.50 $18.50 $8.49 $3.99 $2.95 $9.25 $9.25 $3.00 $3.00 $1.25 $8.75 $9.25 $4.45 $8.75 $11.25 $4.45 $10.98 $22.16 $22.16 $4.45 $8.75 $9.25 $6.49 $9.25 $11.25 $8.75 $9.25 $2.15 $9.25 $4.45 $9.25 $2.95 $9.25 $8.75 $9.25 $1.25 $1.25 $8.75 $8.75 $9.25 $4.45 $11.75 $11.75 $11.75 $9.25 $9.25 $16.98 $16.98 $2.39 $3.39 $3.39 $9.25 $11.75 $11.25 $2.15 $8.75 $9.25 $4.45 $10.98 $11.25 $9.25 $22.50 $22.50 $9.25 $2.95 $1.50 $11.48 $8.49 $1.69 $8.49 $8.49 $8.49 $6.78 $6.78 $11.75 $4.45 $8.75 $4.45 $11.89 $9.39 $8.75 $2.95 $1.25 $9.25 $8.75 $23.78 $23.78 $8.75 $9.25 $2.15 $2.15 $1.25 $8.49 $3.99 $10.98 $1.09 $8.75 $4.45 $8.75 $11.75 $2.95 $4.45 $9.25 $8.75 $8.49 $3.99 $22.50 $22.50 $11.25 $1.25 $8.75 $8.75 $18.50 $18.50 $6.49 $8.75 $8.75 $4.45 $8.49 $3.99 $8.99 $1.09 $8.49 $2.39 $11.48 $1.69 $2.50 $2.50 $9.25 $1.50 $17.50 $17.50 $2.95 $8.75 $4.45 $11.75 $8.75 $8.49 $1.69 $8.49 $3.99 $8.99 $8.99 $3.99 $8.99 $11.25 $4.45 $1.25 $3.99 $10.98 $7.40 $3.00 $7.40 $4.00 $8.49 $3.99 $9.25 $4.45 $11.25 $1.25 $11.75 $1.25 $11.25 $2.15 $11.25 $4.45 $3.75 $3.75 $3.75 $11.75 $8.99 $2.39 $8.75 $4.45 $1.25 $8.99 $8.49 $2.18 $2.18 $8.49 $2.18 $2.18 $1.09 $8.75 $2.95 $1.25 $1.50 $11.25 $9.25 $2.95 $1.25 $8.49 $3.99 $11.48 $3.99 $8.49 $11.25 $1.25 $8.99 $1.69 $11.25 $1.25 $6.49 $8.75 $9.25 $8.75 $2.95 $8.75 $11.75 $8.69 $8.69 $2.29 $3.99 $8.49 $8.75 $8.75 $1.25 $11.75 $11.25 $11.25 $11.25 $1.25 $9.25 $11.75 $6.49 $3.99 $8.49 $11.25 $2.15 $11.25 $11.89 $8.99 $1.69 $8.99 $8.99 $3.99 $8.99 $9.25 $9.25 $2.15 $7.40 $7.40 $8.75 $8.75 $9.25 $4.45 $11.25 $1.25 $11.75 $11.25 $1.25 $3.99 $8.49 $8.49 $8.49 $8.99 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $8.75 $2.95 $1.25 $9.25 $1.25 $2.15 $11.25 $1.25 $4.45 $8.75 $2.50 $2.50 $8.90 $8.90 $8.75 $8.75 $8.75 $11.25 $11.25 $10.98 $3.99 $10.98 $3.99 $1.69 $8.99 $9.25 $8.75 $8.99 $1.09 $9.25 $2.95 $8.75 $9.25 $3.99 $8.49 $8.75 $8.75 $22.50 $22.50 $10.98 $3.27 $3.27 $3.27 $3.99 $8.99 $1.09 $11.08 $8.75 $4.45 $11.08 $3.99 $8.49 $4.30 $4.30 $9.25 $8.75 $11.25 $11.25 $9.25 $8.49 $8.99 $8.49 $8.75 $2.95 $4.45 $9.25 $2.95 $9.25 $8.75 $11.25 $4.45 $16.98 $16.98 $8.49 $2.39 $11.25 $3.75 $3.75 $3.75 $9.25 $4.45 $9.25 $9.25 $4.45 $8.75 $9.25 $8.75 $9.25 $9.25 $9.25 $11.48 $8.99 $22.50 $22.50 $11.75 $11.25 $1.25 $8.75 $2.15 $1.25 $11.25 $8.75 $1.25 $11.25 $1.50 $11.25 $11.25 $9.25 $6.49 $8.90 $8.90 $8.75 $4.45 $11.25 $1.25 $17.50 $17.50 $9.25 $8.75 $11.75 $3.00 $3.00 $8.49 $8.49 $10.98 $8.99 $3.99 $8.75 $4.45 $8.99 $1.69 $11.75 $8.75 $11.25 $4.45 $11.75 $1.25 $11.75 $2.95 $8.99 $8.99 $2.18 $2.18 $17.98 $17.98 $8.99 $8.49 $1.69 $11.75 $11.25 $2.95 $3.75 $3.75 $3.75 $9.25 $11.75 $8.75 $2.15 $1.50 $8.49 $8.49 $3.39 $8.69 $3.89 $8.75 $4.45 $8.75 $11.25 $2.15 $8.75 $8.49 $1.69 $8.49 $8.49 $1.25 $8.75 $11.75 $11.75 $8.99 $1.09 $8.75 $4.45 $8.75 $2.95 $8.75 $2.15 $3.99 $8.49 $8.99 $3.99 $8.49 $1.69 $1.09 $8.99 $1.09 $9.25 $8.75 $8.99 $2.39 $1.25 $1.25 $11.25 $11.25 $9.25 $9.25 $11.25 $1.50 $3.99 $8.49 $11.25 $9.25 $11.25 $17.50 $17.50 $8.75 $8.90 $8.90 $8.75 $8.75 $8.99 $2.39 $11.25 $9.25 $2.15 $11.25 $1.25 $11.75 $1.25 $11.25 $11.75 $1.25 $11.25 $11.25 $8.49 $10.98 $8.75 $1.25 $8.75 $8.49 $8.49 $1.50 $1.50 $8.75 $4.45 $11.25 $1.25 $11.75 $8.49 $2.39 $9.25 $4.45 $9.25 $8.75 $8.99 $1.69 $17.50 $17.50 $2.39 $8.99 $8.99 $11.25 $4.45 $8.75 $4.45 $9.25 $6.49 $10.98 $8.49 $8.49 $1.09 $1.69 $9.25 $4.45 $8.75 $1.25 $2.95 $3.99 $8.49 $11.75 $11.75 $2.15 $11.48 $8.75 $2.15 $1.25 $11.25 $2.15 $1.25 $8.75 $8.75 $6.49 $1.69 $8.99 $8.75 $11.75 $10.98 $1.09 $8.49 $3.39 $8.75 $2.15 $1.25 $11.48 $10.98 $10.98 $8.49 $2.95 $9.25 $9.25 $11.75 $4.45 $11.48 $11.25 $8.75 $4.45 $1.69 $8.99 $8.75 $4.45 $1.50 $11.75 $2.15 $8.99 $2.39 $8.75 $2.95 $1.25 $8.75 $2.15 $1.25 $2.18 $2.18 $2.18 $2.18 $11.48 $8.75 $2.95 $11.75 $11.75 $1.25 $10.58 $8.99 $2.39 $11.75 $4.45 $11.25 $11.25 $17.50 $17.50 $8.75 $8.75 $8.75 $22.50 $22.50 $9.25 $8.75 $4.45 $11.75 $1.25 $11.25 $11.25 $2.95 $8.99 $1.69 $11.25 $4.45 $8.75 $6.49 $8.75 $4.45 $9.25 $4.45 $11.75 $11.75 $4.45 $11.89 $11.75 $11.25 $2.95 $1.50 $4.45 $8.75 $8.99 $1.09 $8.99 $1.09 $3.99 $11.48 $8.49 $9.25 $4.45 $11.48 $9.25 $2.95 $9.25 $8.49 $8.99 $8.99 $8.49 $8.75 $2.95 $4.45 $11.89 $10.58 $8.19 $1.69 $8.75 $2.15 $1.25 $17.50 $17.50 $6.49 $9.25 $2.15 $8.75 $4.45 $8.75 $1.25 $11.48 $11.48 $8.99 $2.18 $2.18 $8.49 $8.99 $2.39 $2.39 $2.18 $2.18 $8.75 $4.45 $11.25 $9.25 $9.25 $11.25 $11.25 $4.45 $2.95 $11.75 $8.49 $8.49 $8.99 $1.69 $9.25 $11.25 $11.75 $9.25 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $10.98 $11.25 $4.45 $10.98 $8.49 $8.99 $3.39 $3.99 $8.99 $1.09 $1.09 $2.39 $17.50 $17.50 $4.45 $11.25 $11.25 $4.45 $9.25 $4.45 $8.75 $2.15 $1.25 $11.89 $2.95 $11.75 $1.25 $11.25 $4.45 $11.48 $11.48 $2.95 $9.25 $8.75 $9.25 $2.95 $11.25 $1.25 $11.75 $1.25 $8.99 $2.39 $1.25 $11.25 $1.25 $11.25 $8.49 $3.99 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $8.75 $11.80 $11.80 $11.80 $11.80 $8.90 $8.90 $5.90 $5.90 $6.49 $10.98 $17.98 $17.98 $2.39 $9.25 $8.75 $2.15 $8.75 $4.45 $8.49 $1.69 $8.19 $8.69 $10.98 $3.99 $11.48 $11.48 $4.45 $8.75 $6.49 $8.75 $8.75 $9.25 $1.25 $4.45 $8.49 $1.69 $9.25 $4.45 $8.99 $1.09 $11.25 $2.95 $11.08 $11.08 $3.89 $10.98 $11.25 $8.75 $11.25 $9.25 $4.30 $4.30 $8.75 $8.49 $3.99 $1.69 $8.99 $8.49 $1.69 $11.75 $11.25 $11.89 $9.25 $2.95 $9.25 $2.95 $8.75 $4.45 $4.45 $8.75 $10.98 $11.48 $8.49 $9.25 $4.45 $11.75 $11.89 $8.99 $8.49 $8.75 $9.25 $8.75 $8.75 $11.75 $11.75 $4.45 $11.25 $11.75 $2.50 $2.50 $8.99 $1.69 $11.75 $2.15 $1.25 $9.25 $8.75 $8.90 $8.90 $9.25 $2.95 $8.75 $11.25 $8.90 $8.90 $11.25 $11.75 $11.48 $1.69 $3.39 $9.25 $2.95 $8.99 $1.69 $8.49 $10.98 $11.25 $2.95 $8.99 $1.69 $8.75 $2.15 $1.25 $8.75 $2.95 $9.25 $2.50 $2.50 $11.25 $1.25 $11.75 $2.50 $2.50 $11.25 $1.50 $8.75 $1.25 $2.95 $11.48 $11.48 $8.75 $8.75 $2.15 $11.75 $1.25 $9.25 $9.25 $6.49 $11.75 $8.49 $8.49 $1.09 $10.98 $8.75 $1.25 $2.15 $11.25 $1.50 $11.25 $11.25 $8.49 $8.49 $8.75 $1.50 $1.25 $1.50 $8.75 $2.50 $2.50 $2.15 $7.40 $7.40 $4.00 $9.25 $9.39 $9.25 $9.25 $9.39 $11.25 $8.90 $8.90 $11.25 $6.00 $6.00 $6.00 $6.00 $11.25 $11.25 $11.25 $22.50 $22.50 $11.48 $1.09 $8.49 $8.49 $17.50 $17.50 $11.25 $1.50 $9.25 $8.75 $3.99 $8.49 $8.75 $8.75 $8.75 $8.75 $8.75 $11.75 $1.50 $11.25 $11.25 $2.95 $8.99 $10.98 $9.25 $8.75 $4.45 $8.49 $1.09 $2.39 $8.75 $8.75 $11.48 $8.99 $8.49 $8.49 $2.39 $10.98 $8.49 $3.99 $11.75 $4.45 $8.75 $2.15 $1.25 $10.98 $8.99 $11.25 $1.50 $8.75 $2.15 $1.25 $8.75 $9.25 $8.75 $11.25 $1.50 $8.75 $1.25 $4.45 $10.98 $8.75 $2.95 $1.25 $8.75 $2.95 $1.25 $8.49 $8.49 $2.39 $11.25 $1.25 $8.75 $8.75 $9.25 $8.75 $11.89 $1.25 $8.75 $2.15 $1.25 $8.99 $1.09 $8.75 $4.45 $26.25 $26.25 $26.25 $8.75 $4.45 $11.75 $2.95 $8.75 $8.75 $11.75 $8.75 $11.25 $11.25 $11.25 $4.45 $1.25 $8.49 $8.49 $8.49 $8.99 $8.99 $2.39 $2.39 $3.99 $8.75 $4.45 $2.15 $9.25 $1.25 $11.25 $11.75 $8.75 $4.45 $11.25 $2.15 $8.75 $4.45 $8.75 $8.75 $1.25 $11.25 $2.15 $8.75 $5.90 $5.90 $11.75 $1.25 $9.25 $3.75 $3.75 $3.75 $8.75 $1.25 $4.45 $11.75 $4.45 $8.75 $23.50 $23.50 $8.75 $2.95 $8.75 $8.75 $11.89 $4.45 $2.95 $1.25 $8.75 $4.45 $2.95 $1.25 $8.75 $2.15 $1.25 $11.75 $2.95 $8.99 $3.39 $9.25 $9.25 $17.50 $17.50 $2.95 $11.89 $1.50 $11.25 $2.95 $9.25 $11.25 $11.25 $2.95 $8.75 $9.25 $4.30 $4.30 $8.75 $8.75 $11.25 $8.75 $4.30 $4.30 $8.75 $1.25 $2.15 $8.49 $8.49 $3.39 $3.39 $10.98 $10.98 $2.39 $11.25 $11.75 $11.75 $1.25 $5.90 $5.90 $8.75 $11.25 $9.25 $4.45 $1.50 $3.39 $8.99 $2.39 $11.25 $2.15 $11.25 $11.75 $11.75 $4.45 $11.75 $4.45 $9.25 $8.75 $8.49 $8.99 $8.49 $8.99 $11.75 $8.75 $8.49 $3.99 $3.89 $11.08 $8.49 $8.99 $8.49 $8.49 $8.49 $11.25 $2.15 $17.50 $17.50 $8.75 $2.95 $8.49 $8.49 $10.98 $1.09 $11.25 $2.15 $2.95 $1.25 $8.75 $9.25 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $8.99 $3.99 $11.75 $2.15 $8.99 $3.39 $9.25 $8.75 $11.25 $11.25 $4.45 $8.75 $2.15 $1.25 $11.75 $4.45 $9.25 $2.95 $8.49 $8.49 $11.25 $8.75 $4.45 $11.25 $11.25 $11.25 $11.25 $4.45 $8.49 $1.69 $8.49 $3.39 $8.75 $11.25 $9.25 $8.75 $11.25 $11.25 $11.75 $11.25 $11.75 $11.25 $11.75 $21.96 $21.96 $10.98 $1.69 $11.48 $8.99 $8.49 $1.69 $9.25 $2.15 $1.50 $11.25 $1.50 $8.75 $8.75 $2.95 $8.49 $1.69 $8.75 $2.95 $1.25 $11.25 $2.15 $11.08 $8.49 $8.49 $8.49 $11.75 $1.25 $11.75 $8.75 $8.75 $8.75 $4.45 $11.25 $1.50 $23.50 $23.50 $11.75 $6.49 $8.75 $4.45 $6.49 $8.75 $2.50 $2.50 $2.15 $8.49 $2.39 $8.75 $11.75 $4.45 $8.99 $10.98 $9.25 $2.95 $9.25 $9.25 $11.75 $8.75 $8.75 $8.75 $10.98 $11.25 $9.25 $8.75 $8.75 $2.15 $11.25 $2.15 $4.45 $11.75 $8.49 $2.39 $9.25 $1.25 $1.25 $1.25 $1.25 $8.75 $2.15 $8.49 $1.69 $11.25 $1.50 $8.75 $8.75 $8.49 $3.99 $8.99 $1.09 $11.25 $1.25 $8.49 $2.39 $8.49 $8.75 $9.25 $11.25 $4.45 $11.25 $11.89 $8.99 $8.49 $8.75 $4.45 $8.75 $11.75 $11.75 $8.90 $8.90 $9.39 $2.95 $8.49 $3.99 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $8.75 $4.45 $8.49 $8.99 $8.49 $3.99 $8.75 $8.75 $2.95 $8.75 $17.50 $17.50 $9.25 $2.95 $8.75 $6.49 $4.30 $4.30 $8.75 $8.75 $2.15 $1.50 $8.49 $8.49 $2.39 $9.25 $4.45 $6.49 $11.75 $4.45 $10.98 $1.69 $9.39 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $11.25 $9.25 $8.75 $11.25 $8.75 $11.25 $2.50 $2.50 $2.50 $2.50 $6.00 $6.00 $6.00 $6.00 $8.90 $8.90 $5.90 $5.90 $11.25 $11.25 $8.49 $10.98 $8.75 $2.15 $1.50 $9.25 $1.25 $1.50 $2.15 $1.25 $8.75 $2.95 $8.49 $3.99 $11.25 $4.30 $4.30 $11.75 $2.15 $18.50 $18.50 $8.49 $2.39 $8.75 $4.45 $11.75 $8.99 $3.99 $9.25 $9.25 $1.50 $8.75 $2.95 $6.49 $11.75 $8.49 $8.99 $8.75 $4.45 $6.49 $22.50 $22.50 $9.25 $2.95 $8.49 $1.69 $10.98 $8.75 $4.45 $11.25 $2.95 $8.99 $8.49 $2.39 $11.75 $6.49 $11.25 $11.75 $2.95 $8.99 $1.69 $8.99 $2.18 $2.18 $1.09 $8.99 $8.99 $1.09 $8.99 $8.99 $8.49 $10.98 $1.09 $11.75 $9.25 $11.25 $11.25 $2.15 $11.25 $8.75 $4.45 $2.95 $11.75 $1.50 $8.99 $10.98 $2.39 $8.75 $2.15 $9.25 $1.50 $8.75 $2.15 $3.99 $8.99 $6.49 $8.75 $8.90 $8.90 $8.99 $3.99 $17.50 $17.50 $11.25 $1.25 $10.98 $9.25 $4.45 $1.25 $3.00 $3.00 $11.25 $4.45 $4.45 $2.95 $9.25 $11.25 $2.15 $11.25 $11.25 $4.45 $2.95 $9.25 $11.25 $1.25 $8.75 $2.95 $1.25 $8.75 $4.45 $11.48 $11.48 $8.49 $2.39 $11.25 $11.75 $2.15 $1.50 $2.15 $8.75 $11.25 $8.90 $8.90 $11.25 $11.25 $1.25 $4.45 $9.25 $9.25 $8.75 $9.25 $8.75 $8.75 $9.25 $8.75 $11.75 $11.75 $8.75 $8.75 $8.90 $8.90 $2.95 $10.98 $8.49 $8.49 $10.98 $8.99 $8.99 $11.75 $17.50 $17.50 $11.75 $3.99 $8.49 $10.98 $1.69 $17.50 $17.50 $8.99 $2.39 $8.99 $2.39 $1.25 $8.75 $2.95 $11.75 $11.25 $17.50 $17.50 $8.49 $8.49 $2.39 $11.25 $1.50 $8.75 $3.00 $3.00 $1.25 $8.75 $4.45 $11.75 $11.75 $4.45 $21.96 $21.96 $8.75 $4.45 $8.75 $11.25 $9.25 $8.99 $2.39 $9.25 $8.75 $10.98 $8.49 $3.99 $3.39 $11.75 $1.50 $4.45 $9.25 $8.75 $1.25 $11.75 $8.75 $1.50 $8.75 $8.75 $2.15 $1.50 $8.75 $2.95 $8.75 $8.75 $17.50 $17.50 $8.75 $6.49 $4.45 $11.25 $11.25 $4.30 $4.30 $8.75 $11.25 $4.45 $8.99 $2.39 $9.25 $9.25 $9.25 $4.45 $11.75 $11.25 $2.95 $2.15 $11.25 $11.25 $8.75 $2.15 $1.50 $9.25 $4.45 $10.98 $8.99 $2.18 $2.18 $8.75 $4.45 $1.25 $8.99 $2.39 $4.45 $8.75 $10.98 $11.75 $1.50 $10.98 $8.99 $8.49 $3.99 $8.99 $8.49 $3.99 $8.49 $8.49 $8.99 $11.25 $11.25 $10.98 $10.98 $10.98 $2.39 $3.39 $8.75 $1.25 $2.95 $11.75 $1.50 $10.98 $1.69 $4.45 $8.75 $8.75 $8.75 $8.75 $4.45 $9.25 $8.75 $11.25 $8.75 $3.99 $8.99 $8.49 $11.25 $11.25 $8.75 $4.45 $8.75 $4.45 $1.25 $8.75 $8.75 $1.50 $2.15 $11.75 $11.75 $11.75 $11.75 $11.75 $1.50 $8.75 $9.25 $1.25 $8.75 $2.15 $8.99 $1.09 $4.45 $11.25 $11.75 $2.15 $8.75 $8.75 $1.25 $9.25 $2.15 $11.75 $11.25 $8.75 $11.25 $4.45 $8.49 $1.69 $8.75 $8.75 $8.99 $8.49 $9.25 $11.25 $2.95 $4.45 $11.75 $6.49 $11.48 $8.99 $4.36 $4.36 $4.36 $4.36 $11.48 $8.99 $8.49 $11.48 $8.75 $2.15 $1.50 $8.99 $1.69 $11.25 $1.25 $9.25 $9.25 $8.75 $9.25 $8.90 $8.90 $2.15 $9.25 $10.98 $8.49 $8.75 $9.25 $4.30 $4.30 $9.25 $8.75 $8.75 $2.15 $1.25 $8.75 $1.25 $8.75 $5.90 $5.90 $9.25 $8.75 $9.25 $4.45 $9.25 $11.75 $2.50 $2.50 $9.25 $2.15 $9.25 $1.50 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $10.98 $8.75 $8.75 $8.75 $2.15 $1.25 $10.98 $8.75 $2.15 $1.50 $8.75 $2.95 $1.25 $9.25 $9.25 $8.49 $2.39 $8.75 $4.45 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.99 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $11.25 $11.25 $8.75 $1.50 $2.15 $16.98 $16.98 $11.75 $1.50 $8.75 $4.30 $4.30 $1.50 $8.75 $2.95 $1.25 $1.25 $9.25 $4.45 $11.25 $8.75 $4.45 $8.75 $2.15 $1.25 $10.98 $1.69 $8.75 $1.25 $8.75 $1.25 $11.25 $8.75 $8.75 $8.49 $1.69 $9.25 $11.75 $8.49 $2.39 $9.25 $2.95 $6.49 $8.75 $8.75 $9.25 $8.75 $6.78 $6.78 $17.98 $17.98 $3.39 $11.75 $11.25 $8.75 $4.45 $11.75 $9.25 $8.75 $6.49 $8.99 $2.39 $8.75 $11.25 $11.75 $4.45 $8.75 $2.15 $9.25 $9.25 $9.25 $11.89 $11.75 $11.25 $9.25 $9.25 $8.75 $8.75 $8.49 $1.69 $1.09 $11.25 $1.50 $11.25 $11.25 $11.75 $1.50 $8.49 $8.99 $22.50 $22.50 $8.75 $4.30 $4.30 $8.75 $11.25 $2.15 $11.25 $2.95 $4.45 $11.25 $8.49 $3.39 $2.39 $11.75 $2.15 $11.75 $8.99 $2.39 $8.75 $11.75 $11.89 $1.25 $7.50 $7.50 $7.50 $7.50 $7.50 $11.89 $1.09 $8.49 $2.39 $8.75 $8.75 $8.75 $8.75 $9.25 $11.25 $8.75 $8.90 $8.90 $9.25 $8.75 $8.75 $11.75 $3.00 $3.00 $1.50 $11.25 $11.75 $8.99 $10.98 $4.45 $8.75 $2.15 $9.25 $11.25 $4.45 $1.69 $10.98 $9.25 $11.75 $9.25 $4.45 $10.98 $3.99 $8.49 $1.25 $9.25 $4.45 $10.98 $8.75 $8.75 $11.75 $11.25 $8.49 $11.48 $4.45 $1.25 $11.25 $8.99 $1.09 $2.39 $11.25 $2.15 $8.75 $4.45 $8.49 $1.69 $10.98 $1.69 $9.25 $4.45 $11.25 $8.75 $11.25 $11.75 $11.25 $22.50 $22.50 $8.49 $2.39 $2.50 $2.50 $8.75 $8.75 $9.25 $9.25 $11.25 $8.99 $1.09 $8.99 $1.69 $11.75 $1.25 $21.96 $21.96 $8.75 $2.15 $1.25 $8.75 $11.25 $9.25 $11.25 $8.75 $8.75 $11.25 $2.15 $8.99 $1.09 $1.69 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $11.48 $8.49 $8.49 $4.78 $4.78 $9.25 $1.25 $1.25 $1.25 $11.25 $11.25 $11.75 $4.45 $11.25 $4.45 $8.99 $1.09 $11.25 $2.15 $11.25 $9.25 $11.75 $11.25 $11.25 $9.25 $2.95 $11.25 $4.45 $8.75 $2.95 $2.95 $11.25 $1.50 $10.98 $16.98 $16.98 $18.50 $18.50 $10.98 $3.99 $1.09 $9.25 $9.25 $8.75 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $1.25 $4.45 $9.25 $8.75 $8.75 $17.50 $17.50 $4.45 $9.39 $1.25 $2.95 $11.25 $8.75 $8.75 $11.25 $2.15 $8.90 $8.90 $11.25 $11.89 $10.98 $11.25 $4.45 $11.25 $11.25 $8.49 $10.98 $8.49 $3.39 $9.25 $8.75 $2.95 $3.00 $3.00 $9.39 $11.75 $2.95 $1.50 $11.25 $11.75 $8.75 $2.15 $1.50 $8.49 $3.39 $11.75 $1.25 $17.50 $17.50 $11.25 $1.25 $8.75 $2.95 $1.25 $11.25 $11.75 $13.35 $13.35 $13.35 $11.25 $11.75 $11.25 $11.25 $4.45 $11.25 $8.49 $3.39 $9.25 $2.95 $4.78 $4.78 $2.39 $3.99 $8.99 $8.99 $11.25 $11.25 $8.75 $11.25 $2.95 $4.45 $9.25 $8.75 $4.45 $8.49 $8.49 $10.98 $10.98 $3.99 $11.75 $8.75 $11.75 $4.45 $1.50 $1.25 $8.49 $8.49 $8.75 $8.75 $8.75 $9.25 $8.75 $2.95 $1.25 $11.25 $1.50 $11.25 $4.45 $9.25 $8.75 $8.75 $9.25 $8.75 $4.45 $1.50 $8.75 $8.75 $8.49 $1.69 $8.75 $2.15 $9.25 $2.15 $1.50 $11.25 $11.75 $2.15 $6.49 $9.25 $9.25 $11.25 $11.25 $11.75 $11.75 $11.75 $11.25 $8.75 $2.15 $1.25 $11.75 $9.25 $11.25 $8.75 $5.90 $5.90 $8.75 $4.45 $9.25 $9.25 $4.45 $11.25 $4.45 $11.25 $8.75 $2.15 $11.89 $11.25 $8.75 $2.95 $1.50 $8.75 $4.30 $4.30 $8.75 $11.25 $11.75 $11.75 $2.15 $11.25 $8.99 $1.09 $8.49 $8.49 $8.49 $3.39 $8.99 $10.98 $3.99 $11.75 $2.15 $8.75 $4.45 $2.50 $2.50 $11.48 $1.09 $8.49 $8.49 $16.98 $16.98 $3.99 $10.98 $1.09 $8.75 $2.95 $8.75 $8.75 $2.95 $9.25 $11.25 $2.15 $9.25 $4.45 $4.45 $9.25 $11.75 $11.75 $2.15 $9.25 $8.75 $11.25 $6.49 $8.75 $11.25 $2.95 $10.98 $3.99 $1.50 $9.25 $2.15 $8.75 $11.25 $11.89 $4.45 $1.50 $1.25 $8.75 $8.75 $4.45 $11.25 $11.75 $8.49 $1.09 $1.09 $1.69 $8.99 $3.39 $8.99 $1.69 $8.49 $8.99 $3.27 $3.27 $3.27 $8.99 $8.99 $1.09 $10.98 $1.69 $3.99 $8.49 $1.09 $8.75 $8.75 $11.75 $8.75 $9.25 $8.75 $3.39 $8.99 $8.99 $2.39 $9.25 $8.75 $9.25 $9.25 $8.99 $3.99 $2.39 $8.49 $1.09 $8.49 $8.99 $3.39 $11.25 $1.25 $8.99 $3.99 $8.75 $8.90 $8.90 $6.49 $8.75 $9.25 $11.25 $11.25 $11.25 $1.25 $8.75 $9.25 $4.45 $1.25 $8.75 $1.25 $2.15 $17.98 $17.98 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $1.50 $8.75 $8.75 $11.08 $8.99 $1.69 $8.99 $1.69 $10.98 $3.99 $3.39 $11.75 $2.15 $11.75 $2.95 $8.75 $8.75 $11.75 $11.25 $11.75 $11.25 $4.45 $11.25 $1.25 $2.18 $2.18 $2.18 $2.18 $2.39 $8.49 $8.99 $2.39 $11.25 $8.75 $11.75 $11.75 $11.25 $4.45 $2.15 $8.19 $10.58 $4.45 $9.25 $1.09 $8.99 $11.25 $1.50 $8.99 $3.99 $4.45 $11.75 $2.15 $11.25 $8.75 $4.45 $8.75 $9.25 $6.45 $6.45 $6.45 $8.75 $11.25 $11.25 $8.75 $11.75 $21.96 $21.96 $8.99 $5.07 $5.07 $5.07 $8.49 $9.25 $11.25 $4.45 $3.39 $8.49 $8.99 $8.49 $17.50 $17.50 $22.96 $22.96 $8.75 $11.25 $11.89 $11.25 $8.49 $1.69 $1.09 $8.99 $8.99 $9.25 $8.75 $9.25 $2.95 $8.49 $3.99 $8.99 $8.49 $7.17 $7.17 $7.17 $8.49 $8.99 $17.50 $17.50 $9.25 $9.25 $11.25 $1.25 $8.99 $1.09 $8.75 $4.45 $11.25 $2.15 $11.75 $11.25 $11.25 $8.75 $8.75 $4.45 $1.25 $11.75 $11.75 $2.50 $2.50 $8.49 $8.99 $2.18 $2.18 $11.25 $4.45 $11.25 $11.75 $8.49 $8.99 $1.69 $1.09 $8.99 $8.99 $11.25 $6.49 $11.25 $8.75 $4.45 $8.99 $1.69 $11.48 $11.75 $2.50 $2.50 $8.49 $1.09 $1.09 $1.69 $8.49 $2.39 $11.75 $1.25 $8.49 $1.69 $8.49 $1.69 $11.75 $4.45 $8.75 $8.75 $4.45 $8.75 $11.25 $11.25 $8.75 $7.98 $7.98 $8.49 $1.09 $8.49 $3.99 $8.49 $3.99 $8.99 $3.99 $11.25 $4.45 $8.49 $2.39 $8.49 $2.39 $3.99 $8.49 $1.25 $11.25 $4.45 $9.25 $4.45 $1.09 $8.99 $3.99 $11.25 $8.90 $8.90 $9.25 $11.25 $8.75 $11.25 $11.25 $11.25 $11.25 $11.25 $8.99 $8.49 $8.75 $8.75 $4.45 $16.98 $16.98 $11.75 $11.25 $9.25 $4.45 $9.25 $2.95 $8.49 $1.69 $3.75 $3.75 $3.75 $4.45 $9.25 $1.50 $11.25 $11.48 $11.25 $2.15 $8.75 $9.39 $8.49 $3.99 $8.19 $2.29 $11.48 $1.69 $11.48 $3.99 $8.49 $1.69 $9.25 $2.95 $8.49 $1.69 $11.25 $4.45 $9.39 $9.25 $8.75 $8.75 $4.45 $11.89 $4.45 $4.45 $8.75 $8.75 $8.75 $2.15 $8.75 $3.75 $3.75 $3.75 $9.25 $11.25 $4.45 $6.49 $16.98 $16.98 $18.50 $18.50 $2.50 $2.50 $2.95 $3.99 $8.49 $8.19 $11.08 $6.49 $11.75 $2.39 $8.99 $1.09 $11.25 $4.45 $11.25 $8.99 $1.69 $21.96 $21.96 $2.18 $2.18 $8.99 $8.99 $2.39 $8.69 $1.69 $8.90 $8.90 $2.50 $2.50 $8.75 $8.99 $1.09 $8.49 $8.49 $8.75 $4.45 $17.50 $17.50 $8.75 $9.25 $8.49 $2.39 $8.75 $4.45 $11.25 $11.25 $11.75 $8.75 $8.49 $8.49 $8.49 $8.99 $8.75 $4.45 $11.48 $8.75 $1.25 $2.15 $9.25 $4.45 $11.75 $2.15 $11.25 $8.99 $2.39 $8.69 $8.69 $11.75 $2.95 $11.75 $1.50 $9.25 $4.45 $1.50 $11.48 $8.99 $2.39 $11.25 $11.89 $2.15 $1.25 $11.75 $4.45 $8.75 $8.75 $11.25 $4.45 $11.25 $2.15 $4.45 $8.49 $1.09 $3.99 $11.25 $11.25 $8.49 $2.39 $8.99 $2.39 $11.25 $2.15 $8.75 $2.95 $1.25 $8.75 $11.25 $17.50 $17.50 $11.75 $11.75 $11.25 $11.25 $4.45 $2.50 $2.50 $8.75 $8.99 $8.99 $1.69 $8.99 $1.69 $11.25 $1.25 $11.08 $8.69 $8.99 $1.09 $11.25 $11.25 $2.95 $1.25 $8.75 $1.25 $8.75 $8.75 $2.15 $1.25 $8.49 $3.99 $8.49 $2.39 $8.49 $7.17 $7.17 $7.17 $8.75 $4.45 $11.48 $8.75 $8.75 $11.48 $8.75 $9.25 $8.49 $3.99 $1.50 $11.25 $11.25 $8.75 $8.75 $4.45 $9.25 $4.45 $8.75 $8.75 $4.30 $4.30 $2.95 $8.75 $4.50 $4.50 $4.50 $9.25 $11.25 $4.45 $11.25 $11.25 $8.75 $9.25 $8.75 $2.15 $1.25 $8.75 $2.15 $1.25 $8.99 $8.49 $8.75 $8.75 $1.25 $11.75 $4.50 $4.50 $4.50 $8.75 $8.75 $3.99 $3.39 $8.49 $2.39 $8.99 $1.50 $11.25 $11.25 $8.75 $8.75 $2.15 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $26.07 $26.07 $26.07 $11.75 $1.50 $8.99 $8.99 $11.48 $9.25 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $8.75 $2.15 $1.25 $11.89 $8.75 $11.75 $4.45 $18.50 $18.50 $9.25 $9.39 $8.49 $2.39 $8.49 $1.69 $1.09 $8.99 $8.49 $2.18 $2.18 $11.25 $1.25 $8.49 $3.39 $8.49 $3.99 $11.25 $8.75 $8.49 $1.69 $16.98 $16.98 $9.25 $9.25 $11.75 $1.25 $11.25 $8.75 $8.49 $8.49 $8.75 $1.25 $1.25 $1.25 $11.25 $12.98 $12.98 $11.75 $11.75 $4.45 $11.25 $11.75 $10.98 $8.49 $8.49 $2.39 $9.25 $11.25 $8.75 $2.95 $1.50 $11.25 $2.95 $9.25 $2.95 $9.25 $8.75 $11.25 $8.75 $8.75 $4.45 $11.25 $1.25 $8.75 $2.95 $2.50 $2.50 $9.25 $9.25 $9.25 $6.49 $17.50 $17.50 $8.49 $1.69 $8.49 $3.99 $8.75 $2.95 $2.95 $8.49 $8.99 $8.99 $1.09 $8.75 $4.45 $1.25 $11.25 $11.75 $11.25 $9.25 $11.25 $1.50 $11.25 $2.15 $10.98 $8.75 $11.75 $2.95 $11.25 $11.25 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $1.25 $11.25 $1.50 $2.15 $8.75 $8.75 $11.25 $8.75 $8.75 $11.25 $4.45 $8.49 $8.49 $8.49 $8.49 $8.99 $1.69 $2.39 $1.09 $1.09 $11.25 $2.95 $35.25 $35.25 $35.25 $8.75 $2.95 $1.25 $11.25 $2.15 $9.25 $4.45 $8.75 $8.75 $8.75 $4.45 $1.25 $11.89 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $8.69 $8.69 $9.25 $2.95 $10.98 $2.39 $22.50 $22.50 $21.96 $21.96 $10.98 $8.49 $1.69 $11.75 $2.95 $8.75 $11.25 $8.75 $9.25 $8.75 $8.19 $10.58 $8.75 $2.95 $1.50 $2.50 $2.50 $9.25 $4.45 $9.25 $11.25 $8.69 $8.69 $3.89 $8.69 $1.69 $4.45 $9.25 $11.25 $4.45 $2.15 $8.19 $8.69 $4.45 $11.25 $1.25 $11.25 $8.75 $11.89 $11.75 $11.75 $9.25 $8.75 $2.15 $1.50 $11.75 $4.45 $3.99 $8.99 $10.98 $2.39 $1.25 $2.95 $8.75 $2.95 $9.25 $9.25 $9.25 $8.75 $2.15 $9.25 $9.25 $3.39 $8.49 $8.49 $2.39 $10.98 $1.09 $1.09 $8.49 $11.25 $1.25 $8.99 $1.09 $8.75 $2.15 $1.50 $3.99 $8.49 $8.75 $2.15 $1.50 $8.49 $10.98 $2.18 $2.18 $8.75 $2.15 $1.50 $4.45 $9.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $6.49 $33.75 $33.75 $33.75 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $3.00 $3.00 $11.25 $11.75 $10.98 $2.39 $2.50 $2.50 $8.75 $4.45 $16.98 $16.98 $8.75 $6.49 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $16.98 $16.98 $8.99 $8.99 $8.49 $9.25 $8.75 $2.95 $11.48 $2.39 $8.99 $2.39 $11.25 $4.45 $8.75 $9.25 $6.49 $26.25 $26.25 $26.25 $8.75 $26.25 $26.25 $26.25 $8.75 $8.75 $11.25 $11.25 $2.15 $1.25 $11.75 $8.75 $2.15 $1.50 $11.25 $2.15 $8.99 $2.39 $11.25 $11.25 $2.15 $11.25 $11.25 $8.75 $4.78 $4.78 $21.96 $21.96 $8.49 $2.39 $9.25 $2.95 $16.98 $16.98 $8.19 $3.89 $8.99 $1.09 $8.99 $3.39 $9.25 $4.45 $10.98 $10.98 $17.50 $17.50 $11.25 $1.25 $8.75 $11.75 $4.45 $11.25 $11.25 $8.99 $1.09 $10.98 $8.49 $1.69 $11.25 $9.25 $16.98 $16.98 $8.75 $4.45 $11.25 $6.49 $11.75 $9.25 $9.25 $8.75 $4.45 $2.50 $2.50 $8.75 $11.75 $8.75 $9.25 $11.75 $9.25 $8.75 $9.25 $8.75 $11.25 $11.75 $9.25 $8.75 $11.75 $8.49 $1.09 $1.09 $8.49 $1.09 $1.69 $11.25 $1.25 $8.75 $2.15 $1.50 $8.49 $1.69 $1.25 $8.75 $2.95 $8.49 $3.99 $8.49 $8.49 $8.75 $11.25 $2.15 $1.50 $11.75 $8.99 $1.09 $10.98 $10.98 $11.25 $1.25 $8.75 $4.45 $4.45 $1.25 $11.89 $8.99 $8.99 $11.25 $4.45 $23.50 $23.50 $8.49 $3.99 $9.25 $4.45 $4.45 $9.25 $8.75 $4.45 $8.75 $8.75 $11.75 $6.49 $17.50 $17.50 $4.45 $8.75 $2.95 $1.50 $8.75 $8.75 $8.75 $4.45 $11.25 $11.25 $11.75 $11.25 $2.95 $11.25 $4.45 $3.00 $3.00 $1.25 $2.95 $9.25 $8.99 $2.39 $6.49 $8.75 $8.90 $8.90 $11.48 $1.09 $10.98 $9.25 $9.25 $11.25 $8.75 $11.75 $11.25 $11.25 $1.25 $9.25 $4.45 $9.25 $6.49 $11.75 $11.75 $8.99 $2.39 $8.49 $8.49 $9.25 $9.25 $1.25 $8.75 $2.95 $11.75 $2.15 $8.49 $8.49 $8.69 $16.38 $16.38 $8.19 $3.89 $2.29 $11.75 $8.75 $8.75 $8.75 $4.45 $8.49 $8.49 $9.25 $8.75 $6.49 $2.95 $11.25 $11.25 $2.15 $9.25 $11.75 $21.96 $21.96 $8.49 $3.39 $1.69 $8.49 $8.75 $4.45 $8.49 $3.99 $11.25 $8.75 $11.25 $2.15 $11.75 $4.45 $11.25 $9.25 $8.75 $18.50 $18.50 $1.50 $8.75 $2.15 $11.48 $2.18 $2.18 $3.99 $11.25 $1.50 $8.99 $2.39 $11.75 $1.50 $11.25 $6.49 $4.45 $11.25 $8.49 $3.99 $2.50 $2.50 $8.75 $9.25 $3.99 $8.99 $8.75 $6.49 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $16.98 $16.98 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $8.75 $8.75 $11.25 $11.25 $8.49 $1.09 $1.69 $1.25 $9.25 $2.95 $8.69 $8.19 $8.49 $2.39 $8.49 $2.39 $10.98 $8.99 $8.99 $1.69 $8.49 $8.75 $8.75 $11.25 $4.45 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $2.15 $1.50 $1.50 $8.99 $1.09 $8.75 $4.45 $8.75 $8.75 $11.25 $4.30 $4.30 $8.49 $1.69 $1.09 $1.09 $8.75 $2.15 $1.50 $8.99 $8.49 $3.99 $8.75 $2.15 $1.25 $9.25 $2.95 $11.25 $4.45 $9.25 $2.95 $3.99 $8.99 $8.49 $8.75 $8.75 $11.25 $9.25 $8.75 $4.45 $11.25 $1.25 $9.25 $11.25 $4.45 $2.95 $10.98 $8.75 $8.75 $18.50 $18.50 $9.25 $9.25 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $16.98 $16.98 $11.25 $2.95 $8.75 $2.15 $1.50 $8.49 $2.39 $9.25 $2.15 $1.25 $8.19 $8.69 $8.19 $8.19 $8.75 $2.95 $1.25 $9.25 $2.95 $11.25 $8.75 $11.25 $11.25 $8.99 $1.09 $9.25 $9.25 $4.45 $8.49 $3.99 $2.39 $1.09 $8.99 $8.49 $8.75 $8.75 $11.25 $11.75 $4.45 $2.50 $2.50 $8.75 $8.49 $3.39 $8.75 $9.25 $4.45 $1.25 $11.25 $2.15 $4.45 $2.50 $2.50 $8.99 $3.99 $8.75 $2.15 $11.75 $11.75 $1.25 $8.75 $9.39 $11.25 $9.25 $9.25 $2.95 $9.25 $4.45 $1.25 $9.25 $8.75 $11.75 $1.50 $8.75 $4.45 $8.99 $1.09 $9.25 $2.95 $8.99 $1.69 $8.69 $1.69 $11.25 $4.45 $8.75 $8.75 $4.45 $11.25 $8.75 $2.95 $1.50 $8.19 $8.69 $1.09 $1.69 $8.49 $8.75 $2.95 $1.25 $8.49 $3.99 $10.98 $3.39 $11.25 $11.25 $2.15 $18.50 $18.50 $8.49 $8.49 $11.25 $1.50 $8.49 $2.39 $8.99 $2.39 $11.75 $4.45 $17.50 $17.50 $9.25 $9.25 $8.75 $4.45 $3.75 $3.75 $3.75 $8.75 $4.45 $11.75 $2.95 $1.25 $4.45 $8.75 $1.25 $1.50 $9.25 $11.25 $11.25 $11.25 $11.25 $9.25 $11.25 $4.45 $8.75 $9.25 $8.75 $8.75 $4.45 $8.75 $1.25 $2.15 $8.75 $8.75 $4.30 $4.30 $8.75 $1.25 $2.95 $9.25 $2.95 $4.45 $11.25 $11.25 $9.25 $9.25 $4.50 $4.50 $4.50 $11.75 $1.25 $11.75 $11.75 $1.25 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $2.15 $1.25 $11.25 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $11.25 $4.45 $4.45 $2.95 $11.25 $2.15 $1.25 $1.50 $8.75 $11.25 $2.95 $11.25 $1.25 $2.15 $11.25 $9.25 $6.49 $1.25 $8.75 $2.15 $8.75 $6.49 $11.25 $1.50 $8.75 $4.45 $8.75 $4.45 $9.25 $9.25 $1.25 $1.25 $8.75 $4.50 $4.50 $4.50 $11.25 $1.25 $1.50 $9.25 $2.15 $11.25 $4.45 $11.25 $4.45 $8.75 $4.45 $9.25 $4.45 $1.25 $11.25 $4.45 $8.75 $4.45 $8.75 $2.15 $8.75 $4.45 $8.75 $11.75 $1.50 $11.25 $4.45 $8.75 $2.15 $1.50 $8.75 $4.45 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $1.50 $8.75 $2.15 $11.75 $2.15 $9.25 $2.95 $18.50 $18.50 $1.25 $4.45 $8.50 $8.50 $8.50 $8.50 $11.25 $11.89 $1.25 $9.39 $4.45 $8.75 $2.15 $1.50 $11.75 $8.75 $9.25 $9.25 $4.45 $1.25 $11.25 $9.25 $2.95 $8.75 $8.75 $2.15 $8.75 $11.25 $11.25 $11.25 $11.75 $11.25 $2.15 $11.25 $2.15 $8.99 $8.99 $8.75 $9.25 $9.25 $11.25 $8.75 $4.45 $8.75 $1.25 $4.45 $11.25 $1.25 $8.75 $8.75 $11.25 $8.75 $1.25 $1.25 $1.25 $9.25 $11.75 $2.15 $8.75 $4.45 $8.75 $4.45 $11.25 $9.25 $8.75 $9.25 $4.45 $11.75 $4.45 $1.25 $4.45 $11.75 $9.25 $11.25 $2.15 $23.50 $23.50 $9.25 $2.15 $18.50 $18.50 $8.75 $5.90 $5.90 $11.89 $4.45 $8.75 $4.45 $9.25 $2.95 $11.25 $2.95 $11.25 $11.25 $11.25 $2.95 $11.75 $9.25 $9.25 $2.95 $11.25 $2.15 $9.25 $8.90 $8.90 $8.75 $11.25 $11.25 $11.25 $8.75 $2.15 $1.25 $11.25 $1.25 $8.75 $4.45 $1.25 $11.25 $11.25 $11.75 $1.25 $11.25 $11.25 $11.25 $8.75 $8.75 $18.50 $18.50 $11.75 $1.25 $4.45 $9.25 $6.49 $4.45 $8.75 $11.25 $6.49 $11.75 $8.75 $9.25 $11.25 $2.15 $8.75 $4.45 $11.25 $4.45 $8.75 $9.25 $4.45 $1.25 $1.25 $8.75 $11.25 $11.75 $2.15 $11.75 $4.45 $11.75 $1.25 $11.75 $11.75 $11.25 $4.30 $4.30 $9.39 $9.39 $8.75 $1.25 $9.25 $4.45 $11.25 $1.25 $8.75 $1.25 $2.95 $11.25 $4.45 $8.75 $1.50 $4.45 $4.45 $9.25 $8.75 $2.95 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $8.75 $4.30 $4.30 $9.25 $9.39 $4.45 $9.25 $1.25 $22.50 $22.50 $4.45 $2.95 $2.15 $23.50 $23.50 $11.75 $2.15 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $8.75 $11.75 $11.25 $8.75 $4.45 $11.75 $1.50 $8.75 $8.75 $11.75 $9.25 $11.25 $4.45 $11.75 $9.25 $4.45 $11.25 $8.75 $8.75 $2.15 $1.50 $8.75 $4.45 $9.25 $8.75 $1.50 $1.25 $1.25 $1.25 $8.75 $2.95 $11.25 $11.25 $1.50 $11.75 $11.25 $2.15 $9.25 $8.75 $11.75 $2.95 $1.50 $8.75 $1.50 $1.25 $8.75 $11.75 $11.25 $11.25 $11.75 $11.25 $11.75 $8.75 $17.80 $17.80 $17.80 $17.80 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $1.25 $11.25 $1.25 $2.15 $11.25 $2.50 $2.50 $9.25 $1.25 $11.25 $2.95 $11.75 $2.15 $11.25 $1.50 $8.99 $1.99 $11.49 $8.75 $4.45 $1.25 $8.75 $4.45 $1.25 $1.50 $11.75 $8.75 $8.75 $11.25 $6.49 $11.75 $8.75 $2.15 $1.25 $6.49 $8.75 $4.45 $8.75 $4.45 $8.75 $11.25 $4.45 $6.49 $9.25 $8.75 $1.25 $4.45 $11.25 $8.75 $1.50 $8.75 $1.50 $1.25 $9.25 $9.39 $4.45 $9.25 $8.75 $4.45 $1.25 $11.25 $11.75 $8.75 $11.25 $9.25 $8.75 $11.25 $2.50 $2.50 $17.50 $17.50 $9.25 $4.45 $11.25 $1.25 $8.75 $4.45 $1.50 $8.75 $1.50 $1.25 $9.39 $8.75 $8.75 $4.45 $11.25 $1.25 $9.25 $4.45 $11.25 $8.75 $3.00 $3.00 $8.75 $2.15 $1.25 $11.25 $11.25 $4.45 $11.25 $11.25 $8.75 $11.75 $11.75 $11.75 $8.75 $4.45 $1.25 $1.50 $8.75 $4.45 $1.25 $9.25 $9.25 $8.75 $4.45 $1.25 $11.75 $11.25 $1.25 $11.75 $11.25 $9.25 $2.15 $1.50 $8.75 $4.45 $11.75 $11.75 $11.25 $8.75 $8.75 ' to numeric\n\n\n\n\n\nStep 17. How many different items are sold?\n\nchipo.item_name.value_counts().count()\n\n50\n\n\n\nSource: Ex2 - Getting and Knowing your Data\n\n01Occupation-Exercises-with-solutions\n\n\nEx3 - Getting and Knowing your Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called users and use the ‘user_id’ as index\n\nusers = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user', \n                      sep='|', index_col='user_id')\n\n\n\nStep 4. See the first 25 entries\n\nusers.head(25)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n1\n24\nM\ntechnician\n85711\n\n\n2\n53\nF\nother\n94043\n\n\n3\n23\nM\nwriter\n32067\n\n\n4\n24\nM\ntechnician\n43537\n\n\n5\n33\nF\nother\n15213\n\n\n6\n42\nM\nexecutive\n98101\n\n\n7\n57\nM\nadministrator\n91344\n\n\n8\n36\nM\nadministrator\n05201\n\n\n9\n29\nM\nstudent\n01002\n\n\n10\n53\nM\nlawyer\n90703\n\n\n11\n39\nF\nother\n30329\n\n\n12\n28\nF\nother\n06405\n\n\n13\n47\nM\neducator\n29206\n\n\n14\n45\nM\nscientist\n55106\n\n\n15\n49\nF\neducator\n97301\n\n\n16\n21\nM\nentertainment\n10309\n\n\n17\n30\nM\nprogrammer\n06355\n\n\n18\n35\nF\nother\n37212\n\n\n19\n40\nM\nlibrarian\n02138\n\n\n20\n42\nF\nhomemaker\n95660\n\n\n21\n26\nM\nwriter\n30068\n\n\n22\n25\nM\nwriter\n40206\n\n\n23\n30\nF\nartist\n48197\n\n\n24\n21\nF\nartist\n94533\n\n\n25\n39\nM\nengineer\n55107\n\n\n\n\n\n\n\n\n\nStep 5. See the last 10 entries\n\nusers.tail(10)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n934\n61\nM\nengineer\n22902\n\n\n935\n42\nM\ndoctor\n66221\n\n\n936\n24\nM\nother\n32789\n\n\n937\n48\nM\neducator\n98072\n\n\n938\n38\nF\ntechnician\n55038\n\n\n939\n26\nF\nstudent\n33319\n\n\n940\n32\nM\nadministrator\n02215\n\n\n941\n20\nM\nstudent\n97229\n\n\n942\n48\nF\nlibrarian\n78209\n\n\n943\n22\nM\nstudent\n77841\n\n\n\n\n\n\n\n\n\nStep 6. What is the number of observations in the dataset?\n\nusers.shape[0]\n\n943\n\n\n\n\nStep 7. What is the number of columns in the dataset?\n\nusers.shape[1]\n\n4\n\n\n\n\nStep 8. Print the name of all the columns.\n\nusers.columns\n\nIndex(['age', 'gender', 'occupation', 'zip_code'], dtype='object')\n\n\n\n\nStep 9. How is the dataset indexed?\n\nusers.index\n\nIndex([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       934, 935, 936, 937, 938, 939, 940, 941, 942, 943],\n      dtype='int64', name='user_id', length=943)\n\n\n\n\nStep 10. What is the data type of each column?\n\nusers.dtypes\n\nage            int64\ngender        object\noccupation    object\nzip_code      object\ndtype: object\n\n\n\n\nStep 11. Print only the occupation column\n\nusers.occupation\n\n#or\n\nusers['occupation']\n\nuser_id\n1         technician\n2              other\n3             writer\n4         technician\n5              other\n           ...      \n939          student\n940    administrator\n941          student\n942        librarian\n943          student\nName: occupation, Length: 943, dtype: object\n\n\n\n\nStep 12. How many different occupations are in this dataset?\n\nusers.occupation.nunique()\n\n21\n\n\n\n\nStep 13. What is the most frequent occupation?\n\nusers.occupation.value_counts().head(1).index[0]\n\n'student'\n\n\n\n\nStep 14. Summarize the DataFrame.\n\nusers.describe() \n\n\n\n\n\n\n\n\nage\n\n\n\n\ncount\n943.000000\n\n\nmean\n34.051962\n\n\nstd\n12.192740\n\n\nmin\n7.000000\n\n\n25%\n25.000000\n\n\n50%\n31.000000\n\n\n75%\n43.000000\n\n\nmax\n73.000000\n\n\n\n\n\n\n\n\n\nStep 15. Summarize all the columns\n\nusers.describe(include = \"all\")\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\n\n\ncount\n943.000000\n943\n943\n943\n\n\nunique\nNaN\n2\n21\n795\n\n\ntop\nNaN\nM\nstudent\n55414\n\n\nfreq\nNaN\n670\n196\n9\n\n\nmean\n34.051962\nNaN\nNaN\nNaN\n\n\nstd\n12.192740\nNaN\nNaN\nNaN\n\n\nmin\n7.000000\nNaN\nNaN\nNaN\n\n\n25%\n25.000000\nNaN\nNaN\nNaN\n\n\n50%\n31.000000\nNaN\nNaN\nNaN\n\n\n75%\n43.000000\nNaN\nNaN\nNaN\n\n\nmax\n73.000000\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nStep 16. Summarize only the occupation column\n\nusers.occupation.describe()\n\ncount         943\nunique         21\ntop       student\nfreq          196\nName: occupation, dtype: object\n\n\n\n\nStep 17. What is the mean age of users?\n\nround(users.age.mean())\n\n34\n\n\n\n\nStep 18. What is the age with least occurrence?\n\nusers.age.value_counts().tail()\n\nage\n7     1\n66    1\n11    1\n10    1\n73    1\nName: count, dtype: int64\n\n\n\nSource: Ex3 - Getting and Knowing your Data\n\n01World-Food-Facts-Exercises-with-solutions\n\n\nExercise 1\n\nStep 1. Go to https://www.kaggle.com/openfoodfacts/world-food-facts/data\n\n\nStep 2. Download the dataset to your computer and unzip it.\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 3. Use the tsv file and assign it to a dataframe called food\n\nfood = pd.read_csv('E:\\Yang Fan\\Lab 1\\en.openfoodfacts.org.products.tsv', sep='\\t')\n\n\n\nStep 4. See the first 5 entries\n\nfood.head()\n\n\n\n\n\n\n\n\ncode\nurl\ncreator\ncreated_t\ncreated_datetime\nlast_modified_t\nlast_modified_datetime\nproduct_name\ngeneric_name\nquantity\n...\nfruits-vegetables-nuts_100g\nfruits-vegetables-nuts-estimate_100g\ncollagen-meat-protein-ratio_100g\ncocoa_100g\nchlorophyl_100g\ncarbon-footprint_100g\nnutrition-score-fr_100g\nnutrition-score-uk_100g\nglycemic-index_100g\nwater-hardness_100g\n\n\n\n\n0\n3087\nhttp://world-en.openfoodfacts.org/product/0000...\nopenfoodfacts-contributors\n1474103866\n2016-09-17T09:17:46Z\n1474103893\n2016-09-17T09:18:13Z\nFarine de blé noir\nNaN\n1kg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n4530\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nBanana Chips Sweetened (Whole)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n14.0\n14.0\nNaN\nNaN\n\n\n2\n4559\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nPeanuts\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\n\n\n3\n16087\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055731\n2017-03-09T10:35:31Z\n1489055731\n2017-03-09T10:35:31Z\nOrganic Salted Nut Mix\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n12.0\nNaN\nNaN\n\n\n4\n16094\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055653\n2017-03-09T10:34:13Z\n1489055653\n2017-03-09T10:34:13Z\nOrganic Polenta\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 163 columns\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\nfood.shape\n\n(356027, 163)\n\n\n\nfood.shape[0]\n\n356027\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nprint(food.shape) \nprint(food.shape[1]) \n\n#OR\n\nfood.info() \n\n(356027, 163)\n163\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 356027 entries, 0 to 356026\nColumns: 163 entries, code to water-hardness_100g\ndtypes: float64(107), object(56)\nmemory usage: 442.8+ MB\n\n\n\n\nStep 7. Print the name of all the columns.\n\nfood.columns\n\nIndex(['code', 'url', 'creator', 'created_t', 'created_datetime',\n       'last_modified_t', 'last_modified_datetime', 'product_name',\n       'generic_name', 'quantity',\n       ...\n       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g',\n       'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g',\n       'carbon-footprint_100g', 'nutrition-score-fr_100g',\n       'nutrition-score-uk_100g', 'glycemic-index_100g',\n       'water-hardness_100g'],\n      dtype='object', length=163)\n\n\n\n\nStep 8. What is the name of 105th column?\n\nfood.columns[104]\n\n'-glucose_100g'\n\n\n\n\nStep 9. What is the type of the observations of the 105th column?\n\nfood.dtypes['-glucose_100g']\n\ndtype('float64')\n\n\n\n\nStep 10. How is the dataset indexed?\n\nfood.index\n\nRangeIndex(start=0, stop=356027, step=1)\n\n\n\n\nStep 11. What is the product name of the 19th observation?\n\nfood.values[18][7]\n\n'Lotus Organic Brown Jasmine Rice'\n\n\n\nSource: Exercise 1\n\n02Chipotle-Exercises-with-solutions\n\n\nEx1 - Filtering and Sorting Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n\nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. How many products cost more than $10.00?\n\n\nprices = [float(value[1 : -1]) for value in chipo.item_price]\n\nchipo.item_price = prices\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity','choice_description'])\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\nchipo_one_prod\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n2.39\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n10.98\n\n\n...\n...\n...\n...\n...\n...\n\n\n4602\n1827\n1\nBarbacoa Burrito\n[Tomatillo Green Chili Salsa]\n9.25\n\n\n4607\n1829\n1\nSteak Burrito\n[Tomatillo Green Chili Salsa, [Rice, Cheese, S...\n11.75\n\n\n4610\n1830\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Sour Cream, Cheese...\n11.75\n\n\n4611\n1830\n1\nVeggie Burrito\n[Tomatillo Green Chili Salsa, [Rice, Fajita Ve...\n11.25\n\n\n4612\n1831\n1\nCarnitas Bowl\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n9.25\n\n\n\n\n1806 rows × 5 columns\n\n\n\n\nchipo.query('item_price &gt; 10').item_name.nunique()\n\n31\n\n\n\n\nStep 5. What is the price of each item?\n\nprint a data frame with only two columns item_name and item_price\n\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity'])\n\nchipo[(chipo['item_name'] == 'Chicken Bowl') & (chipo['quantity'] == 1)]\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\n\nprice_per_item = chipo_one_prod[['item_name', 'item_price']]\n\nprice_per_item.sort_values(by = \"item_price\", ascending = False).head(20)\n\n\n\n\n\n\n\n\nitem_name\nitem_price\n\n\n\n\n606\nSteak Salad Bowl\n11.89\n\n\n1229\nBarbacoa Salad Bowl\n11.89\n\n\n1132\nCarnitas Salad Bowl\n11.89\n\n\n7\nSteak Burrito\n11.75\n\n\n168\nBarbacoa Crispy Tacos\n11.75\n\n\n39\nBarbacoa Bowl\n11.75\n\n\n738\nVeggie Soft Tacos\n11.25\n\n\n186\nVeggie Salad Bowl\n11.25\n\n\n62\nVeggie Bowl\n11.25\n\n\n57\nVeggie Burrito\n11.25\n\n\n250\nChicken Salad\n10.98\n\n\n5\nChicken Bowl\n10.98\n\n\n8\nSteak Soft Tacos\n9.25\n\n\n554\nCarnitas Crispy Tacos\n9.25\n\n\n237\nCarnitas Soft Tacos\n9.25\n\n\n56\nBarbacoa Soft Tacos\n9.25\n\n\n92\nSteak Crispy Tacos\n9.25\n\n\n664\nSteak Salad\n8.99\n\n\n54\nSteak Bowl\n8.99\n\n\n3750\nCarnitas Salad\n8.99\n\n\n\n\n\n\n\n\n\n\nStep 6. Sort by the name of the item\n\nchipo.item_name.sort_values()\n\n# OR\n\nchipo.sort_values(by = \"item_name\")\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3389\n1360\n2\n6 Pack Soft Drink\n[Diet Coke]\n12.98\n\n\n341\n148\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n1849\n749\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n1860\n754\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n2713\n1076\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n...\n...\n...\n...\n...\n...\n\n\n2384\n948\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa, [Fajita Vegetables,...\n8.75\n\n\n781\n322\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Black Beans, Cheese, Sou...\n8.75\n\n\n2851\n1132\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa (Medium), [Black Bea...\n8.49\n\n\n1699\n688\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n11.25\n\n\n1395\n567\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa (Mild), [Pinto Beans, Rice...\n8.49\n\n\n\n\n4622 rows × 5 columns\n\n\n\n\n\nStep 7. What was the quantity of the most expensive item ordered?\n\nchipo.sort_values(by = \"item_price\", ascending = False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3598\n1443\n15\nChips and Fresh Tomato Salsa\nNaN\n44.25\n\n\n\n\n\n\n\n\n\nStep 8. How many times was a Veggie Salad Bowl ordered?\n\nchipo_salad = chipo[chipo.item_name == \"Veggie Salad Bowl\"]\n# chipo_salad = chipo.query('item_name == \"Veggie Salad Bowl\"')\n\nlen(chipo_salad)\n\n18\n\n\n\n\nStep 9. How many times did someone order more than one Canned Soda?\n\nchipo_drink_steak_bowl = chipo[(chipo.item_name == \"Canned Soda\") & (chipo.quantity &gt; 1)]\n# chipo_drink_steak_bowl = chipo.query('item_name == \"Canned Soda\" & quantity &gt; 1')\n\nlen(chipo_drink_steak_bowl)\n\n20\n\n\n\nSource: Ex1 - Filtering and Sorting Data\n\n02Euro12-Exercises-with-solutions\n\n\nEx2 - Filtering and Sorting Data\nThis time we are going to pull data directly from the internet.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called euro12.\n\neuro12 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv', sep=',')\neuro12\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n13\n81.3%\n41\n62\n2\n9\n0\n9\n9\n16\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n9\n60.1%\n53\n73\n8\n7\n0\n11\n11\n19\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n10\n66.7%\n25\n38\n8\n4\n0\n7\n7\n15\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n22\n88.1%\n43\n45\n6\n5\n0\n11\n11\n16\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n6\n54.6%\n36\n51\n5\n6\n0\n11\n11\n19\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n20\n74.1%\n101\n89\n16\n16\n0\n18\n18\n19\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n12\n70.6%\n35\n30\n3\n5\n0\n7\n7\n15\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n6\n66.7%\n48\n56\n3\n7\n1\n7\n7\n17\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n10\n71.5%\n73\n90\n10\n12\n0\n14\n14\n16\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n17\n65.4%\n43\n51\n11\n6\n1\n10\n10\n17\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n10\n77.0%\n34\n43\n4\n6\n0\n7\n7\n16\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n8\n61.6%\n35\n51\n7\n7\n0\n9\n9\n18\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n13\n76.5%\n48\n31\n4\n5\n0\n9\n9\n18\n\n\n\n\n16 rows × 35 columns\n\n\n\n\n\nStep 4. Select only the Goal column.\n\neuro12.Goals\n\n0      4\n1      4\n2      4\n3      5\n4      3\n5     10\n6      5\n7      6\n8      2\n9      2\n10     6\n11     1\n12     5\n13    12\n14     5\n15     2\nName: Goals, dtype: int64\n\n\n\n\nStep 5. How many team participated in the Euro2012?\n\neuro12.shape[0]\n\n16\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\neuro12.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 35 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Team                        16 non-null     object \n 1   Goals                       16 non-null     int64  \n 2   Shots on target             16 non-null     int64  \n 3   Shots off target            16 non-null     int64  \n 4   Shooting Accuracy           16 non-null     object \n 5   % Goals-to-shots            16 non-null     object \n 6   Total shots (inc. Blocked)  16 non-null     int64  \n 7   Hit Woodwork                16 non-null     int64  \n 8   Penalty goals               16 non-null     int64  \n 9   Penalties not scored        16 non-null     int64  \n 10  Headed goals                16 non-null     int64  \n 11  Passes                      16 non-null     int64  \n 12  Passes completed            16 non-null     int64  \n 13  Passing Accuracy            16 non-null     object \n 14  Touches                     16 non-null     int64  \n 15  Crosses                     16 non-null     int64  \n 16  Dribbles                    16 non-null     int64  \n 17  Corners Taken               16 non-null     int64  \n 18  Tackles                     16 non-null     int64  \n 19  Clearances                  16 non-null     int64  \n 20  Interceptions               16 non-null     int64  \n 21  Clearances off line         15 non-null     float64\n 22  Clean Sheets                16 non-null     int64  \n 23  Blocks                      16 non-null     int64  \n 24  Goals conceded              16 non-null     int64  \n 25  Saves made                  16 non-null     int64  \n 26  Saves-to-shots ratio        16 non-null     object \n 27  Fouls Won                   16 non-null     int64  \n 28  Fouls Conceded              16 non-null     int64  \n 29  Offsides                    16 non-null     int64  \n 30  Yellow Cards                16 non-null     int64  \n 31  Red Cards                   16 non-null     int64  \n 32  Subs on                     16 non-null     int64  \n 33  Subs off                    16 non-null     int64  \n 34  Players Used                16 non-null     int64  \ndtypes: float64(1), int64(29), object(5)\nmemory usage: 4.5+ KB\n\n\n\n\nStep 7. View only the columns Team, Yellow Cards and Red Cards and assign them to a dataframe called discipline\n\ndiscipline = euro12[['Team', 'Yellow Cards', 'Red Cards']]\ndiscipline\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n2\nDenmark\n4\n0\n\n\n3\nEngland\n5\n0\n\n\n4\nFrance\n6\n0\n\n\n5\nGermany\n4\n0\n\n\n6\nGreece\n9\n1\n\n\n7\nItaly\n16\n0\n\n\n8\nNetherlands\n5\n0\n\n\n9\nPoland\n7\n1\n\n\n10\nPortugal\n12\n0\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n12\nRussia\n6\n0\n\n\n13\nSpain\n11\n0\n\n\n14\nSweden\n7\n0\n\n\n15\nUkraine\n5\n0\n\n\n\n\n\n\n\n\n\nStep 8. Sort the teams by Red Cards, then to Yellow Cards\n\ndiscipline.sort_values(['Red Cards', 'Yellow Cards'], ascending = False)\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n6\nGreece\n9\n1\n\n\n9\nPoland\n7\n1\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n7\nItaly\n16\n0\n\n\n10\nPortugal\n12\n0\n\n\n13\nSpain\n11\n0\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n14\nSweden\n7\n0\n\n\n4\nFrance\n6\n0\n\n\n12\nRussia\n6\n0\n\n\n3\nEngland\n5\n0\n\n\n8\nNetherlands\n5\n0\n\n\n15\nUkraine\n5\n0\n\n\n2\nDenmark\n4\n0\n\n\n5\nGermany\n4\n0\n\n\n\n\n\n\n\n\n\nStep 9. Calculate the mean Yellow Cards given per Team\n\n\nround(discipline['Yellow Cards'].mean())\n\n7\n\n\n\n\nStep 10. Filter teams that scored more than 6 goals\n\neuro12[euro12.Goals &gt; 6]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 11. Select the teams that start with G\n\neuro12[euro12.Team.str.startswith('G')]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 12. Select the first 7 columns\n\neuro12.iloc[: , 0:7]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n\n\n\n\n\n\n\n\n\nStep 13. Select all columns except the last 3.\n\neuro12.iloc[: , :-3]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nClean Sheets\nBlocks\nGoals conceded\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n0\n10\n3\n13\n81.3%\n41\n62\n2\n9\n0\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n1\n10\n6\n9\n60.1%\n53\n73\n8\n7\n0\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n1\n10\n5\n10\n66.7%\n25\n38\n8\n4\n0\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n2\n29\n3\n22\n88.1%\n43\n45\n6\n5\n0\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n1\n7\n5\n6\n54.6%\n36\n51\n5\n6\n0\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n1\n11\n6\n10\n62.6%\n63\n49\n12\n4\n0\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n1\n23\n7\n13\n65.1%\n67\n48\n12\n9\n1\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n2\n18\n7\n20\n74.1%\n101\n89\n16\n16\n0\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n0\n9\n5\n12\n70.6%\n35\n30\n3\n5\n0\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n0\n8\n3\n6\n66.7%\n48\n56\n3\n7\n1\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n2\n11\n4\n10\n71.5%\n73\n90\n10\n12\n0\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n0\n23\n9\n17\n65.4%\n43\n51\n11\n6\n1\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n0\n8\n3\n10\n77.0%\n34\n43\n4\n6\n0\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n5\n8\n1\n15\n93.8%\n102\n83\n19\n11\n0\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n1\n12\n5\n8\n61.6%\n35\n51\n7\n7\n0\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n0\n4\n4\n13\n76.5%\n48\n31\n4\n5\n0\n\n\n\n\n16 rows × 32 columns\n\n\n\n\n\nStep 14. Present only the Shooting Accuracy from England, Italy and Russia\n\neuro12.loc[euro12.Team.isin(['England', 'Italy', 'Russia']), ['Team','Shooting Accuracy']]\n\n\n\n\n\n\n\n\nTeam\nShooting Accuracy\n\n\n\n\n3\nEngland\n50.0%\n\n\n7\nItaly\n43.0%\n\n\n12\nRussia\n22.5%\n\n\n\n\n\n\n\n\nSource: Ex2 - Filtering and Sorting Data\n\n03Chipotle-Exercises-with-solutions(1)\n\n\nVisualizing Chipotle’s Data\nThis time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set this so the graphs open internally\n%matplotlib inline\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. Create a histogram of the top 5 items bought\n\n# get the Series of the names\nx = chipo.item_name\n\n# use the Counter class from collections to create a dictionary with keys(text) and frequency\nletter_counts = Counter(x)\n\n# convert the dictionary to a DataFrame\ndf = pd.DataFrame.from_dict(letter_counts, orient='index')\n\n# sort the values from the top to the least value and slice the first 5 items\ndf = df[0].sort_values(ascending = True)[45:50]\n\n# create the plot\ndf.plot(kind='bar')\n\n# Set the title and labels\nplt.xlabel('Items')\nplt.ylabel('Number of Times Ordered')\nplt.title('Most ordered Chipotle\\'s Items')\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 6. Create a scatterplot with the number of items orderered per order price\n\nHint: Price should be in the X-axis and Items ordered in the Y-axis\n\n# create a list of prices\nchipo.item_price = [float(value[1:-1]) for value in chipo.item_price] # strip the dollar sign and trailing space\n\n# then groupby the orders and sum\norders = chipo.groupby('order_id').sum()\n\n# creates the scatterplot\n# plt.scatter(orders.quantity, orders.item_price, s = 50, c = 'green')\nplt.scatter(x = orders.item_price, y = orders.quantity, s = 50, c = 'green')\n\n# Set the title and labels\nplt.xlabel('Order Price')\nplt.ylabel('Items ordered')\nplt.title('Number of items ordered per order price')\nplt.ylim(0)\n\n(0.0, 36.7)\n\n\n\n\n\nStep 7. BONUS: Create a question and a graph to answer your own question.\n\nSource: Visualizing Chipotle's Data\n\n03Scores-Exercises-with-solutions\n\n\nScores\n\nIntroduction:\nThis time you will create the data.\nExercise based on Chris Albon work, the credits belong to him.\n\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\nStep 2. Create the DataFrame that should look like the one below.\n\nraw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n            'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'], \n            'female': [0, 1, 1, 0, 1],\n            'age': [42, 52, 36, 24, 73], \n            'preTestScore': [4, 24, 31, 2, 3],\n            'postTestScore': [25, 94, 57, 62, 70]}\n\ndf = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'female', 'preTestScore', 'postTestScore'])\n\ndf\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\nfemale\npreTestScore\npostTestScore\n\n\n\n\n0\nJason\nMiller\n42\n0\n4\n25\n\n\n1\nMolly\nJacobson\n52\n1\n24\n94\n\n\n2\nTina\nAli\n36\n1\n31\n57\n\n\n3\nJake\nMilner\n24\n0\n2\n62\n\n\n4\nAmy\nCooze\n73\n1\n3\n70\n\n\n\n\n\n\n\n\n\nStep 3. Create a Scatterplot of preTestScore and postTestScore, with the size of each point determined by age\n\nHint: Don’t forget to place the labels\n\nplt.scatter(df.preTestScore, df.postTestScore, s=df.age)\n\n#set labels and titles\nplt.title(\"preTestScore x postTestScore\")\nplt.xlabel('preTestScore')\nplt.ylabel('preTestScore')\n\nText(0, 0.5, 'preTestScore')\n\n\n\n\n\nStep 4. Create a Scatterplot of preTestScore and postTestScore.\n\n\nThis time the size should be 4.5 times the postTestScore and the color determined by sex\n\nplt.scatter(df.preTestScore, df.postTestScore, s= df.postTestScore * 4.5, c = df.female)\n\n#set labels and titles\nplt.title(\"preTestScore x postTestScore\")\nplt.xlabel('preTestScore')\nplt.ylabel('preTestScore')\n\nText(46.972222222222214, 0.5, 'preTestScore')\n\n\n\n\nBONUS: Create your own question and answer it.\n\nSource: Scores"
  },
  {
    "objectID": "lab/03Chipotle-Exercises-with-solutions(1).html",
    "href": "lab/03Chipotle-Exercises-with-solutions(1).html",
    "title": "Visualizing Chipotle’s Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set this so the graphs open internally\n%matplotlib inline\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. Create a histogram of the top 5 items bought\n\n# get the Series of the names\nx = chipo.item_name\n\n# use the Counter class from collections to create a dictionary with keys(text) and frequency\nletter_counts = Counter(x)\n\n# convert the dictionary to a DataFrame\ndf = pd.DataFrame.from_dict(letter_counts, orient='index')\n\n# sort the values from the top to the least value and slice the first 5 items\ndf = df[0].sort_values(ascending = True)[45:50]\n\n# create the plot\ndf.plot(kind='bar')\n\n# Set the title and labels\nplt.xlabel('Items')\nplt.ylabel('Number of Times Ordered')\nplt.title('Most ordered Chipotle\\'s Items')\n\n# show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\nStep 6. Create a scatterplot with the number of items orderered per order price\n\nHint: Price should be in the X-axis and Items ordered in the Y-axis\n\n# create a list of prices\nchipo.item_price = [float(value[1:-1]) for value in chipo.item_price] # strip the dollar sign and trailing space\n\n# then groupby the orders and sum\norders = chipo.groupby('order_id').sum()\n\n# creates the scatterplot\n# plt.scatter(orders.quantity, orders.item_price, s = 50, c = 'green')\nplt.scatter(x = orders.item_price, y = orders.quantity, s = 50, c = 'green')\n\n# Set the title and labels\nplt.xlabel('Order Price')\nplt.ylabel('Items ordered')\nplt.title('Number of items ordered per order price')\nplt.ylim(0)\n\n(0.0, 36.7)\n\n\n\n\n\nStep 7. BONUS: Create a question and a graph to answer your own question."
  },
  {
    "objectID": "lab/02Chipotle-Exercises-with-solutions.html",
    "href": "lab/02Chipotle-Exercises-with-solutions.html",
    "title": "Ex1 - Filtering and Sorting Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n\nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. How many products cost more than $10.00?\n\n\nprices = [float(value[1 : -1]) for value in chipo.item_price]\n\nchipo.item_price = prices\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity','choice_description'])\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\nchipo_one_prod\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n2.39\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n10.98\n\n\n...\n...\n...\n...\n...\n...\n\n\n4602\n1827\n1\nBarbacoa Burrito\n[Tomatillo Green Chili Salsa]\n9.25\n\n\n4607\n1829\n1\nSteak Burrito\n[Tomatillo Green Chili Salsa, [Rice, Cheese, S...\n11.75\n\n\n4610\n1830\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Sour Cream, Cheese...\n11.75\n\n\n4611\n1830\n1\nVeggie Burrito\n[Tomatillo Green Chili Salsa, [Rice, Fajita Ve...\n11.25\n\n\n4612\n1831\n1\nCarnitas Bowl\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n9.25\n\n\n\n\n1806 rows × 5 columns\n\n\n\n\nchipo.query('item_price &gt; 10').item_name.nunique()\n\n31\n\n\n\n\nStep 5. What is the price of each item?\n\nprint a data frame with only two columns item_name and item_price\n\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity'])\n\nchipo[(chipo['item_name'] == 'Chicken Bowl') & (chipo['quantity'] == 1)]\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\n\nprice_per_item = chipo_one_prod[['item_name', 'item_price']]\n\nprice_per_item.sort_values(by = \"item_price\", ascending = False).head(20)\n\n\n\n\n\n\n\n\nitem_name\nitem_price\n\n\n\n\n606\nSteak Salad Bowl\n11.89\n\n\n1229\nBarbacoa Salad Bowl\n11.89\n\n\n1132\nCarnitas Salad Bowl\n11.89\n\n\n7\nSteak Burrito\n11.75\n\n\n168\nBarbacoa Crispy Tacos\n11.75\n\n\n39\nBarbacoa Bowl\n11.75\n\n\n738\nVeggie Soft Tacos\n11.25\n\n\n186\nVeggie Salad Bowl\n11.25\n\n\n62\nVeggie Bowl\n11.25\n\n\n57\nVeggie Burrito\n11.25\n\n\n250\nChicken Salad\n10.98\n\n\n5\nChicken Bowl\n10.98\n\n\n8\nSteak Soft Tacos\n9.25\n\n\n554\nCarnitas Crispy Tacos\n9.25\n\n\n237\nCarnitas Soft Tacos\n9.25\n\n\n56\nBarbacoa Soft Tacos\n9.25\n\n\n92\nSteak Crispy Tacos\n9.25\n\n\n664\nSteak Salad\n8.99\n\n\n54\nSteak Bowl\n8.99\n\n\n3750\nCarnitas Salad\n8.99\n\n\n\n\n\n\n\n\n\n\nStep 6. Sort by the name of the item\n\nchipo.item_name.sort_values()\n\n# OR\n\nchipo.sort_values(by = \"item_name\")\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3389\n1360\n2\n6 Pack Soft Drink\n[Diet Coke]\n12.98\n\n\n341\n148\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n1849\n749\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n1860\n754\n1\n6 Pack Soft Drink\n[Diet Coke]\n6.49\n\n\n2713\n1076\n1\n6 Pack Soft Drink\n[Coke]\n6.49\n\n\n...\n...\n...\n...\n...\n...\n\n\n2384\n948\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa, [Fajita Vegetables,...\n8.75\n\n\n781\n322\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Black Beans, Cheese, Sou...\n8.75\n\n\n2851\n1132\n1\nVeggie Soft Tacos\n[Roasted Chili Corn Salsa (Medium), [Black Bea...\n8.49\n\n\n1699\n688\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa, [Fajita Vegetables, Rice,...\n11.25\n\n\n1395\n567\n1\nVeggie Soft Tacos\n[Fresh Tomato Salsa (Mild), [Pinto Beans, Rice...\n8.49\n\n\n\n\n4622 rows × 5 columns\n\n\n\n\n\nStep 7. What was the quantity of the most expensive item ordered?\n\nchipo.sort_values(by = \"item_price\", ascending = False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n3598\n1443\n15\nChips and Fresh Tomato Salsa\nNaN\n44.25\n\n\n\n\n\n\n\n\n\nStep 8. How many times was a Veggie Salad Bowl ordered?\n\nchipo_salad = chipo[chipo.item_name == \"Veggie Salad Bowl\"]\n# chipo_salad = chipo.query('item_name == \"Veggie Salad Bowl\"')\n\nlen(chipo_salad)\n\n18\n\n\n\n\nStep 9. How many times did someone order more than one Canned Soda?\n\nchipo_drink_steak_bowl = chipo[(chipo.item_name == \"Canned Soda\") & (chipo.quantity &gt; 1)]\n# chipo_drink_steak_bowl = chipo.query('item_name == \"Canned Soda\" & quantity &gt; 1')\n\nlen(chipo_drink_steak_bowl)\n\n20"
  },
  {
    "objectID": "lab/01Occupation-Exercises-with-solutions.html",
    "href": "lab/01Occupation-Exercises-with-solutions.html",
    "title": "Ex3 - Getting and Knowing your Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called users and use the ‘user_id’ as index\n\nusers = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user', \n                      sep='|', index_col='user_id')\n\n\n\nStep 4. See the first 25 entries\n\nusers.head(25)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n1\n24\nM\ntechnician\n85711\n\n\n2\n53\nF\nother\n94043\n\n\n3\n23\nM\nwriter\n32067\n\n\n4\n24\nM\ntechnician\n43537\n\n\n5\n33\nF\nother\n15213\n\n\n6\n42\nM\nexecutive\n98101\n\n\n7\n57\nM\nadministrator\n91344\n\n\n8\n36\nM\nadministrator\n05201\n\n\n9\n29\nM\nstudent\n01002\n\n\n10\n53\nM\nlawyer\n90703\n\n\n11\n39\nF\nother\n30329\n\n\n12\n28\nF\nother\n06405\n\n\n13\n47\nM\neducator\n29206\n\n\n14\n45\nM\nscientist\n55106\n\n\n15\n49\nF\neducator\n97301\n\n\n16\n21\nM\nentertainment\n10309\n\n\n17\n30\nM\nprogrammer\n06355\n\n\n18\n35\nF\nother\n37212\n\n\n19\n40\nM\nlibrarian\n02138\n\n\n20\n42\nF\nhomemaker\n95660\n\n\n21\n26\nM\nwriter\n30068\n\n\n22\n25\nM\nwriter\n40206\n\n\n23\n30\nF\nartist\n48197\n\n\n24\n21\nF\nartist\n94533\n\n\n25\n39\nM\nengineer\n55107\n\n\n\n\n\n\n\n\n\nStep 5. See the last 10 entries\n\nusers.tail(10)\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\nuser_id\n\n\n\n\n\n\n\n\n934\n61\nM\nengineer\n22902\n\n\n935\n42\nM\ndoctor\n66221\n\n\n936\n24\nM\nother\n32789\n\n\n937\n48\nM\neducator\n98072\n\n\n938\n38\nF\ntechnician\n55038\n\n\n939\n26\nF\nstudent\n33319\n\n\n940\n32\nM\nadministrator\n02215\n\n\n941\n20\nM\nstudent\n97229\n\n\n942\n48\nF\nlibrarian\n78209\n\n\n943\n22\nM\nstudent\n77841\n\n\n\n\n\n\n\n\n\nStep 6. What is the number of observations in the dataset?\n\nusers.shape[0]\n\n943\n\n\n\n\nStep 7. What is the number of columns in the dataset?\n\nusers.shape[1]\n\n4\n\n\n\n\nStep 8. Print the name of all the columns.\n\nusers.columns\n\nIndex(['age', 'gender', 'occupation', 'zip_code'], dtype='object')\n\n\n\n\nStep 9. How is the dataset indexed?\n\nusers.index\n\nIndex([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       934, 935, 936, 937, 938, 939, 940, 941, 942, 943],\n      dtype='int64', name='user_id', length=943)\n\n\n\n\nStep 10. What is the data type of each column?\n\nusers.dtypes\n\nage            int64\ngender        object\noccupation    object\nzip_code      object\ndtype: object\n\n\n\n\nStep 11. Print only the occupation column\n\nusers.occupation\n\n#or\n\nusers['occupation']\n\nuser_id\n1         technician\n2              other\n3             writer\n4         technician\n5              other\n           ...      \n939          student\n940    administrator\n941          student\n942        librarian\n943          student\nName: occupation, Length: 943, dtype: object\n\n\n\n\nStep 12. How many different occupations are in this dataset?\n\nusers.occupation.nunique()\n\n21\n\n\n\n\nStep 13. What is the most frequent occupation?\n\nusers.occupation.value_counts().head(1).index[0]\n\n'student'\n\n\n\n\nStep 14. Summarize the DataFrame.\n\nusers.describe() \n\n\n\n\n\n\n\n\nage\n\n\n\n\ncount\n943.000000\n\n\nmean\n34.051962\n\n\nstd\n12.192740\n\n\nmin\n7.000000\n\n\n25%\n25.000000\n\n\n50%\n31.000000\n\n\n75%\n43.000000\n\n\nmax\n73.000000\n\n\n\n\n\n\n\n\n\nStep 15. Summarize all the columns\n\nusers.describe(include = \"all\")\n\n\n\n\n\n\n\n\nage\ngender\noccupation\nzip_code\n\n\n\n\ncount\n943.000000\n943\n943\n943\n\n\nunique\nNaN\n2\n21\n795\n\n\ntop\nNaN\nM\nstudent\n55414\n\n\nfreq\nNaN\n670\n196\n9\n\n\nmean\n34.051962\nNaN\nNaN\nNaN\n\n\nstd\n12.192740\nNaN\nNaN\nNaN\n\n\nmin\n7.000000\nNaN\nNaN\nNaN\n\n\n25%\n25.000000\nNaN\nNaN\nNaN\n\n\n50%\n31.000000\nNaN\nNaN\nNaN\n\n\n75%\n43.000000\nNaN\nNaN\nNaN\n\n\nmax\n73.000000\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\nStep 16. Summarize only the occupation column\n\nusers.occupation.describe()\n\ncount         943\nunique         21\ntop       student\nfreq          196\nName: occupation, dtype: object\n\n\n\n\nStep 17. What is the mean age of users?\n\nround(users.age.mean())\n\n34\n\n\n\n\nStep 18. What is the age with least occurrence?\n\nusers.age.value_counts().tail()\n\nage\n7     1\n66    1\n11    1\n10    1\n73    1\nName: count, dtype: int64"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fenliang Li’s course blog",
    "section": "",
    "text": "This website showcases my self introduction, practice, and homework."
  },
  {
    "objectID": "index.html#welcome-to-browse-this-website-_",
    "href": "index.html#welcome-to-browse-this-website-_",
    "title": "Fenliang Li’s course blog",
    "section": "",
    "text": "This website showcases my self introduction, practice, and homework."
  },
  {
    "objectID": "homework/Li Fenliang HW_3_2.html",
    "href": "homework/Li Fenliang HW_3_2.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nurl ='https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ndf = pd.read_csv(url)\nprint(df.head())\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\nfrom skimpy import clean_columns\ndf = clean_columns(df,case=\"snake\")\nprint(df.columns)\n\nIndex(['passenger_id', 'survived', 'pclass', 'name', 'sex', 'age', 'sib_sp',\n       'parch', 'ticket', 'fare', 'cabin', 'embarked'],\n      dtype='object')\n\n\n\ndf.fillna(\"-\")\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nname\nsex\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\n-\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\n-\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\n-\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\n-\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\n-\n1\n2\nW./C. 6607\n23.4500\n-\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\n-\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nage\nsib_sp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\nsum_table = df.describe().round(2)\nsum_table\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nage\nsib_sp\nparch\nfare\n\n\n\n\ncount\n891.00\n891.00\n891.00\n714.00\n891.00\n891.00\n891.00\n\n\nmean\n446.00\n0.38\n2.31\n29.70\n0.52\n0.38\n32.20\n\n\nstd\n257.35\n0.49\n0.84\n14.53\n1.10\n0.81\n49.69\n\n\nmin\n1.00\n0.00\n1.00\n0.42\n0.00\n0.00\n0.00\n\n\n25%\n223.50\n0.00\n2.00\n20.12\n0.00\n0.00\n7.91\n\n\n50%\n446.00\n0.00\n3.00\n28.00\n0.00\n0.00\n14.45\n\n\n75%\n668.50\n1.00\n3.00\n38.00\n1.00\n0.00\n31.00\n\n\nmax\n891.00\n1.00\n3.00\n80.00\n8.00\n6.00\n512.33\n\n\n\n\n\n\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nname\nsex\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n10\n11\n1\n3\nSandstrom, Miss. Marguerite Rut\nfemale\n4.0\n1\n1\nPP 9549\n16.7000\nG6\nS\n\n\n11\n12\n1\n1\nBonnell, Miss. Elizabeth\nfemale\n58.0\n0\n0\n113783\n26.5500\nC103\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n871\n872\n1\n1\nBeckwith, Mrs. Richard Leonard (Sallie Monypeny)\nfemale\n47.0\n1\n1\n11751\n52.5542\nD35\nS\n\n\n872\n873\n0\n1\nCarlsson, Mr. Frans Olof\nmale\n33.0\n0\n0\n695\n5.0000\nB51 B53 B55\nS\n\n\n879\n880\n1\n1\nPotter, Mrs. Thomas Jr (Lily Alexenia Wilson)\nfemale\n56.0\n0\n1\n11767\n83.1583\nC50\nC\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n\n\n183 rows × 12 columns"
  },
  {
    "objectID": "homework/Li Fenliang HW 5.html",
    "href": "homework/Li Fenliang HW 5.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\nLetsPlot.setup_html(no_js=True)\nplt.style.use(\n    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n)\n\n\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\ndf\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n140\n2020\n1.58\n1.69\n1.66\n1.39\n1.26\n1.14\n1.10\n1.12\n1.19\n1.20\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.32\n\n\n141\n2021\n1.25\n0.95\n1.20\n1.12\n1.04\n1.20\n1.07\n1.02\n1.04\n1.29\n1.29\n1.16\n1.14\n1.14\n1.13\n1.12\n1.10\n1.21\n\n\n142\n2022\n1.24\n1.16\n1.41\n1.08\n1.02\n1.12\n1.06\n1.16\n1.14\n1.31\n1.09\n1.06\n1.15\n1.16\n1.19\n1.17\n1.11\n1.18\n\n\n143\n2023\n1.29\n1.29\n1.63\n1.01\n1.12\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.49\n1.43\n1.21\n1.26\n1.40\n1.84\n\n\n144\n2024\n1.66\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.57\nNaN\nNaN\nNaN\nNaN\nNaN\n1.81\n1.67\n1.46\nNaN\n\n\n\n\n145 rows × 19 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n\n\n\n\n\n\ndf.info()\nna_values=\"***\"\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     144 non-null    float64\n 11  Nov     144 non-null    float64\n 12  Dec     144 non-null    float64\n 13  J-D     144 non-null    float64\n 14  D-N     143 non-null    float64\n 15  DJF     144 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     144 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.7 KB\n\n\n\ndf = df.set_index(\"Year\")\ndf.head()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020\n1.58\n1.69\n1.66\n1.39\n1.26\n1.14\n1.10\n1.12\n1.19\n1.20\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.32\n\n\n2021\n1.25\n0.95\n1.20\n1.12\n1.04\n1.20\n1.07\n1.02\n1.04\n1.29\n1.29\n1.16\n1.14\n1.14\n1.13\n1.12\n1.10\n1.21\n\n\n2022\n1.24\n1.16\n1.41\n1.08\n1.02\n1.12\n1.06\n1.16\n1.14\n1.31\n1.09\n1.06\n1.15\n1.16\n1.19\n1.17\n1.11\n1.18\n\n\n2023\n1.29\n1.29\n1.63\n1.01\n1.12\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.49\n1.43\n1.21\n1.26\n1.40\n1.84\n\n\n2024\n1.66\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.57\nNaN\nNaN\nNaN\nNaN\nNaN\n1.81\n1.67\n1.46\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\ndf[\"Jan\"].plot(ax=ax)\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"Jan\"])\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\nmonth = \"J-D\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\ndf[\"Period\"] = pd.cut(\n    df.index,\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\ndf[\"Period\"].tail(20)\n\nYear\n2005    1981—2010\n2006    1981—2010\n2007    1981—2010\n2008    1981—2010\n2009    1981—2010\n2010    1981—2010\n2011          NaN\n2012          NaN\n2013          NaN\n2014          NaN\n2015          NaN\n2016          NaN\n2017          NaN\n2018          NaN\n2019          NaN\n2020          NaN\n2021          NaN\n2022          NaN\n2023          NaN\n2024          NaN\nName: Period, dtype: category\nCategories (3, object): ['1921—1950' &lt; '1951—1980' &lt; '1981—2010']\n\n\n\nlist_of_months = [\"Jun\", \"Jul\", \"Aug\"]\ndf[list_of_months].stack().head()\n\nYear     \n1880  Jun   -0.18\n      Jul   -0.22\n      Aug   -0.26\n1881  Jun   -0.34\n      Jul    0.09\ndtype: float64\n\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\nfor ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n    ax.set_title(period)\nplt.suptitle(\"Histogram of temperature anomalies\")\naxes[1].set_xlabel(\"Summer temperature distribution\")\nplt.tight_layout();\n\n\n\n\n\n\n\n\n\n# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1951) & (df.index &lt;= 1980), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at this data:\ntemp_all_months\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1951\nJan\n-0.36\n\n\n1\n1951\nFeb\n-0.51\n\n\n2\n1951\nMar\n-0.19\n\n\n3\n1951\nApr\n0.07\n\n\n4\n1951\nMay\n0.17\n\n\n...\n...\n...\n...\n\n\n355\n1980\nAug\n0.09\n\n\n356\n1980\nSep\n0.10\n\n\n357\n1980\nOct\n0.12\n\n\n358\n1980\nNov\n0.20\n\n\n359\n1980\nDec\n0.09\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nquantiles = [0.3, 0.7]\nlist_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n\nprint(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\nprint(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")\n\nThe cold threshold of 30.0% is -0.1\nThe hot threshold of 70.0% is 0.1\n\n\n\n# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1981) & (df.index &lt;= 2010), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at the start of this data data:\ntemp_all_months.head()\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1981\nJan\n0.79\n\n\n1\n1981\nFeb\n0.62\n\n\n2\n1981\nMar\n0.68\n\n\n3\n1981\nApr\n0.39\n\n\n4\n1981\nMay\n0.18\n\n\n\n\n\n\n\n\nentries_less_than_q30 = temp_all_months[\"values\"] &lt; list_of_percentiles[0]\nproportion_under_q30 = entries_less_than_q30.mean()\nprint(\n    f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\"\n)\n\nThe proportion under -0.1 is 1.94%\n\n\n\nproportion_over_q70 = (temp_all_months[\"values\"] &gt; list_of_percentiles[1]).mean()\nprint(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")\n\nThe proportion over 0.1 is 84.72%\n\n\n\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"Season\", 0: \"Values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nSeason\nValues\nPeriod\n\n\n\n\n443\n1991\nDJF\n0.51\n1981—2010\n\n\n444\n1991\nMAM\n0.45\n1981—2010\n\n\n445\n1991\nJJA\n0.42\n1981—2010\n\n\n446\n1991\nSON\n0.32\n1981—2010\n\n\n447\n1992\nDJF\n0.43\n1981—2010\n\n\n448\n1992\nMAM\n0.30\n1981—2010\n\n\n449\n1992\nJJA\n-0.04\n1981—2010\n\n\n450\n1992\nSON\n-0.15\n1981—2010\n\n\n451\n1993\nDJF\n0.37\n1981—2010\n\n\n452\n1993\nMAM\n0.31\n1981—2010\n\n\n\n\n\n\n\n\ngrp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\n    [np.mean, np.var]\n)\ngrp_mean_var\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\nSeason\nPeriod\n\n\n\n\n\n\nDJF\n1921—1950\n-0.027931\n0.057703\n\n\n1951—1980\n-0.003333\n0.050375\n\n\n1981—2010\n0.522000\n0.078644\n\n\nJJA\n1921—1950\n-0.054483\n0.021611\n\n\n1951—1980\n0.001333\n0.014640\n\n\n1981—2010\n0.399000\n0.067775\n\n\nMAM\n1921—1950\n-0.041724\n0.031136\n\n\n1951—1980\n0.000333\n0.025272\n\n\n1981—2010\n0.507667\n0.075812\n\n\nSON\n1921—1950\n0.081379\n0.027798\n\n\n1951—1980\n-0.001333\n0.026384\n\n\n1981—2010\n0.427000\n0.110739\n\n\n\n\n\n\n\n\nmin_year = 1880\n(\n    ggplot(temp_all_months, aes(x=\"Year\", y=\"Values\", color=\"Season\"))\n    + geom_abline(slope=0, color=\"black\", size=1)\n    + geom_line(size=1)\n    + labs(\n        title=f\"Average annual temperature anomaly in \\n in the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})\",\n        y=\"Annual temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n    + geom_text(\n        x=min_year, y=0.1, label=\"1951—1980 average\", hjust=\"left\", color=\"black\"\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n              \n            \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                    1951—1980 average\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1880\n              \n            \n          \n          \n            \n            \n            \n              \n                1900\n              \n            \n          \n          \n            \n            \n            \n              \n                1920\n              \n            \n          \n          \n            \n            \n            \n              \n                1940\n              \n            \n          \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2020\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -1.0\n              \n            \n          \n          \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.5\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n          \n            \n              \n                1.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Average annual temperature anomaly in \n      \n      \n         in the northern hemisphere (1880—2024)\n      \n    \n    \n      \n        Annual temperature anomalies\n      \n    \n    \n      \n        Year\n      \n    \n    \n      \n      \n      \n        \n          \n            Season\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                MAM\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                JJA\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                SON\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                DJF\n              \n            \n          \n        \n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_co2 = pd.read_csv(\"data2.csv\")\ndf_co2.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n1\n1958\n4\n317.45\n317.45\n315.29\n\n\n2\n1958\n5\n317.50\n317.50\n314.71\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n4\n1958\n7\n315.86\n315.86\n314.98\n\n\n\n\n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 6]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n15\n1959\n6\n318.15\n318.15\n315.92\n\n\n27\n1960\n6\n319.59\n319.59\n317.36\n\n\n39\n1961\n6\n319.77\n319.77\n317.48\n\n\n51\n1962\n6\n320.55\n320.55\n318.27\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Jun\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nJun\nTrend\n\n\n\n\n0\n1958\n0.05\n314.85\n\n\n1\n1959\n0.14\n315.92\n\n\n2\n1960\n0.18\n317.36\n\n\n3\n1961\n0.18\n317.48\n\n\n4\n1962\n-0.13\n318.27\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Jun\", y=\"Trend\"))\n    + geom_point(color=\"black\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_temp_co2[[\"Jun\", \"Trend\"]].corr(method=\"pearson\")\n\n\n\n\n\n\n\n\nJun\nTrend\n\n\n\n\nJun\n1.000000\n0.914371\n\n\nTrend\n0.914371\n1.000000\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Year\", y=\"Jun\"))\n    + geom_line(size=1)\n    + labs(\n        title=\"June temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1970\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                1990\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2010\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.6\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n        \n      \n    \n    \n      \n        June temperature anomalies\n      \n    \n    \n      \n        Jun\n      \n    \n    \n      \n        Year\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\nbase_plot = ggplot(df_temp_co2) + scale_x_continuous(format=\"d\")\nplot_p = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Jun\"), size=1)\n    + labs(title=\"June temperature anomalies\")\n)\nplot_q = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Trend\"), size=1)\n    + labs(title=\"Carbon dioxide emissions\")\n)\ngggrid([plot_p, plot_q], ncol=2)\n\n\n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  0.0\n                \n              \n            \n            \n              \n                \n                  0.2\n                \n              \n            \n            \n              \n                \n                  0.4\n                \n              \n            \n            \n              \n                \n                  0.6\n                \n              \n            \n            \n              \n                \n                  0.8\n                \n              \n            \n            \n              \n                \n                  1.0\n                \n              \n            \n          \n        \n      \n      \n        \n          June temperature anomalies\n        \n      \n      \n        \n          Jun\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  320\n                \n              \n            \n            \n              \n                \n                  340\n                \n              \n            \n            \n              \n                \n                  360\n                \n              \n            \n            \n              \n                \n                  380\n                \n              \n            \n            \n              \n                \n                  400\n                \n              \n            \n          \n        \n      \n      \n        \n          Carbon dioxide emissions\n        \n      \n      \n        \n          Trend\n        \n      \n      \n        \n          Year"
  },
  {
    "objectID": "homework/Li Fenliang HW 2.html",
    "href": "homework/Li Fenliang HW 2.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv('seattle_pet_licenses.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 66042 entries, 0 to 66041\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   animal_s_name       64685 non-null  object \n 1   license_issue_date  66042 non-null  object \n 2   license_number      43885 non-null  float64\n 3   primary_breed       66042 non-null  object \n 4   secondary_breed     22538 non-null  object \n 5   species             66042 non-null  object \n 6   zip_code            65884 non-null  object \ndtypes: float64(1), object(6)\nmemory usage: 3.5+ MB\n\n\n\npet_name_counts = df['animal_s_name' ].value_counts()  \ntop_3_pets = pet_name_counts.head(3)  \n\nprint('Top three most common  pet names:')\nfor pet_name, count in top_3_pets.items():  \n    print(f\"{pet_name}, {count}\")\n\nTop three most common  pet names:\nLucy, 566\nBella, 451\nCharlie, 447"
  },
  {
    "objectID": "docs/practice/practice 4.out.html",
    "href": "docs/practice/practice 4.out.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimdb_data = pd.read_csv('IMDB_Top250.csv')  # Replace with actual file path\ndouban_data = pd.read_csv('douban_top250.csv')  # Replace with actual file path\n\n\nfrom bs4 import BeautifulSoup\nimport re\nimport urllib.request, urllib.error  # for URL requests\nimport csv  # for saving as CSV\n\n\n# Regular expressions to extract information\nfindLink = re.compile(r'&lt;a href=\"(.*?)\"&gt;')  # detail link\nfindImgSrc = re.compile(r'&lt;img.*src=\"(.*?)\"', re.S)  # image link\nfindTitle = re.compile(r'&lt;span class=\"title\"&gt;(.*)&lt;/span&gt;')  # movie title\nfindRating = re.compile(r'&lt;span class=\"rating_num\" property=\"v:average\"&gt;(.*)&lt;/span&gt;')  # rating\nfindJudge = re.compile(r'&lt;span&gt;(\\d*)人评价&lt;/span&gt;')  # number of reviews\nfindInq = re.compile(r'&lt;span class=\"inq\"&gt;(.*)&lt;/span&gt;')  # summary\nfindBd = re.compile(r'&lt;p class=\"\"&gt;(.*?)&lt;/p&gt;', re.S)  # additional info\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load datasets\ndouban_file_path = 'douban_top250.csv'  \nimdb_file_path = 'IMDB_Top250.csv'      \n\ndouban_data = pd.read_csv(douban_file_path, encoding='utf-8', on_bad_lines='skip')\nimdb_data = pd.read_csv(imdb_file_path, encoding='utf-8', on_bad_lines='skip')\n\n# Renaming columns for clarity and merging compatibility\ndouban_data.rename(columns={\n    '影片中文名': 'Title',\n    '评分': 'Douban_Score',\n    '评价数': 'Douban_Reviews',\n    '相关信息': 'Douban_Info'\n}, inplace=True)\n\n\nimdb_data.rename(columns={\n    'Name': 'Title',\n    'Year': 'Release_Year',\n    'IMDB Ranking': 'IMDB_Score',\n    'Genre': 'IMDB_Genre',\n    'Director': 'IMDB_Director'\n}, inplace=True)\n\n\n# Calculate average scores for both platforms\ndouban_avg_score = douban_data['Douban_Score'].mean()\nimdb_avg_score = imdb_data['IMDB_Score'].mean()\n\n# Find overlapping movies by title\noverlap_movies = pd.merge(douban_data, imdb_data, on='Title')\n\n# Visualize average scores\nplt.figure(figsize=(8, 5))\nplt.bar(['Douban', 'IMDb'], [douban_avg_score, imdb_avg_score], alpha=0.7)\nplt.title('Average Scores: Douban vs IMDb')\nplt.ylabel('Average Score')\nplt.show()\n\n# Analyze release year distribution\nplt.figure(figsize=(10, 5))\ndouban_data['Douban_Info'] = douban_data['Douban_Info'].astype(str)\ndouban_years = douban_data['Douban_Info'].str.extract(r'(\\d{4})').dropna()\ndouban_years = douban_years[0].astype(int).value_counts().sort_index()\n\nimdb_years = imdb_data['Release_Year'].value_counts().sort_index()\n\ndouban_years.plot(kind='bar', alpha=0.7, label='Douban', figsize=(10, 5))\nimdb_years.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Release Year Distribution')\nplt.xlabel('Year')\nplt.ylabel('Number of Movies')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Analyze genre distribution\nimdb_genres = imdb_data['IMDB_Genre'].str.split(',').explode().str.strip().value_counts()\nplt.figure(figsize=(10, 5))\nimdb_genres.head(10).plot(kind='bar', alpha=0.7, color='orange')\nplt.title('Top 10 IMDb Genres')\nplt.xlabel('Genre')\nplt.ylabel('Count')\nplt.show()\n\n# Top directors by movie count\ndouban_directors = douban_data['Douban_Info'].str.extract(r'导演: (.+?) ').dropna()\ndouban_top_directors = douban_directors[0].value_counts().head(10)\n\nimdb_top_directors = imdb_data['IMDB_Director'].value_counts().head(10)\n\nplt.figure(figsize=(10, 5))\ndouban_top_directors.plot(kind='bar', alpha=0.7, label='Douban', color='blue')\nplt.title('Top 10 Douban Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\nplt.figure(figsize=(10, 5))\nimdb_top_directors.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Top 10 IMDb Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\n# Save overlapping movies to a CSV file\noverlap_movies.to_csv('overlap_movies.csv', index=False)\n\n# Print results\nprint(f\"豆瓣平均评分: {douban_avg_score}\")\nprint(f\"IMDb平均评分: {imdb_avg_score}\")\nprint(f\"重叠电影数量: {len(overlap_movies)}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n豆瓣平均评分: 8.9396\nIMDb平均评分: 8.254\n重叠电影数量: 0"
  },
  {
    "objectID": "docs/practice/practice 3_2.out.html",
    "href": "docs/practice/practice 3_2.out.html",
    "title": "",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\n\n\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)"
  },
  {
    "objectID": "docs/practice/practice 2_2.out.html",
    "href": "docs/practice/practice 2_2.out.html",
    "title": "",
    "section": "",
    "text": "%pip install lets-plot\n\nRequirement already satisfied: lets-plot in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.5.1)\nRequirement already satisfied: pypng in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lets-plot) (0.20220715.0)\nRequirement already satisfied: palettable in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lets-plot) (3.3.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom lets_plot import *\n\n\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom lets_plot import *\n\ndata = {\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 4, 1, 3, 5]\n}\ndf = pd.DataFrame(data)\n\nggplot(df, aes(x='x', y='y')) + geom_line()\n\n   \n   \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom lets_plot import *\n\ndata = {\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 4, 1, 3, 5]\n}\ndf = pd.DataFrame(data)\n\nggplot(df, aes(x='x', y='y')) + geom_bar(stat='identity')\n\n   \n   \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.bar(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\nplt.hist(x, bins=5)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\nplt.boxplot(x)\nplt.show()"
  },
  {
    "objectID": "docs/practice/practice 1.out.html",
    "href": "docs/practice/practice 1.out.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\nLetsPlot.setup_html(no_js=True)\nplt.style.use(\n    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n)\n\n\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\ndf\n\n\n145 rows × 19 columns"
  },
  {
    "objectID": "docs/lab/03Chipotle-Exercises-with-solutions(1).out.html",
    "href": "docs/lab/03Chipotle-Exercises-with-solutions(1).out.html",
    "title": "Visualizing Chipotle’s Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# set this so the graphs open internally\n%matplotlib inline\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n    \nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)"
  },
  {
    "objectID": "docs/lab/02Chipotle-Exercises-with-solutions.out.html",
    "href": "docs/lab/02Chipotle-Exercises-with-solutions.out.html",
    "title": "Ex1 - Filtering and Sorting Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called chipo.\n\nurl = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv'\n\nchipo = pd.read_csv(url, sep = '\\t')\n\n\n\nStep 4. How many products cost more than $10.00?\n\n\nprices = [float(value[1 : -1]) for value in chipo.item_price]\n\nchipo.item_price = prices\n\nchipo_filtered = chipo.drop_duplicates(['item_name','quantity','choice_description'])\n\nchipo_one_prod = chipo_filtered[chipo_filtered.quantity == 1]\nchipo_one_prod\n\n\n1806 rows × 5 columns"
  },
  {
    "objectID": "docs/lab/01Occupation-Exercises-with-solutions.out.html",
    "href": "docs/lab/01Occupation-Exercises-with-solutions.out.html",
    "title": "Ex3 - Getting and Knowing your Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called users and use the ‘user_id’ as index\n\nusers = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user', \n                      sep='|', index_col='user_id')\n\n\n\nStep 4. See the first 25 entries\n\nusers.head(25)"
  },
  {
    "objectID": "docs/homework/Li Fenliang HW_3_2.out.html",
    "href": "docs/homework/Li Fenliang HW_3_2.out.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nurl ='https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ndf = pd.read_csv(url)\nprint(df.head())\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\ndf\n\n\n891 rows × 12 columns"
  },
  {
    "objectID": "docs/homework/Li Fenliang HW 5.out.html",
    "href": "docs/homework/Li Fenliang HW 5.out.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\nLetsPlot.setup_html(no_js=True)\nplt.style.use(\n    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n)\n\n\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\ndf\n\n\n145 rows × 19 columns"
  },
  {
    "objectID": "docs/homework/Li Fenliang HW 2.out.html",
    "href": "docs/homework/Li Fenliang HW 2.out.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv('seattle_pet_licenses.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 66042 entries, 0 to 66041\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   animal_s_name       64685 non-null  object \n 1   license_issue_date  66042 non-null  object \n 2   license_number      43885 non-null  float64\n 3   primary_breed       66042 non-null  object \n 4   secondary_breed     22538 non-null  object \n 5   species             66042 non-null  object \n 6   zip_code            65884 non-null  object \ndtypes: float64(1), object(6)\nmemory usage: 3.5+ MB\n\n\n\npet_name_counts = df['animal_s_name' ].value_counts()  \ntop_3_pets = pet_name_counts.head(3)  \n\nprint('Top three most common  pet names:')\nfor pet_name, count in top_3_pets.items():  \n    print(f\"{pet_name}, {count}\")\n\nTop three most common  pet names:\nLucy, 566\nBella, 451\nCharlie, 447"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Fenliang Li",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "docs/homework/Li Fenliang HW 1.out.html",
    "href": "docs/homework/Li Fenliang HW 1.out.html",
    "title": "",
    "section": "",
    "text": "# 导入库\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 读取CSV文件\niris_data = pd.read_csv('Iris.csv')\n\n# 查看数据前5行\nprint(iris_data.head())\n\n# 数据分析：计算不同种类鸢尾花的平均特征值\nmean_values_by_species = iris_data.groupby('Species').mean()\n\n# 打印分析结果\nprint(mean_values_by_species)\n\n# 可视化：绘制不同鸢尾花种类的特征平均值\nmean_values_by_species.plot(kind='bar', figsize=(10, 6), color=['blue', 'green', 'red', 'purple'])\nplt.title('Mean Feature Values by Iris Species')\nplt.ylabel('Mean Value')\nplt.xlabel('Species')\nplt.xticks(rotation=0)\nplt.legend(loc='upper right')\nplt.show()\n\n   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\n                    Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  \\\nSpecies                                                              \nIris-setosa       25.5          5.006         3.418          1.464   \nIris-versicolor   75.5          5.936         2.770          4.260   \nIris-virginica   125.5          6.588         2.974          5.552   \n\n                 PetalWidthCm  \nSpecies                        \nIris-setosa             0.244  \nIris-versicolor         1.326  \nIris-virginica          2.026"
  },
  {
    "objectID": "docs/homework/Li Fenliang HW 4.out.html",
    "href": "docs/homework/Li Fenliang HW 4.out.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv('all-ages.csv')\ndf\n\n\n173 rows × 11 columns"
  },
  {
    "objectID": "docs/homework/Li Fenliang HW_3_1.out.html",
    "href": "docs/homework/Li Fenliang HW_3_1.out.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nurl ='https://raw.githubusercontent.com/tidyverse/datascience-box/refs/heads/main/course-materials/lab-instructions/lab-03/data/nobel.csv'\ndf = pd.read_csv(url)\nprint(df.head())\n\n   id       firstname    surname  year category  \\\n0   1  Wilhelm Conrad    Röntgen  1901  Physics   \n1   2      Hendrik A.    Lorentz  1902  Physics   \n2   3          Pieter     Zeeman  1902  Physics   \n3   4           Henri  Becquerel  1903  Physics   \n4   5          Pierre      Curie  1903  Physics   \n\n                                         affiliation       city      country  \\\n0                                  Munich University     Munich      Germany   \n1                                  Leiden University     Leiden  Netherlands   \n2                               Amsterdam University  Amsterdam  Netherlands   \n3                                École Polytechnique      Paris       France   \n4  École municipale de physique et de chimie indu...      Paris       France   \n\n    born_date   died_date  ... died_country_code overall_motivation share  \\\n0  1845-03-27  1923-02-10  ...                DE                NaN     1   \n1  1853-07-18  1928-02-04  ...                NL                NaN     2   \n2  1865-05-25  1943-10-09  ...                NL                NaN     2   \n3  1852-12-15  1908-08-25  ...                FR                NaN     2   \n4  1859-05-15  1906-04-19  ...                FR                NaN     4   \n\n                                          motivation  born_country_original  \\\n0  \"in recognition of the extraordinary services ...  Prussia (now Germany)   \n1  \"in recognition of the extraordinary service t...        the Netherlands   \n2  \"in recognition of the extraordinary service t...        the Netherlands   \n3  \"in recognition of the extraordinary services ...                 France   \n4  \"in recognition of the extraordinary services ...                 France   \n\n       born_city_original died_country_original died_city_original  \\\n0  Lennep (now Remscheid)               Germany             Munich   \n1                  Arnhem       the Netherlands                NaN   \n2              Zonnemaire       the Netherlands          Amsterdam   \n3                   Paris                France                NaN   \n4                   Paris                France              Paris   \n\n   city_original country_original  \n0         Munich          Germany  \n1         Leiden  the Netherlands  \n2      Amsterdam  the Netherlands  \n3          Paris           France  \n4          Paris           France  \n\n[5 rows x 26 columns]\n\n\n\ndf\n\n\n935 rows × 26 columns"
  },
  {
    "objectID": "docs/lab/01Chipotle-Exercises-with-solutions.out.html",
    "href": "docs/lab/01Chipotle-Exercises-with-solutions.out.html",
    "title": "Ex2 - Getting and Knowing your Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 3. Assign it to a variable called chipo.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)"
  },
  {
    "objectID": "docs/lab/01World-Food-Facts-Exercises-with-solutions.out.html",
    "href": "docs/lab/01World-Food-Facts-Exercises-with-solutions.out.html",
    "title": "Exercise 1",
    "section": "",
    "text": "Step 1. Go to https://www.kaggle.com/openfoodfacts/world-food-facts/data\n\n\nStep 2. Download the dataset to your computer and unzip it.\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 3. Use the tsv file and assign it to a dataframe called food\n\nfood = pd.read_csv('E:\\Yang Fan\\Lab 1\\en.openfoodfacts.org.products.tsv', sep='\\t')\n\n\n\nStep 4. See the first 5 entries\n\nfood.head()\n\n\n5 rows × 163 columns"
  },
  {
    "objectID": "docs/lab/02Euro12-Exercises-with-solutions.out.html",
    "href": "docs/lab/02Euro12-Exercises-with-solutions.out.html",
    "title": "Ex2 - Filtering and Sorting Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called euro12.\n\neuro12 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv', sep=',')\neuro12\n\n\n16 rows × 35 columns"
  },
  {
    "objectID": "docs/lab/03Scores-Exercises-with-solutions.out.html",
    "href": "docs/lab/03Scores-Exercises-with-solutions.out.html",
    "title": "Scores",
    "section": "",
    "text": "Introduction:\nThis time you will create the data.\nExercise based on Chris Albon work, the credits belong to him.\n\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\nStep 2. Create the DataFrame that should look like the one below.\n\nraw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n            'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'], \n            'female': [0, 1, 1, 0, 1],\n            'age': [42, 52, 36, 24, 73], \n            'preTestScore': [4, 24, 31, 2, 3],\n            'postTestScore': [25, 94, 57, 62, 70]}\n\ndf = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'female', 'preTestScore', 'postTestScore'])\n\ndf"
  },
  {
    "objectID": "docs/practice/practice 2.out.html",
    "href": "docs/practice/practice 2.out.html",
    "title": "",
    "section": "",
    "text": "%pip install openpyxl\n\nCollecting openpyxlNote: you may need to restart the kernel to use updated packages.\n\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\nCollecting et-xmlfile (from openpyxl)\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\nDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n   ---------------------------------------- 0.0/250.9 kB ? eta -:--:--\n   - -------------------------------------- 10.2/250.9 kB ? eta -:--:--\n   ------ -------------------------------- 41.0/250.9 kB 653.6 kB/s eta 0:00:01\n   ------------------- -------------------- 122.9/250.9 kB 1.2 MB/s eta 0:00:01\n   ---------------------------------------  245.8/250.9 kB 1.5 MB/s eta 0:00:01\n   ---------------------------------------- 250.9/250.9 kB 1.5 MB/s eta 0:00:00\nDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n\n\n\nimport pandas as pd\ndata_np = pd.read_excel(\n    \"doing-economics-datafile-working-in-excel-project-2.xlsx\",\n    usecols=\"A:Q\",\n    header=1,\n    index_col=\"Period\",\n)\ndata_n = data_np.iloc[:10, :].copy()\ndata_p = data_np.iloc[14:24, :].copy()\n\n\ntest_data = {\n    \"City A\": [14.1, 14.1, 13.7],\n    \"City B\": [11.0, 12.6, 12.1],\n}\n\n\n# Original dataframe\ntest_df = pd.DataFrame.from_dict(test_data)\n# A copy of the dataframe\ntest_copy = test_df.copy()\n# A pointer to the dataframe\ntest_pointer = test_df\n\n\ntest_pointer.iloc[1, 1] = 99\n\n\nprint(\"test_df=\")\nprint(f\"{test_df}\\n\")\nprint(\"test_copy=\")\nprint(f\"{test_copy}\\n\")\n\ntest_df=\n   City A  City B\n0    14.1    11.0\n1    14.1    99.0\n2    13.7    12.1\n\ntest_copy=\n   City A  City B\n0    14.1    11.0\n1    14.1    12.6\n2    13.7    12.1\n\n\n\ndata_n.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10 entries, 1 to 10\nData columns (total 16 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Copenhagen       10 non-null     object\n 1   Dnipropetrovs’k  10 non-null     object\n 2   Minsk            10 non-null     object\n 3   St. Gallen       10 non-null     object\n 4   Muscat           10 non-null     object\n 5   Samara           10 non-null     object\n 6   Zurich           10 non-null     object\n 7   Boston           10 non-null     object\n 8   Bonn             10 non-null     object\n 9   Chengdu          10 non-null     object\n 10  Seoul            10 non-null     object\n 11  Riyadh           10 non-null     object\n 12  Nottingham       10 non-null     object\n 13  Athens           10 non-null     object\n 14  Istanbul         10 non-null     object\n 15  Melbourne        10 non-null     object\ndtypes: object(16)\nmemory usage: 1.3+ KB\n\n\n\ndata_n = data_n.astype(\"double\")\ndata_p = data_p.astype(\"double\")\n\n\nimport numpy as np\nmean_n_c = data_n.mean(axis=1)\nmean_p_c = data_p.agg(np.mean, axis=1)\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nmean_n_c.plot(ax=ax, label=\"Without punishment\")\nmean_p_c.plot(ax=ax, label=\"With punishment\")\nax.set_title(\"Average contributions to the public goods game\")\nax.set_ylabel(\"Average contribution\")\nax.legend();\n\n\n\n\n\n\n\n\nquestion:Describe any differences and similarities you see in the mean contribution over time in both experiments answer:From the chart,we can see that the trend of the blue line(the experiment without punishment)goes down obviously during each period.However the trend of orange line(the experiment with punishment)goes up slowly.They start from the same point.But they goes differently.\n\npartial_names_list = [\"F. Kennedy\", \"Lennon\", \"Maynard Keynes\", \"Wayne\"]\n[\"John \" + name for name in partial_names_list]\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n['John F. Kennedy', 'John Lennon', 'John Maynard Keynes', 'John Wayne']\n\n\n\n# Create new dataframe with bars in\ncompare_grps = pd.DataFrame(\n    [mean_n_c.loc[[1, 10]], mean_p_c.loc[[1, 10]]],\n    index=[\"Without punishment\", \"With punishment\"],\n)\n# Rename columns to have 'round' in them\ncompare_grps.columns = [\"Round \" + str(i) for i in compare_grps.columns]\n# Swap the column and index variables around with the transpose function, ready for plotting (.T is transpose)\ncompare_grps = compare_grps.T\n# Make a bar chart\ncompare_grps.plot.bar(rot=0);\n\n\n\n\n\n\n\n\nquestion:explain whether this means that the two sets of data are the same. answer:No,although the mean contribution for both experiments was 10.6 in Period 1,but in round 10,the mean contribution is totally different.The data from experiment with punishment is much higher than that from experiment without punishment.\n\nn_c = data_n.agg([\"std\", \"var\", \"mean\"], 1)\nn_c"
  },
  {
    "objectID": "docs/practice/practice 3.out.html",
    "href": "docs/practice/practice 3.out.html",
    "title": "",
    "section": "",
    "text": "%pip install pandas-datareader\n\nRequirement already satisfied: pandas-datareader in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.10.0)\nRequirement already satisfied: lxml in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (5.3.0)\nRequirement already satisfied: pandas&gt;=0.23 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (2.2.3)\nRequirement already satisfied: requests&gt;=2.19.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas-datareader) (2.32.3)\nRequirement already satisfied: numpy&gt;=1.23.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (1.26.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2024.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2024.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (2.2.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (2024.8.30)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.23-&gt;pandas-datareader) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install BeautifulSoup\n\nCollecting BeautifulSoup\n  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'error'\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n%pip install bs4\n\nCollecting bs4\n  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nCollecting beautifulsoup4 (from bs4)\n  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\nCollecting soupsieve&gt;1.2 (from beautifulsoup4-&gt;bs4)\n  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\nDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\nDownloading soupsieve-2.6-py3-none-any.whl (36 kB)\nInstalling collected packages: soupsieve, beautifulsoup4, bs4\nSuccessfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\n\n\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)"
  },
  {
    "objectID": "docs/practice/practice 3_3.out.html",
    "href": "docs/practice/practice 3_3.out.html",
    "title": "",
    "section": "",
    "text": "from bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n#saving the list as dataframe\n#then converting into .csv file\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)\n\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\nimport pandas as pd\n\n\n# Downloading imdb top 250 movie's data\nurl = 'http://www.imdb.com/chart/top'\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\nmovies = soup.select('td.titleColumn')\ncrew = [a.attrs.get('title') for a in soup.select('td.titleColumn a')]\nratings = [b.attrs.get('data-value')\n        for b in soup.select('td.posterColumn span[name=ir]')]\n\n\n\n\n# create a empty list for storing\n# movie information\nlist = []\n\n# Iterating over movies to extract\n# each movie's details\nfor index in range(0, len(movies)):\n    \n    # Separating movie into: 'place',\n    # 'title', 'year'\n    movie_string = movies[index].get_text()\n    movie = (' '.join(movie_string.split()).replace('.', ''))\n    movie_title = movie[len(str(index))+1:-7]\n    year = re.search('\\((.*?)\\)', movie_string).group(1)\n    place = movie[:len(str(index))-(len(movie))]\n    data = {\"place\": place,\n            \"movie_title\": movie_title,\n            \"rating\": ratings[index],\n            \"year\": year,\n            \"star_cast\": crew[index],\n            }\n    list.append(data)\n\n# printing movie details with its rating.\nfor movie in list:\n    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n        ') -', 'Starring:', movie['star_cast'], movie['rating'])\n\n\n##.......##\ndf = pd.DataFrame(list)\ndf.to_csv('imdb_top_250_movies.csv',index=False)"
  },
  {
    "objectID": "homework/Li Fenliang HW 1.html",
    "href": "homework/Li Fenliang HW 1.html",
    "title": "",
    "section": "",
    "text": "# 导入库\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 读取CSV文件\niris_data = pd.read_csv('Iris.csv')\n\n# 查看数据前5行\nprint(iris_data.head())\n\n# 数据分析：计算不同种类鸢尾花的平均特征值\nmean_values_by_species = iris_data.groupby('Species').mean()\n\n# 打印分析结果\nprint(mean_values_by_species)\n\n# 可视化：绘制不同鸢尾花种类的特征平均值\nmean_values_by_species.plot(kind='bar', figsize=(10, 6), color=['blue', 'green', 'red', 'purple'])\nplt.title('Mean Feature Values by Iris Species')\nplt.ylabel('Mean Value')\nplt.xlabel('Species')\nplt.xticks(rotation=0)\nplt.legend(loc='upper right')\nplt.show()\n\n   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\n                    Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  \\\nSpecies                                                              \nIris-setosa       25.5          5.006         3.418          1.464   \nIris-versicolor   75.5          5.936         2.770          4.260   \nIris-virginica   125.5          6.588         2.974          5.552   \n\n                 PetalWidthCm  \nSpecies                        \nIris-setosa             0.244  \nIris-versicolor         1.326  \nIris-virginica          2.026"
  },
  {
    "objectID": "homework/Li Fenliang HW 4.html",
    "href": "homework/Li Fenliang HW 4.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv('all-ages.csv')\ndf\n\n\n\n\n\n\n\n\nMajor_code\nMajor\nMajor_category\nTotal\nEmployed\nEmployed_full_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\n\n\n\n\n0\n1100\nGENERAL AGRICULTURE\nAgriculture & Natural Resources\n128148\n90245\n74078\n2423\n0.026147\n50000\n34000\n80000.0\n\n\n1\n1101\nAGRICULTURE PRODUCTION AND MANAGEMENT\nAgriculture & Natural Resources\n95326\n76865\n64240\n2266\n0.028636\n54000\n36000\n80000.0\n\n\n2\n1102\nAGRICULTURAL ECONOMICS\nAgriculture & Natural Resources\n33955\n26321\n22810\n821\n0.030248\n63000\n40000\n98000.0\n\n\n3\n1103\nANIMAL SCIENCES\nAgriculture & Natural Resources\n103549\n81177\n64937\n3619\n0.042679\n46000\n30000\n72000.0\n\n\n4\n1104\nFOOD SCIENCE\nAgriculture & Natural Resources\n24280\n17281\n12722\n894\n0.049188\n62000\n38500\n90000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n6211\nHOSPITALITY MANAGEMENT\nBusiness\n200854\n163393\n122499\n8862\n0.051447\n49000\n33000\n70000.0\n\n\n169\n6212\nMANAGEMENT INFORMATION SYSTEMS AND STATISTICS\nBusiness\n156673\n134478\n118249\n6186\n0.043977\n72000\n50000\n100000.0\n\n\n170\n6299\nMISCELLANEOUS BUSINESS & MEDICAL ADMINISTRATION\nBusiness\n102753\n77471\n61603\n4308\n0.052679\n53000\n36000\n83000.0\n\n\n171\n6402\nHISTORY\nHumanities & Liberal Arts\n712509\n478416\n354163\n33725\n0.065851\n50000\n35000\n80000.0\n\n\n172\n6403\nUNITED STATES HISTORY\nHumanities & Liberal Arts\n17746\n11887\n8204\n943\n0.073500\n50000\n39000\n81000.0\n\n\n\n\n173 rows × 11 columns\n\n\n\n\nresult = df.groupby([\"Major\"]).sum().sort_values([\"Unemployment_rate\"])\nprint(result)\n\n                                            Major_code  \\\nMajor                                                    \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING            2411   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION        2301   \nPHARMACOLOGY                                      3607   \nMATERIALS SCIENCE                                 5008   \nMATHEMATICS AND COMPUTER SCIENCE                  4005   \n...                                                ...   \nLIBRARY SCIENCE                                   3501   \nSCHOOL STUDENT COUNSELING                         2303   \nMILITARY TECHNOLOGIES                             3801   \nCLINICAL PSYCHOLOGY                               5202   \nMISCELLANEOUS FINE ARTS                           6099   \n\n                                                                 Major_category  \\\nMajor                                                                             \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                              Engineering   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                            Education   \nPHARMACOLOGY                                             Biology & Life Science   \nMATERIALS SCIENCE                                                   Engineering   \nMATHEMATICS AND COMPUTER SCIENCE                        Computers & Mathematics   \n...                                                                         ...   \nLIBRARY SCIENCE                                                       Education   \nSCHOOL STUDENT COUNSELING                                             Education   \nMILITARY TECHNOLOGIES                       Industrial Arts & Consumer Services   \nCLINICAL PSYCHOLOGY                                    Psychology & Social Work   \nMISCELLANEOUS FINE ARTS                                                    Arts   \n\n                                            Total  Employed  \\\nMajor                                                         \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       6264      4120   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   4037      3113   \nPHARMACOLOGY                                 5015      3481   \nMATERIALS SCIENCE                            7208      5866   \nMATHEMATICS AND COMPUTER SCIENCE             7184      5874   \n...                                           ...       ...   \nLIBRARY SCIENCE                             16193      7091   \nSCHOOL STUDENT COUNSELING                    2396      1492   \nMILITARY TECHNOLOGIES                        4315      1650   \nCLINICAL PSYCHOLOGY                          7638      5128   \nMISCELLANEOUS FINE ARTS                      8511      6431   \n\n                                            Employed_full_time_year_round  \\\nMajor                                                                       \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                               3350   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                           2468   \nPHARMACOLOGY                                                         2579   \nMATERIALS SCIENCE                                                    4505   \nMATHEMATICS AND COMPUTER SCIENCE                                     5039   \n...                                                                   ...   \nLIBRARY SCIENCE                                                      4330   \nSCHOOL STUDENT COUNSELING                                            1093   \nMILITARY TECHNOLOGIES                                                1708   \nCLINICAL PSYCHOLOGY                                                  3297   \nMISCELLANEOUS FINE ARTS                                              3802   \n\n                                            Unemployed  Unemployment_rate  \\\nMajor                                                                       \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING               0           0.000000   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION           0           0.000000   \nPHARMACOLOGY                                        57           0.016111   \nMATERIALS SCIENCE                                  134           0.022333   \nMATHEMATICS AND COMPUTER SCIENCE                   150           0.024900   \n...                                                ...                ...   \nLIBRARY SCIENCE                                    743           0.094843   \nSCHOOL STUDENT COUNSELING                          169           0.101746   \nMILITARY TECHNOLOGIES                              187           0.101796   \nCLINICAL PSYCHOLOGY                                587           0.102712   \nMISCELLANEOUS FINE ARTS                           1190           0.156147   \n\n                                            Median  P25th     P75th  \nMajor                                                                \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       85000  55000  125000.0  \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   58000  44750   79000.0  \nPHARMACOLOGY                                 60000  35000  105000.0  \nMATERIALS SCIENCE                            75000  60000  100000.0  \nMATHEMATICS AND COMPUTER SCIENCE             92000  53000  136000.0  \n...                                            ...    ...       ...  \nLIBRARY SCIENCE                              40000  30000   55000.0  \nSCHOOL STUDENT COUNSELING                    41000  33200   50000.0  \nMILITARY TECHNOLOGIES                        64000  39750   90000.0  \nCLINICAL PSYCHOLOGY                          45000  26100   62000.0  \nMISCELLANEOUS FINE ARTS                      45000  30000   60000.0  \n\n[173 rows x 10 columns]\n\n\n\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\n\n\n\n\n\n\n\n\nRank\nMajor_code\nMajor\nTotal\nMen\nWomen\nMajor_category\nShareWomen\nSample_size\nEmployed\n...\nPart_time\nFull_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\nCollege_jobs\nNon_college_jobs\nLow_wage_jobs\n\n\n\n\n0\n1\n2419\nPETROLEUM ENGINEERING\n2339.0\n2057.0\n282.0\nEngineering\n0.120564\n36\n1976\n...\n270\n1207\n37\n0.018381\n110000\n95000\n125000\n1534\n364\n193\n\n\n1\n2\n2416\nMINING AND MINERAL ENGINEERING\n756.0\n679.0\n77.0\nEngineering\n0.101852\n7\n640\n...\n170\n388\n85\n0.117241\n75000\n55000\n90000\n350\n257\n50\n\n\n2\n3\n2415\nMETALLURGICAL ENGINEERING\n856.0\n725.0\n131.0\nEngineering\n0.153037\n3\n648\n...\n133\n340\n16\n0.024096\n73000\n50000\n105000\n456\n176\n0\n\n\n3\n4\n2417\nNAVAL ARCHITECTURE AND MARINE ENGINEERING\n1258.0\n1123.0\n135.0\nEngineering\n0.107313\n16\n758\n...\n150\n692\n40\n0.050125\n70000\n43000\n80000\n529\n102\n0\n\n\n4\n5\n2405\nCHEMICAL ENGINEERING\n32260.0\n21239.0\n11021.0\nEngineering\n0.341631\n289\n25694\n...\n5180\n16697\n1672\n0.061098\n65000\n50000\n75000\n18314\n4440\n972\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n169\n3609\nZOOLOGY\n8409.0\n3050.0\n5359.0\nBiology & Life Science\n0.637293\n47\n6259\n...\n2190\n3602\n304\n0.046320\n26000\n20000\n39000\n2771\n2947\n743\n\n\n169\n170\n5201\nEDUCATIONAL PSYCHOLOGY\n2854.0\n522.0\n2332.0\nPsychology & Social Work\n0.817099\n7\n2125\n...\n572\n1211\n148\n0.065112\n25000\n24000\n34000\n1488\n615\n82\n\n\n170\n171\n5202\nCLINICAL PSYCHOLOGY\n2838.0\n568.0\n2270.0\nPsychology & Social Work\n0.799859\n13\n2101\n...\n648\n1293\n368\n0.149048\n25000\n25000\n40000\n986\n870\n622\n\n\n171\n172\n5203\nCOUNSELING PSYCHOLOGY\n4626.0\n931.0\n3695.0\nPsychology & Social Work\n0.798746\n21\n3777\n...\n965\n2738\n214\n0.053621\n23400\n19200\n26000\n2403\n1245\n308\n\n\n172\n173\n3501\nLIBRARY SCIENCE\n1098.0\n134.0\n964.0\nEducation\n0.877960\n2\n742\n...\n237\n410\n87\n0.104946\n22000\n20000\n22000\n288\n338\n192\n\n\n\n\n173 rows × 21 columns\n\n\n\n\nresult = df.groupby([\"Major\"]).sum().sort_values([\"ShareWomen\"],ascending=False)\nprint(result)\n\n                                               Rank  Major_code     Total  \\\nMajor                                                                       \nEARLY CHILDHOOD EDUCATION                       165        2307   37589.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   164        6102   38279.0   \nMEDICAL ASSISTING SERVICES                       52        6104   11123.0   \nELEMENTARY EDUCATION                            139        2304  170862.0   \nFAMILY AND CONSUMER SCIENCES                    151        2901   58001.0   \n...                                             ...         ...       ...   \nMINING AND MINERAL ENGINEERING                    2        2416     756.0   \nCONSTRUCTION SERVICES                            27        5601   18498.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      67        2504    4790.0   \nMILITARY TECHNOLOGIES                            74        3801     124.0   \nFOOD SCIENCE                                     22        1104       0.0   \n\n                                                   Men     Women  \\\nMajor                                                              \nEARLY CHILDHOOD EDUCATION                       1167.0   36422.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   1225.0   37054.0   \nMEDICAL ASSISTING SERVICES                       803.0   10320.0   \nELEMENTARY EDUCATION                           13029.0  157833.0   \nFAMILY AND CONSUMER SCIENCES                    5166.0   52835.0   \n...                                                ...       ...   \nMINING AND MINERAL ENGINEERING                   679.0      77.0   \nCONSTRUCTION SERVICES                          16820.0    1678.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     4419.0     371.0   \nMILITARY TECHNOLOGIES                            124.0       0.0   \nFOOD SCIENCE                                       0.0       0.0   \n\n                                                                    Major_category  \\\nMajor                                                                                \nEARLY CHILDHOOD EDUCATION                                                Education   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                               Health   \nMEDICAL ASSISTING SERVICES                                                  Health   \nELEMENTARY EDUCATION                                                     Education   \nFAMILY AND CONSUMER SCIENCES                   Industrial Arts & Consumer Services   \n...                                                                            ...   \nMINING AND MINERAL ENGINEERING                                         Engineering   \nCONSTRUCTION SERVICES                          Industrial Arts & Consumer Services   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                            Engineering   \nMILITARY TECHNOLOGIES                          Industrial Arts & Consumer Services   \nFOOD SCIENCE                                       Agriculture & Natural Resources   \n\n                                               ShareWomen  Sample_size  \\\nMajor                                                                    \nEARLY CHILDHOOD EDUCATION                        0.968954          342   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES    0.967998           95   \nMEDICAL ASSISTING SERVICES                       0.927807           67   \nELEMENTARY EDUCATION                             0.923745         1629   \nFAMILY AND CONSUMER SCIENCES                     0.910933          518   \n...                                                   ...          ...   \nMINING AND MINERAL ENGINEERING                   0.101852            7   \nCONSTRUCTION SERVICES                            0.090713          295   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      0.077453           71   \nMILITARY TECHNOLOGIES                            0.000000            4   \nFOOD SCIENCE                                     0.000000           36   \n\n                                               Employed  Full_time  Part_time  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                         32551      27569       7001   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES     29763      19975      13862   \nMEDICAL ASSISTING SERVICES                         9168       5643       4107   \nELEMENTARY EDUCATION                             149339     123177      37965   \nFAMILY AND CONSUMER SCIENCES                      46624      36747      15872   \n...                                                 ...        ...        ...   \nMINING AND MINERAL ENGINEERING                      640        556        170   \nCONSTRUCTION SERVICES                             16318      15690       1751   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES        4186       4175        247   \nMILITARY TECHNOLOGIES                                 0        111          0   \nFOOD SCIENCE                                       3149       2558       1121   \n\n                                               Full_time_year_round  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                                     20748   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                 14460   \nMEDICAL ASSISTING SERVICES                                     4290   \nELEMENTARY EDUCATION                                          86540   \nFAMILY AND CONSUMER SCIENCES                                  26906   \n...                                                             ...   \nMINING AND MINERAL ENGINEERING                                  388   \nCONSTRUCTION SERVICES                                         12313   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                    3607   \nMILITARY TECHNOLOGIES                                           111   \nFOOD SCIENCE                                                   1735   \n\n                                               Unemployed  Unemployment_rate  \\\nMajor                                                                          \nEARLY CHILDHOOD EDUCATION                            1360           0.040105   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES        1487           0.047584   \nMEDICAL ASSISTING SERVICES                            407           0.042507   \nELEMENTARY EDUCATION                                 7297           0.046586   \nFAMILY AND CONSUMER SCIENCES                         3355           0.067128   \n...                                                   ...                ...   \nMINING AND MINERAL ENGINEERING                         85           0.117241   \nCONSTRUCTION SERVICES                                1042           0.060023   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES           250           0.056357   \nMILITARY TECHNOLOGIES                                   0           0.000000   \nFOOD SCIENCE                                          338           0.096931   \n\n                                               Median  P25th  P75th  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                       28000  21000  35000   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   28000  20000  40000   \nMEDICAL ASSISTING SERVICES                      42000  30000  65000   \nELEMENTARY EDUCATION                            32000  23400  38000   \nFAMILY AND CONSUMER SCIENCES                    30000  22900  40000   \n...                                               ...    ...    ...   \nMINING AND MINERAL ENGINEERING                  75000  55000  90000   \nCONSTRUCTION SERVICES                           50000  36000  60000   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     40000  27000  52000   \nMILITARY TECHNOLOGIES                           40000  40000  40000   \nFOOD SCIENCE                                    53000  32000  70000   \n\n                                               College_jobs  Non_college_jobs  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                             23515              7705   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES         19957              9404   \nMEDICAL ASSISTING SERVICES                             2091              6948   \nELEMENTARY EDUCATION                                 108085             36972   \nFAMILY AND CONSUMER SCIENCES                          20985             20133   \n...                                                     ...               ...   \nMINING AND MINERAL ENGINEERING                          350               257   \nCONSTRUCTION SERVICES                                  3275              5351   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES            1861              2121   \nMILITARY TECHNOLOGIES                                     0                 0   \nFOOD SCIENCE                                           1183              1274   \n\n                                               Low_wage_jobs  \nMajor                                                         \nEARLY CHILDHOOD EDUCATION                               2868  \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES           5125  \nMEDICAL ASSISTING SERVICES                              1270  \nELEMENTARY EDUCATION                                   11502  \nFAMILY AND CONSUMER SCIENCES                            5248  \n...                                                      ...  \nMINING AND MINERAL ENGINEERING                            50  \nCONSTRUCTION SERVICES                                    703  \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES              406  \nMILITARY TECHNOLOGIES                                      0  \nFOOD SCIENCE                                             485  \n\n[173 rows x 20 columns]\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\na=df['Median'].groupby(df['Major_category']).sum()\na.plot.bar()\nplt.show()"
  },
  {
    "objectID": "homework/Li Fenliang HW_3_1.html",
    "href": "homework/Li Fenliang HW_3_1.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nurl ='https://raw.githubusercontent.com/tidyverse/datascience-box/refs/heads/main/course-materials/lab-instructions/lab-03/data/nobel.csv'\ndf = pd.read_csv(url)\nprint(df.head())\n\n   id       firstname    surname  year category  \\\n0   1  Wilhelm Conrad    Röntgen  1901  Physics   \n1   2      Hendrik A.    Lorentz  1902  Physics   \n2   3          Pieter     Zeeman  1902  Physics   \n3   4           Henri  Becquerel  1903  Physics   \n4   5          Pierre      Curie  1903  Physics   \n\n                                         affiliation       city      country  \\\n0                                  Munich University     Munich      Germany   \n1                                  Leiden University     Leiden  Netherlands   \n2                               Amsterdam University  Amsterdam  Netherlands   \n3                                École Polytechnique      Paris       France   \n4  École municipale de physique et de chimie indu...      Paris       France   \n\n    born_date   died_date  ... died_country_code overall_motivation share  \\\n0  1845-03-27  1923-02-10  ...                DE                NaN     1   \n1  1853-07-18  1928-02-04  ...                NL                NaN     2   \n2  1865-05-25  1943-10-09  ...                NL                NaN     2   \n3  1852-12-15  1908-08-25  ...                FR                NaN     2   \n4  1859-05-15  1906-04-19  ...                FR                NaN     4   \n\n                                          motivation  born_country_original  \\\n0  \"in recognition of the extraordinary services ...  Prussia (now Germany)   \n1  \"in recognition of the extraordinary service t...        the Netherlands   \n2  \"in recognition of the extraordinary service t...        the Netherlands   \n3  \"in recognition of the extraordinary services ...                 France   \n4  \"in recognition of the extraordinary services ...                 France   \n\n       born_city_original died_country_original died_city_original  \\\n0  Lennep (now Remscheid)               Germany             Munich   \n1                  Arnhem       the Netherlands                NaN   \n2              Zonnemaire       the Netherlands          Amsterdam   \n3                   Paris                France                NaN   \n4                   Paris                France              Paris   \n\n   city_original country_original  \n0         Munich          Germany  \n1         Leiden  the Netherlands  \n2      Amsterdam  the Netherlands  \n3          Paris           France  \n4          Paris           France  \n\n[5 rows x 26 columns]\n\n\n\ndf\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n0\n1\nWilhelm Conrad\nRöntgen\n1901\nPhysics\nMunich University\nMunich\nGermany\n1845-03-27\n1923-02-10\n...\nDE\nNaN\n1\n\"in recognition of the extraordinary services ...\nPrussia (now Germany)\nLennep (now Remscheid)\nGermany\nMunich\nMunich\nGermany\n\n\n1\n2\nHendrik A.\nLorentz\n1902\nPhysics\nLeiden University\nLeiden\nNetherlands\n1853-07-18\n1928-02-04\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nArnhem\nthe Netherlands\nNaN\nLeiden\nthe Netherlands\n\n\n2\n3\nPieter\nZeeman\n1902\nPhysics\nAmsterdam University\nAmsterdam\nNetherlands\n1865-05-25\n1943-10-09\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nZonnemaire\nthe Netherlands\nAmsterdam\nAmsterdam\nthe Netherlands\n\n\n3\n4\nHenri\nBecquerel\n1903\nPhysics\nÉcole Polytechnique\nParis\nFrance\n1852-12-15\n1908-08-25\n...\nFR\nNaN\n2\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nNaN\nParis\nFrance\n\n\n4\n5\nPierre\nCurie\n1903\nPhysics\nÉcole municipale de physique et de chimie indu...\nParis\nFrance\n1859-05-15\n1906-04-19\n...\nFR\nNaN\n4\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nParis\nParis\nFrance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n931\n966\nDenis\nMukwege\n2018\nPeace\nNaN\nNaN\nNaN\n1955-03-01\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nBelgian Congo (now Democratic Republic of the ...\nBukavu\nNaN\nNaN\nNaN\nNaN\n\n\n932\n967\nNadia\nMurad\n2018\nPeace\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nIraq\nKojo\nNaN\nNaN\nNaN\nNaN\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n935 rows × 26 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 935 entries, 0 to 934\nData columns (total 26 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   id                     935 non-null    int64 \n 1   firstname              935 non-null    object\n 2   surname                906 non-null    object\n 3   year                   935 non-null    int64 \n 4   category               935 non-null    object\n 5   affiliation            685 non-null    object\n 6   city                   680 non-null    object\n 7   country                681 non-null    object\n 8   born_date              902 non-null    object\n 9   died_date              627 non-null    object\n 10  gender                 935 non-null    object\n 11  born_city              907 non-null    object\n 12  born_country           907 non-null    object\n 13  born_country_code      907 non-null    object\n 14  died_city              608 non-null    object\n 15  died_country           614 non-null    object\n 16  died_country_code      614 non-null    object\n 17  overall_motivation     17 non-null     object\n 18  share                  935 non-null    int64 \n 19  motivation             935 non-null    object\n 20  born_country_original  907 non-null    object\n 21  born_city_original     907 non-null    object\n 22  died_country_original  614 non-null    object\n 23  died_city_original     608 non-null    object\n 24  city_original          680 non-null    object\n 25  country_original       681 non-null    object\ndtypes: int64(3), object(23)\nmemory usage: 190.1+ KB\n\n\n\ndf[(df['died_date'].isna())]\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n68\n68\nChen Ning\nYang\n1957\nPhysics\nInstitute for Advanced Study\nPrinceton NJ\nUSA\n1922-09-22\nNaN\n...\nNaN\nNaN\n2\n\"for their penetrating investigation of the so...\nChina\nHofei Anhwei\nNaN\nNaN\nPrinceton NJ\nUSA\n\n\n69\n69\nTsung-Dao\nLee\n1957\nPhysics\nColumbia University\nNew York NY\nUSA\n1926-11-24\nNaN\n...\nNaN\nNaN\n2\n\"for their penetrating investigation of the so...\nChina\nShanghai\nNaN\nNaN\nNew York NY\nUSA\n\n\n94\n95\nLeon N.\nCooper\n1972\nPhysics\nBrown University\nProvidence RI\nUSA\n1930-02-28\nNaN\n...\nNaN\nNaN\n3\n\"for their jointly developed theory of superco...\nUSA\nNew York NY\nNaN\nNaN\nProvidence RI\nUSA\n\n\n96\n97\nLeo\nEsaki\n1973\nPhysics\nIBM Thomas J. Watson Research Center\nYorktown Heights NY\nUSA\n1925-03-12\nNaN\n...\nNaN\nNaN\n4\n\"for their experimental discoveries regarding ...\nJapan\nOsaka\nNaN\nNaN\nYorktown Heights NY\nUSA\n\n\n97\n98\nIvar\nGiaever\n1973\nPhysics\nGeneral Electric Company\nSchenectady NY\nUSA\n1929-04-05\nNaN\n...\nNaN\nNaN\n4\n\"for their experimental discoveries regarding ...\nNorway\nBergen\nNaN\nNaN\nSchenectady NY\nUSA\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n931\n966\nDenis\nMukwege\n2018\nPeace\nNaN\nNaN\nNaN\n1955-03-01\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nBelgian Congo (now Democratic Republic of the ...\nBukavu\nNaN\nNaN\nNaN\nNaN\n\n\n932\n967\nNadia\nMurad\n2018\nPeace\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nIraq\nKojo\nNaN\nNaN\nNaN\nNaN\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n308 rows × 26 columns\n\n\n\n\ndf[(df['country'].notna())]\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n0\n1\nWilhelm Conrad\nRöntgen\n1901\nPhysics\nMunich University\nMunich\nGermany\n1845-03-27\n1923-02-10\n...\nDE\nNaN\n1\n\"in recognition of the extraordinary services ...\nPrussia (now Germany)\nLennep (now Remscheid)\nGermany\nMunich\nMunich\nGermany\n\n\n1\n2\nHendrik A.\nLorentz\n1902\nPhysics\nLeiden University\nLeiden\nNetherlands\n1853-07-18\n1928-02-04\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nArnhem\nthe Netherlands\nNaN\nLeiden\nthe Netherlands\n\n\n2\n3\nPieter\nZeeman\n1902\nPhysics\nAmsterdam University\nAmsterdam\nNetherlands\n1865-05-25\n1943-10-09\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nZonnemaire\nthe Netherlands\nAmsterdam\nAmsterdam\nthe Netherlands\n\n\n3\n4\nHenri\nBecquerel\n1903\nPhysics\nÉcole Polytechnique\nParis\nFrance\n1852-12-15\n1908-08-25\n...\nFR\nNaN\n2\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nNaN\nParis\nFrance\n\n\n4\n5\nPierre\nCurie\n1903\nPhysics\nÉcole municipale de physique et de chimie indu...\nParis\nFrance\n1859-05-15\n1906-04-19\n...\nFR\nNaN\n4\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nParis\nParis\nFrance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n928\n963\nFrances H.\nArnold\n2018\nChemistry\nCalifornia Institute of Technology (Caltech)\nPasadena CA\nUSA\n1956-07-25\nNaN\n...\nNaN\nNaN\n2\n\"for the directed evolution of enzymes\"\nUSA\nPittsburgh PA\nNaN\nNaN\nPasadena CA\nUSA\n\n\n929\n964\nGeorge P.\nSmith\n2018\nChemistry\nUniversity of Missouri\nColumbia\nUSA\n1941-03-10\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUSA\nNorwalk CT\nNaN\nNaN\nColumbia\nUSA\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n681 rows × 26 columns"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Lecture1-homework1",
    "section": "",
    "text": "Lecture1-homework1\n\n\n# 导入库\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 读取CSV文件\niris_data = pd.read_csv('Iris.csv')\n\n# 查看数据前5行\nprint(iris_data.head())\n\n# 数据分析：计算不同种类鸢尾花的平均特征值\nmean_values_by_species = iris_data.groupby('Species').mean()\n\n# 打印分析结果\nprint(mean_values_by_species)\n\n# 可视化：绘制不同鸢尾花种类的特征平均值\nmean_values_by_species.plot(kind='bar', figsize=(10, 6), color=['blue', 'green', 'red', 'purple'])\nplt.title('Mean Feature Values by Iris Species')\nplt.ylabel('Mean Value')\nplt.xlabel('Species')\nplt.xticks(rotation=0)\nplt.legend(loc='upper right')\nplt.show()\n\n   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n0   1            5.1           3.5            1.4           0.2  Iris-setosa\n1   2            4.9           3.0            1.4           0.2  Iris-setosa\n2   3            4.7           3.2            1.3           0.2  Iris-setosa\n3   4            4.6           3.1            1.5           0.2  Iris-setosa\n4   5            5.0           3.6            1.4           0.2  Iris-setosa\n                    Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  \\\nSpecies                                                              \nIris-setosa       25.5          5.006         3.418          1.464   \nIris-versicolor   75.5          5.936         2.770          4.260   \nIris-virginica   125.5          6.588         2.974          5.552   \n\n                 PetalWidthCm  \nSpecies                        \nIris-setosa             0.244  \nIris-versicolor         1.326  \nIris-virginica          2.026  \n\n\n\n\n\n\n\n\n\nSource: 导入库\n\n\nLecture2-homework2\n\n\nimport pandas as pd\ndf = pd.read_csv('seattle_pet_licenses.csv')\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 66042 entries, 0 to 66041\nData columns (total 7 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   animal_s_name       64685 non-null  object \n 1   license_issue_date  66042 non-null  object \n 2   license_number      43885 non-null  float64\n 3   primary_breed       66042 non-null  object \n 4   secondary_breed     22538 non-null  object \n 5   species             66042 non-null  object \n 6   zip_code            65884 non-null  object \ndtypes: float64(1), object(6)\nmemory usage: 3.5+ MB\n\n\n\npet_name_counts = df['animal_s_name' ].value_counts()  \ntop_3_pets = pet_name_counts.head(3)  \n\nprint('Top three most common  pet names:')\nfor pet_name, count in top_3_pets.items():  \n    print(f\"{pet_name}, {count}\")\n\nTop three most common  pet names:\nLucy, 566\nBella, 451\nCharlie, 447\n\n\nSource: Li Fenliang HW 2.ipynb\n\n\nLecture3-homework3-1\n\n\nimport pandas as pd\nurl ='https://raw.githubusercontent.com/tidyverse/datascience-box/refs/heads/main/course-materials/lab-instructions/lab-03/data/nobel.csv'\ndf = pd.read_csv(url)\nprint(df.head())\n\n   id       firstname    surname  year category  \\\n0   1  Wilhelm Conrad    Röntgen  1901  Physics   \n1   2      Hendrik A.    Lorentz  1902  Physics   \n2   3          Pieter     Zeeman  1902  Physics   \n3   4           Henri  Becquerel  1903  Physics   \n4   5          Pierre      Curie  1903  Physics   \n\n                                         affiliation       city      country  \\\n0                                  Munich University     Munich      Germany   \n1                                  Leiden University     Leiden  Netherlands   \n2                               Amsterdam University  Amsterdam  Netherlands   \n3                                École Polytechnique      Paris       France   \n4  École municipale de physique et de chimie indu...      Paris       France   \n\n    born_date   died_date  ... died_country_code overall_motivation share  \\\n0  1845-03-27  1923-02-10  ...                DE                NaN     1   \n1  1853-07-18  1928-02-04  ...                NL                NaN     2   \n2  1865-05-25  1943-10-09  ...                NL                NaN     2   \n3  1852-12-15  1908-08-25  ...                FR                NaN     2   \n4  1859-05-15  1906-04-19  ...                FR                NaN     4   \n\n                                          motivation  born_country_original  \\\n0  \"in recognition of the extraordinary services ...  Prussia (now Germany)   \n1  \"in recognition of the extraordinary service t...        the Netherlands   \n2  \"in recognition of the extraordinary service t...        the Netherlands   \n3  \"in recognition of the extraordinary services ...                 France   \n4  \"in recognition of the extraordinary services ...                 France   \n\n       born_city_original died_country_original died_city_original  \\\n0  Lennep (now Remscheid)               Germany             Munich   \n1                  Arnhem       the Netherlands                NaN   \n2              Zonnemaire       the Netherlands          Amsterdam   \n3                   Paris                France                NaN   \n4                   Paris                France              Paris   \n\n   city_original country_original  \n0         Munich          Germany  \n1         Leiden  the Netherlands  \n2      Amsterdam  the Netherlands  \n3          Paris           France  \n4          Paris           France  \n\n[5 rows x 26 columns]\n\n\n\ndf\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n0\n1\nWilhelm Conrad\nRöntgen\n1901\nPhysics\nMunich University\nMunich\nGermany\n1845-03-27\n1923-02-10\n...\nDE\nNaN\n1\n\"in recognition of the extraordinary services ...\nPrussia (now Germany)\nLennep (now Remscheid)\nGermany\nMunich\nMunich\nGermany\n\n\n1\n2\nHendrik A.\nLorentz\n1902\nPhysics\nLeiden University\nLeiden\nNetherlands\n1853-07-18\n1928-02-04\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nArnhem\nthe Netherlands\nNaN\nLeiden\nthe Netherlands\n\n\n2\n3\nPieter\nZeeman\n1902\nPhysics\nAmsterdam University\nAmsterdam\nNetherlands\n1865-05-25\n1943-10-09\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nZonnemaire\nthe Netherlands\nAmsterdam\nAmsterdam\nthe Netherlands\n\n\n3\n4\nHenri\nBecquerel\n1903\nPhysics\nÉcole Polytechnique\nParis\nFrance\n1852-12-15\n1908-08-25\n...\nFR\nNaN\n2\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nNaN\nParis\nFrance\n\n\n4\n5\nPierre\nCurie\n1903\nPhysics\nÉcole municipale de physique et de chimie indu...\nParis\nFrance\n1859-05-15\n1906-04-19\n...\nFR\nNaN\n4\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nParis\nParis\nFrance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n931\n966\nDenis\nMukwege\n2018\nPeace\nNaN\nNaN\nNaN\n1955-03-01\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nBelgian Congo (now Democratic Republic of the ...\nBukavu\nNaN\nNaN\nNaN\nNaN\n\n\n932\n967\nNadia\nMurad\n2018\nPeace\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nIraq\nKojo\nNaN\nNaN\nNaN\nNaN\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n935 rows × 26 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 935 entries, 0 to 934\nData columns (total 26 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   id                     935 non-null    int64 \n 1   firstname              935 non-null    object\n 2   surname                906 non-null    object\n 3   year                   935 non-null    int64 \n 4   category               935 non-null    object\n 5   affiliation            685 non-null    object\n 6   city                   680 non-null    object\n 7   country                681 non-null    object\n 8   born_date              902 non-null    object\n 9   died_date              627 non-null    object\n 10  gender                 935 non-null    object\n 11  born_city              907 non-null    object\n 12  born_country           907 non-null    object\n 13  born_country_code      907 non-null    object\n 14  died_city              608 non-null    object\n 15  died_country           614 non-null    object\n 16  died_country_code      614 non-null    object\n 17  overall_motivation     17 non-null     object\n 18  share                  935 non-null    int64 \n 19  motivation             935 non-null    object\n 20  born_country_original  907 non-null    object\n 21  born_city_original     907 non-null    object\n 22  died_country_original  614 non-null    object\n 23  died_city_original     608 non-null    object\n 24  city_original          680 non-null    object\n 25  country_original       681 non-null    object\ndtypes: int64(3), object(23)\nmemory usage: 190.1+ KB\n\n\n\ndf[(df['died_date'].isna())]\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n68\n68\nChen Ning\nYang\n1957\nPhysics\nInstitute for Advanced Study\nPrinceton NJ\nUSA\n1922-09-22\nNaN\n...\nNaN\nNaN\n2\n\"for their penetrating investigation of the so...\nChina\nHofei Anhwei\nNaN\nNaN\nPrinceton NJ\nUSA\n\n\n69\n69\nTsung-Dao\nLee\n1957\nPhysics\nColumbia University\nNew York NY\nUSA\n1926-11-24\nNaN\n...\nNaN\nNaN\n2\n\"for their penetrating investigation of the so...\nChina\nShanghai\nNaN\nNaN\nNew York NY\nUSA\n\n\n94\n95\nLeon N.\nCooper\n1972\nPhysics\nBrown University\nProvidence RI\nUSA\n1930-02-28\nNaN\n...\nNaN\nNaN\n3\n\"for their jointly developed theory of superco...\nUSA\nNew York NY\nNaN\nNaN\nProvidence RI\nUSA\n\n\n96\n97\nLeo\nEsaki\n1973\nPhysics\nIBM Thomas J. Watson Research Center\nYorktown Heights NY\nUSA\n1925-03-12\nNaN\n...\nNaN\nNaN\n4\n\"for their experimental discoveries regarding ...\nJapan\nOsaka\nNaN\nNaN\nYorktown Heights NY\nUSA\n\n\n97\n98\nIvar\nGiaever\n1973\nPhysics\nGeneral Electric Company\nSchenectady NY\nUSA\n1929-04-05\nNaN\n...\nNaN\nNaN\n4\n\"for their experimental discoveries regarding ...\nNorway\nBergen\nNaN\nNaN\nSchenectady NY\nUSA\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n931\n966\nDenis\nMukwege\n2018\nPeace\nNaN\nNaN\nNaN\n1955-03-01\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nBelgian Congo (now Democratic Republic of the ...\nBukavu\nNaN\nNaN\nNaN\nNaN\n\n\n932\n967\nNadia\nMurad\n2018\nPeace\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for their efforts to end the use of sexual vi...\nIraq\nKojo\nNaN\nNaN\nNaN\nNaN\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n308 rows × 26 columns\n\n\n\n\ndf[(df['country'].notna())]\n\n\n\n\n\n\n\n\nid\nfirstname\nsurname\nyear\ncategory\naffiliation\ncity\ncountry\nborn_date\ndied_date\n...\ndied_country_code\noverall_motivation\nshare\nmotivation\nborn_country_original\nborn_city_original\ndied_country_original\ndied_city_original\ncity_original\ncountry_original\n\n\n\n\n0\n1\nWilhelm Conrad\nRöntgen\n1901\nPhysics\nMunich University\nMunich\nGermany\n1845-03-27\n1923-02-10\n...\nDE\nNaN\n1\n\"in recognition of the extraordinary services ...\nPrussia (now Germany)\nLennep (now Remscheid)\nGermany\nMunich\nMunich\nGermany\n\n\n1\n2\nHendrik A.\nLorentz\n1902\nPhysics\nLeiden University\nLeiden\nNetherlands\n1853-07-18\n1928-02-04\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nArnhem\nthe Netherlands\nNaN\nLeiden\nthe Netherlands\n\n\n2\n3\nPieter\nZeeman\n1902\nPhysics\nAmsterdam University\nAmsterdam\nNetherlands\n1865-05-25\n1943-10-09\n...\nNL\nNaN\n2\n\"in recognition of the extraordinary service t...\nthe Netherlands\nZonnemaire\nthe Netherlands\nAmsterdam\nAmsterdam\nthe Netherlands\n\n\n3\n4\nHenri\nBecquerel\n1903\nPhysics\nÉcole Polytechnique\nParis\nFrance\n1852-12-15\n1908-08-25\n...\nFR\nNaN\n2\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nNaN\nParis\nFrance\n\n\n4\n5\nPierre\nCurie\n1903\nPhysics\nÉcole municipale de physique et de chimie indu...\nParis\nFrance\n1859-05-15\n1906-04-19\n...\nFR\nNaN\n4\n\"in recognition of the extraordinary services ...\nFrance\nParis\nFrance\nParis\nParis\nFrance\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n928\n963\nFrances H.\nArnold\n2018\nChemistry\nCalifornia Institute of Technology (Caltech)\nPasadena CA\nUSA\n1956-07-25\nNaN\n...\nNaN\nNaN\n2\n\"for the directed evolution of enzymes\"\nUSA\nPittsburgh PA\nNaN\nNaN\nPasadena CA\nUSA\n\n\n929\n964\nGeorge P.\nSmith\n2018\nChemistry\nUniversity of Missouri\nColumbia\nUSA\n1941-03-10\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUSA\nNorwalk CT\nNaN\nNaN\nColumbia\nUSA\n\n\n930\n965\nSir Gregory P.\nWinter\n2018\nChemistry\nMRC Laboratory of Molecular Biology\nCambridge\nUnited Kingdom\n1951-04-14\nNaN\n...\nNaN\nNaN\n4\n\"for the phage display of peptides and antibod...\nUnited Kingdom\nLeicester\nNaN\nNaN\nCambridge\nUnited Kingdom\n\n\n933\n968\nWilliam D.\nNordhaus\n2018\nEconomics\nYale University\nNew Haven CT\nUSA\n1941-05-31\nNaN\n...\nNaN\nNaN\n2\n\"for integrating climate change into long-run ...\nUSA\nAlbuquerque NM\nNaN\nNaN\nNew Haven CT\nUSA\n\n\n934\n969\nPaul M.\nRomer\n2018\nEconomics\nNYU Stern School of Business\nNew York NY\nUSA\nNaN\nNaN\n...\nNaN\nNaN\n2\n\"for integrating technological innovations int...\nUSA\nDenver CO\nNaN\nNaN\nNew York NY\nUSA\n\n\n\n\n681 rows × 26 columns\n\n\n\nSource: Li Fenliang HW_3_1.ipynb\n\n\nLecture3-homework3-2\n\n\nimport pandas as pd\nurl ='https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\ndf = pd.read_csv(url)\nprint(df.head())\n\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\n\n\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\nfrom skimpy import clean_columns\ndf = clean_columns(df,case=\"snake\")\nprint(df.columns)\n\nIndex(['passenger_id', 'survived', 'pclass', 'name', 'sex', 'age', 'sib_sp',\n       'parch', 'ticket', 'fare', 'cabin', 'embarked'],\n      dtype='object')\n\n\n\ndf.fillna(\"-\")\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nname\nsex\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\n-\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\n-\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\n-\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\n-\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\n-\n1\n2\nW./C. 6607\n23.4500\n-\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\n-\nQ\n\n\n\n\n891 rows × 12 columns\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nage\nsib_sp\nparch\nfare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\nsum_table = df.describe().round(2)\nsum_table\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nage\nsib_sp\nparch\nfare\n\n\n\n\ncount\n891.00\n891.00\n891.00\n714.00\n891.00\n891.00\n891.00\n\n\nmean\n446.00\n0.38\n2.31\n29.70\n0.52\n0.38\n32.20\n\n\nstd\n257.35\n0.49\n0.84\n14.53\n1.10\n0.81\n49.69\n\n\nmin\n1.00\n0.00\n1.00\n0.42\n0.00\n0.00\n0.00\n\n\n25%\n223.50\n0.00\n2.00\n20.12\n0.00\n0.00\n7.91\n\n\n50%\n446.00\n0.00\n3.00\n28.00\n0.00\n0.00\n14.45\n\n\n75%\n668.50\n1.00\n3.00\n38.00\n1.00\n0.00\n31.00\n\n\nmax\n891.00\n1.00\n3.00\n80.00\n8.00\n6.00\n512.33\n\n\n\n\n\n\n\n\ndf.dropna()\n\n\n\n\n\n\n\n\npassenger_id\nsurvived\npclass\nname\nsex\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\n\n\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n10\n11\n1\n3\nSandstrom, Miss. Marguerite Rut\nfemale\n4.0\n1\n1\nPP 9549\n16.7000\nG6\nS\n\n\n11\n12\n1\n1\nBonnell, Miss. Elizabeth\nfemale\n58.0\n0\n0\n113783\n26.5500\nC103\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n871\n872\n1\n1\nBeckwith, Mrs. Richard Leonard (Sallie Monypeny)\nfemale\n47.0\n1\n1\n11751\n52.5542\nD35\nS\n\n\n872\n873\n0\n1\nCarlsson, Mr. Frans Olof\nmale\n33.0\n0\n0\n695\n5.0000\nB51 B53 B55\nS\n\n\n879\n880\n1\n1\nPotter, Mrs. Thomas Jr (Lily Alexenia Wilson)\nfemale\n56.0\n0\n1\n11767\n83.1583\nC50\nC\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n\n\n183 rows × 12 columns\n\n\n\nSource: Li Fenliang HW_3_2.ipynb\n\n\nLecture4-homework4\n\n\nimport pandas as pd\ndf = pd.read_csv('all-ages.csv')\ndf\n\n\n\n\n\n\n\n\nMajor_code\nMajor\nMajor_category\nTotal\nEmployed\nEmployed_full_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\n\n\n\n\n0\n1100\nGENERAL AGRICULTURE\nAgriculture & Natural Resources\n128148\n90245\n74078\n2423\n0.026147\n50000\n34000\n80000.0\n\n\n1\n1101\nAGRICULTURE PRODUCTION AND MANAGEMENT\nAgriculture & Natural Resources\n95326\n76865\n64240\n2266\n0.028636\n54000\n36000\n80000.0\n\n\n2\n1102\nAGRICULTURAL ECONOMICS\nAgriculture & Natural Resources\n33955\n26321\n22810\n821\n0.030248\n63000\n40000\n98000.0\n\n\n3\n1103\nANIMAL SCIENCES\nAgriculture & Natural Resources\n103549\n81177\n64937\n3619\n0.042679\n46000\n30000\n72000.0\n\n\n4\n1104\nFOOD SCIENCE\nAgriculture & Natural Resources\n24280\n17281\n12722\n894\n0.049188\n62000\n38500\n90000.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n6211\nHOSPITALITY MANAGEMENT\nBusiness\n200854\n163393\n122499\n8862\n0.051447\n49000\n33000\n70000.0\n\n\n169\n6212\nMANAGEMENT INFORMATION SYSTEMS AND STATISTICS\nBusiness\n156673\n134478\n118249\n6186\n0.043977\n72000\n50000\n100000.0\n\n\n170\n6299\nMISCELLANEOUS BUSINESS & MEDICAL ADMINISTRATION\nBusiness\n102753\n77471\n61603\n4308\n0.052679\n53000\n36000\n83000.0\n\n\n171\n6402\nHISTORY\nHumanities & Liberal Arts\n712509\n478416\n354163\n33725\n0.065851\n50000\n35000\n80000.0\n\n\n172\n6403\nUNITED STATES HISTORY\nHumanities & Liberal Arts\n17746\n11887\n8204\n943\n0.073500\n50000\n39000\n81000.0\n\n\n\n\n173 rows × 11 columns\n\n\n\n\nresult = df.groupby([\"Major\"]).sum().sort_values([\"Unemployment_rate\"])\nprint(result)\n\n                                            Major_code  \\\nMajor                                                    \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING            2411   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION        2301   \nPHARMACOLOGY                                      3607   \nMATERIALS SCIENCE                                 5008   \nMATHEMATICS AND COMPUTER SCIENCE                  4005   \n...                                                ...   \nLIBRARY SCIENCE                                   3501   \nSCHOOL STUDENT COUNSELING                         2303   \nMILITARY TECHNOLOGIES                             3801   \nCLINICAL PSYCHOLOGY                               5202   \nMISCELLANEOUS FINE ARTS                           6099   \n\n                                                                 Major_category  \\\nMajor                                                                             \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                              Engineering   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                            Education   \nPHARMACOLOGY                                             Biology & Life Science   \nMATERIALS SCIENCE                                                   Engineering   \nMATHEMATICS AND COMPUTER SCIENCE                        Computers & Mathematics   \n...                                                                         ...   \nLIBRARY SCIENCE                                                       Education   \nSCHOOL STUDENT COUNSELING                                             Education   \nMILITARY TECHNOLOGIES                       Industrial Arts & Consumer Services   \nCLINICAL PSYCHOLOGY                                    Psychology & Social Work   \nMISCELLANEOUS FINE ARTS                                                    Arts   \n\n                                            Total  Employed  \\\nMajor                                                         \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       6264      4120   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   4037      3113   \nPHARMACOLOGY                                 5015      3481   \nMATERIALS SCIENCE                            7208      5866   \nMATHEMATICS AND COMPUTER SCIENCE             7184      5874   \n...                                           ...       ...   \nLIBRARY SCIENCE                             16193      7091   \nSCHOOL STUDENT COUNSELING                    2396      1492   \nMILITARY TECHNOLOGIES                        4315      1650   \nCLINICAL PSYCHOLOGY                          7638      5128   \nMISCELLANEOUS FINE ARTS                      8511      6431   \n\n                                            Employed_full_time_year_round  \\\nMajor                                                                       \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING                               3350   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION                           2468   \nPHARMACOLOGY                                                         2579   \nMATERIALS SCIENCE                                                    4505   \nMATHEMATICS AND COMPUTER SCIENCE                                     5039   \n...                                                                   ...   \nLIBRARY SCIENCE                                                      4330   \nSCHOOL STUDENT COUNSELING                                            1093   \nMILITARY TECHNOLOGIES                                                1708   \nCLINICAL PSYCHOLOGY                                                  3297   \nMISCELLANEOUS FINE ARTS                                              3802   \n\n                                            Unemployed  Unemployment_rate  \\\nMajor                                                                       \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING               0           0.000000   \nEDUCATIONAL ADMINISTRATION AND SUPERVISION           0           0.000000   \nPHARMACOLOGY                                        57           0.016111   \nMATERIALS SCIENCE                                  134           0.022333   \nMATHEMATICS AND COMPUTER SCIENCE                   150           0.024900   \n...                                                ...                ...   \nLIBRARY SCIENCE                                    743           0.094843   \nSCHOOL STUDENT COUNSELING                          169           0.101746   \nMILITARY TECHNOLOGIES                              187           0.101796   \nCLINICAL PSYCHOLOGY                                587           0.102712   \nMISCELLANEOUS FINE ARTS                           1190           0.156147   \n\n                                            Median  P25th     P75th  \nMajor                                                                \nGEOLOGICAL AND GEOPHYSICAL ENGINEERING       85000  55000  125000.0  \nEDUCATIONAL ADMINISTRATION AND SUPERVISION   58000  44750   79000.0  \nPHARMACOLOGY                                 60000  35000  105000.0  \nMATERIALS SCIENCE                            75000  60000  100000.0  \nMATHEMATICS AND COMPUTER SCIENCE             92000  53000  136000.0  \n...                                            ...    ...       ...  \nLIBRARY SCIENCE                              40000  30000   55000.0  \nSCHOOL STUDENT COUNSELING                    41000  33200   50000.0  \nMILITARY TECHNOLOGIES                        64000  39750   90000.0  \nCLINICAL PSYCHOLOGY                          45000  26100   62000.0  \nMISCELLANEOUS FINE ARTS                      45000  30000   60000.0  \n\n[173 rows x 10 columns]\n\n\n\nimport pandas as pd\ndf = pd.read_csv('recent-grads.csv')\ndf\n\n\n\n\n\n\n\n\nRank\nMajor_code\nMajor\nTotal\nMen\nWomen\nMajor_category\nShareWomen\nSample_size\nEmployed\n...\nPart_time\nFull_time_year_round\nUnemployed\nUnemployment_rate\nMedian\nP25th\nP75th\nCollege_jobs\nNon_college_jobs\nLow_wage_jobs\n\n\n\n\n0\n1\n2419\nPETROLEUM ENGINEERING\n2339.0\n2057.0\n282.0\nEngineering\n0.120564\n36\n1976\n...\n270\n1207\n37\n0.018381\n110000\n95000\n125000\n1534\n364\n193\n\n\n1\n2\n2416\nMINING AND MINERAL ENGINEERING\n756.0\n679.0\n77.0\nEngineering\n0.101852\n7\n640\n...\n170\n388\n85\n0.117241\n75000\n55000\n90000\n350\n257\n50\n\n\n2\n3\n2415\nMETALLURGICAL ENGINEERING\n856.0\n725.0\n131.0\nEngineering\n0.153037\n3\n648\n...\n133\n340\n16\n0.024096\n73000\n50000\n105000\n456\n176\n0\n\n\n3\n4\n2417\nNAVAL ARCHITECTURE AND MARINE ENGINEERING\n1258.0\n1123.0\n135.0\nEngineering\n0.107313\n16\n758\n...\n150\n692\n40\n0.050125\n70000\n43000\n80000\n529\n102\n0\n\n\n4\n5\n2405\nCHEMICAL ENGINEERING\n32260.0\n21239.0\n11021.0\nEngineering\n0.341631\n289\n25694\n...\n5180\n16697\n1672\n0.061098\n65000\n50000\n75000\n18314\n4440\n972\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n168\n169\n3609\nZOOLOGY\n8409.0\n3050.0\n5359.0\nBiology & Life Science\n0.637293\n47\n6259\n...\n2190\n3602\n304\n0.046320\n26000\n20000\n39000\n2771\n2947\n743\n\n\n169\n170\n5201\nEDUCATIONAL PSYCHOLOGY\n2854.0\n522.0\n2332.0\nPsychology & Social Work\n0.817099\n7\n2125\n...\n572\n1211\n148\n0.065112\n25000\n24000\n34000\n1488\n615\n82\n\n\n170\n171\n5202\nCLINICAL PSYCHOLOGY\n2838.0\n568.0\n2270.0\nPsychology & Social Work\n0.799859\n13\n2101\n...\n648\n1293\n368\n0.149048\n25000\n25000\n40000\n986\n870\n622\n\n\n171\n172\n5203\nCOUNSELING PSYCHOLOGY\n4626.0\n931.0\n3695.0\nPsychology & Social Work\n0.798746\n21\n3777\n...\n965\n2738\n214\n0.053621\n23400\n19200\n26000\n2403\n1245\n308\n\n\n172\n173\n3501\nLIBRARY SCIENCE\n1098.0\n134.0\n964.0\nEducation\n0.877960\n2\n742\n...\n237\n410\n87\n0.104946\n22000\n20000\n22000\n288\n338\n192\n\n\n\n\n173 rows × 21 columns\n\n\n\n\nresult = df.groupby([\"Major\"]).sum().sort_values([\"ShareWomen\"],ascending=False)\nprint(result)\n\n                                               Rank  Major_code     Total  \\\nMajor                                                                       \nEARLY CHILDHOOD EDUCATION                       165        2307   37589.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   164        6102   38279.0   \nMEDICAL ASSISTING SERVICES                       52        6104   11123.0   \nELEMENTARY EDUCATION                            139        2304  170862.0   \nFAMILY AND CONSUMER SCIENCES                    151        2901   58001.0   \n...                                             ...         ...       ...   \nMINING AND MINERAL ENGINEERING                    2        2416     756.0   \nCONSTRUCTION SERVICES                            27        5601   18498.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      67        2504    4790.0   \nMILITARY TECHNOLOGIES                            74        3801     124.0   \nFOOD SCIENCE                                     22        1104       0.0   \n\n                                                   Men     Women  \\\nMajor                                                              \nEARLY CHILDHOOD EDUCATION                       1167.0   36422.0   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   1225.0   37054.0   \nMEDICAL ASSISTING SERVICES                       803.0   10320.0   \nELEMENTARY EDUCATION                           13029.0  157833.0   \nFAMILY AND CONSUMER SCIENCES                    5166.0   52835.0   \n...                                                ...       ...   \nMINING AND MINERAL ENGINEERING                   679.0      77.0   \nCONSTRUCTION SERVICES                          16820.0    1678.0   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     4419.0     371.0   \nMILITARY TECHNOLOGIES                            124.0       0.0   \nFOOD SCIENCE                                       0.0       0.0   \n\n                                                                    Major_category  \\\nMajor                                                                                \nEARLY CHILDHOOD EDUCATION                                                Education   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                               Health   \nMEDICAL ASSISTING SERVICES                                                  Health   \nELEMENTARY EDUCATION                                                     Education   \nFAMILY AND CONSUMER SCIENCES                   Industrial Arts & Consumer Services   \n...                                                                            ...   \nMINING AND MINERAL ENGINEERING                                         Engineering   \nCONSTRUCTION SERVICES                          Industrial Arts & Consumer Services   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                            Engineering   \nMILITARY TECHNOLOGIES                          Industrial Arts & Consumer Services   \nFOOD SCIENCE                                       Agriculture & Natural Resources   \n\n                                               ShareWomen  Sample_size  \\\nMajor                                                                    \nEARLY CHILDHOOD EDUCATION                        0.968954          342   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES    0.967998           95   \nMEDICAL ASSISTING SERVICES                       0.927807           67   \nELEMENTARY EDUCATION                             0.923745         1629   \nFAMILY AND CONSUMER SCIENCES                     0.910933          518   \n...                                                   ...          ...   \nMINING AND MINERAL ENGINEERING                   0.101852            7   \nCONSTRUCTION SERVICES                            0.090713          295   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES      0.077453           71   \nMILITARY TECHNOLOGIES                            0.000000            4   \nFOOD SCIENCE                                     0.000000           36   \n\n                                               Employed  Full_time  Part_time  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                         32551      27569       7001   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES     29763      19975      13862   \nMEDICAL ASSISTING SERVICES                         9168       5643       4107   \nELEMENTARY EDUCATION                             149339     123177      37965   \nFAMILY AND CONSUMER SCIENCES                      46624      36747      15872   \n...                                                 ...        ...        ...   \nMINING AND MINERAL ENGINEERING                      640        556        170   \nCONSTRUCTION SERVICES                             16318      15690       1751   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES        4186       4175        247   \nMILITARY TECHNOLOGIES                                 0        111          0   \nFOOD SCIENCE                                       3149       2558       1121   \n\n                                               Full_time_year_round  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                                     20748   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES                 14460   \nMEDICAL ASSISTING SERVICES                                     4290   \nELEMENTARY EDUCATION                                          86540   \nFAMILY AND CONSUMER SCIENCES                                  26906   \n...                                                             ...   \nMINING AND MINERAL ENGINEERING                                  388   \nCONSTRUCTION SERVICES                                         12313   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES                    3607   \nMILITARY TECHNOLOGIES                                           111   \nFOOD SCIENCE                                                   1735   \n\n                                               Unemployed  Unemployment_rate  \\\nMajor                                                                          \nEARLY CHILDHOOD EDUCATION                            1360           0.040105   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES        1487           0.047584   \nMEDICAL ASSISTING SERVICES                            407           0.042507   \nELEMENTARY EDUCATION                                 7297           0.046586   \nFAMILY AND CONSUMER SCIENCES                         3355           0.067128   \n...                                                   ...                ...   \nMINING AND MINERAL ENGINEERING                         85           0.117241   \nCONSTRUCTION SERVICES                                1042           0.060023   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES           250           0.056357   \nMILITARY TECHNOLOGIES                                   0           0.000000   \nFOOD SCIENCE                                          338           0.096931   \n\n                                               Median  P25th  P75th  \\\nMajor                                                                 \nEARLY CHILDHOOD EDUCATION                       28000  21000  35000   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES   28000  20000  40000   \nMEDICAL ASSISTING SERVICES                      42000  30000  65000   \nELEMENTARY EDUCATION                            32000  23400  38000   \nFAMILY AND CONSUMER SCIENCES                    30000  22900  40000   \n...                                               ...    ...    ...   \nMINING AND MINERAL ENGINEERING                  75000  55000  90000   \nCONSTRUCTION SERVICES                           50000  36000  60000   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES     40000  27000  52000   \nMILITARY TECHNOLOGIES                           40000  40000  40000   \nFOOD SCIENCE                                    53000  32000  70000   \n\n                                               College_jobs  Non_college_jobs  \\\nMajor                                                                           \nEARLY CHILDHOOD EDUCATION                             23515              7705   \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES         19957              9404   \nMEDICAL ASSISTING SERVICES                             2091              6948   \nELEMENTARY EDUCATION                                 108085             36972   \nFAMILY AND CONSUMER SCIENCES                          20985             20133   \n...                                                     ...               ...   \nMINING AND MINERAL ENGINEERING                          350               257   \nCONSTRUCTION SERVICES                                  3275              5351   \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES            1861              2121   \nMILITARY TECHNOLOGIES                                     0                 0   \nFOOD SCIENCE                                           1183              1274   \n\n                                               Low_wage_jobs  \nMajor                                                         \nEARLY CHILDHOOD EDUCATION                               2868  \nCOMMUNICATION DISORDERS SCIENCES AND SERVICES           5125  \nMEDICAL ASSISTING SERVICES                              1270  \nELEMENTARY EDUCATION                                   11502  \nFAMILY AND CONSUMER SCIENCES                            5248  \n...                                                      ...  \nMINING AND MINERAL ENGINEERING                            50  \nCONSTRUCTION SERVICES                                    703  \nMECHANICAL ENGINEERING RELATED TECHNOLOGIES              406  \nMILITARY TECHNOLOGIES                                      0  \nFOOD SCIENCE                                             485  \n\n[173 rows x 20 columns]\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\na=df['Median'].groupby(df['Major_category']).sum()\na.plot.bar()\nplt.show()\n\n\n\n\n\n\n\n\nSource: Li Fenliang HW 4.ipynb\n\n\nLecture5-homework5\n\n\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\nLetsPlot.setup_html(no_js=True)\nplt.style.use(\n    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n)\n\n\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\ndf\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n140\n2020\n1.58\n1.69\n1.66\n1.39\n1.26\n1.14\n1.10\n1.12\n1.19\n1.20\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.32\n\n\n141\n2021\n1.25\n0.95\n1.20\n1.12\n1.04\n1.20\n1.07\n1.02\n1.04\n1.29\n1.29\n1.16\n1.14\n1.14\n1.13\n1.12\n1.10\n1.21\n\n\n142\n2022\n1.24\n1.16\n1.41\n1.08\n1.02\n1.12\n1.06\n1.16\n1.14\n1.31\n1.09\n1.06\n1.15\n1.16\n1.19\n1.17\n1.11\n1.18\n\n\n143\n2023\n1.29\n1.29\n1.63\n1.01\n1.12\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.49\n1.43\n1.21\n1.26\n1.40\n1.84\n\n\n144\n2024\n1.66\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.57\nNaN\nNaN\nNaN\nNaN\nNaN\n1.81\n1.67\n1.46\nNaN\n\n\n\n\n145 rows × 19 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n\n\n\n\n\n\ndf.info()\nna_values=\"***\"\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     144 non-null    float64\n 11  Nov     144 non-null    float64\n 12  Dec     144 non-null    float64\n 13  J-D     144 non-null    float64\n 14  D-N     143 non-null    float64\n 15  DJF     144 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     144 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.7 KB\n\n\n\ndf = df.set_index(\"Year\")\ndf.head()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020\n1.58\n1.69\n1.66\n1.39\n1.26\n1.14\n1.10\n1.12\n1.19\n1.20\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.32\n\n\n2021\n1.25\n0.95\n1.20\n1.12\n1.04\n1.20\n1.07\n1.02\n1.04\n1.29\n1.29\n1.16\n1.14\n1.14\n1.13\n1.12\n1.10\n1.21\n\n\n2022\n1.24\n1.16\n1.41\n1.08\n1.02\n1.12\n1.06\n1.16\n1.14\n1.31\n1.09\n1.06\n1.15\n1.16\n1.19\n1.17\n1.11\n1.18\n\n\n2023\n1.29\n1.29\n1.63\n1.01\n1.12\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.49\n1.43\n1.21\n1.26\n1.40\n1.84\n\n\n2024\n1.66\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.57\nNaN\nNaN\nNaN\nNaN\nNaN\n1.81\n1.67\n1.46\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\ndf[\"Jan\"].plot(ax=ax)\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"Jan\"])\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\nmonth = \"J-D\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\ndf[\"Period\"] = pd.cut(\n    df.index,\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\ndf[\"Period\"].tail(20)\n\nYear\n2005    1981—2010\n2006    1981—2010\n2007    1981—2010\n2008    1981—2010\n2009    1981—2010\n2010    1981—2010\n2011          NaN\n2012          NaN\n2013          NaN\n2014          NaN\n2015          NaN\n2016          NaN\n2017          NaN\n2018          NaN\n2019          NaN\n2020          NaN\n2021          NaN\n2022          NaN\n2023          NaN\n2024          NaN\nName: Period, dtype: category\nCategories (3, object): ['1921—1950' &lt; '1951—1980' &lt; '1981—2010']\n\n\n\nlist_of_months = [\"Jun\", \"Jul\", \"Aug\"]\ndf[list_of_months].stack().head()\n\nYear     \n1880  Jun   -0.18\n      Jul   -0.22\n      Aug   -0.26\n1881  Jun   -0.34\n      Jul    0.09\ndtype: float64\n\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\nfor ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n    ax.set_title(period)\nplt.suptitle(\"Histogram of temperature anomalies\")\naxes[1].set_xlabel(\"Summer temperature distribution\")\nplt.tight_layout();\n\n\n\n\n\n\n\n\n\n# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1951) & (df.index &lt;= 1980), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at this data:\ntemp_all_months\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1951\nJan\n-0.36\n\n\n1\n1951\nFeb\n-0.51\n\n\n2\n1951\nMar\n-0.19\n\n\n3\n1951\nApr\n0.07\n\n\n4\n1951\nMay\n0.17\n\n\n...\n...\n...\n...\n\n\n355\n1980\nAug\n0.09\n\n\n356\n1980\nSep\n0.10\n\n\n357\n1980\nOct\n0.12\n\n\n358\n1980\nNov\n0.20\n\n\n359\n1980\nDec\n0.09\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nquantiles = [0.3, 0.7]\nlist_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n\nprint(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\nprint(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")\n\nThe cold threshold of 30.0% is -0.1\nThe hot threshold of 70.0% is 0.1\n\n\n\n# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1981) & (df.index &lt;= 2010), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at the start of this data data:\ntemp_all_months.head()\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1981\nJan\n0.79\n\n\n1\n1981\nFeb\n0.62\n\n\n2\n1981\nMar\n0.68\n\n\n3\n1981\nApr\n0.39\n\n\n4\n1981\nMay\n0.18\n\n\n\n\n\n\n\n\nentries_less_than_q30 = temp_all_months[\"values\"] &lt; list_of_percentiles[0]\nproportion_under_q30 = entries_less_than_q30.mean()\nprint(\n    f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\"\n)\n\nThe proportion under -0.1 is 1.94%\n\n\n\nproportion_over_q70 = (temp_all_months[\"values\"] &gt; list_of_percentiles[1]).mean()\nprint(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")\n\nThe proportion over 0.1 is 84.72%\n\n\n\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"Season\", 0: \"Values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nSeason\nValues\nPeriod\n\n\n\n\n443\n1991\nDJF\n0.51\n1981—2010\n\n\n444\n1991\nMAM\n0.45\n1981—2010\n\n\n445\n1991\nJJA\n0.42\n1981—2010\n\n\n446\n1991\nSON\n0.32\n1981—2010\n\n\n447\n1992\nDJF\n0.43\n1981—2010\n\n\n448\n1992\nMAM\n0.30\n1981—2010\n\n\n449\n1992\nJJA\n-0.04\n1981—2010\n\n\n450\n1992\nSON\n-0.15\n1981—2010\n\n\n451\n1993\nDJF\n0.37\n1981—2010\n\n\n452\n1993\nMAM\n0.31\n1981—2010\n\n\n\n\n\n\n\n\ngrp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\n    [np.mean, np.var]\n)\ngrp_mean_var\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\nSeason\nPeriod\n\n\n\n\n\n\nDJF\n1921—1950\n-0.027931\n0.057703\n\n\n1951—1980\n-0.003333\n0.050375\n\n\n1981—2010\n0.522000\n0.078644\n\n\nJJA\n1921—1950\n-0.054483\n0.021611\n\n\n1951—1980\n0.001333\n0.014640\n\n\n1981—2010\n0.399000\n0.067775\n\n\nMAM\n1921—1950\n-0.041724\n0.031136\n\n\n1951—1980\n0.000333\n0.025272\n\n\n1981—2010\n0.507667\n0.075812\n\n\nSON\n1921—1950\n0.081379\n0.027798\n\n\n1951—1980\n-0.001333\n0.026384\n\n\n1981—2010\n0.427000\n0.110739\n\n\n\n\n\n\n\n\nmin_year = 1880\n(\n    ggplot(temp_all_months, aes(x=\"Year\", y=\"Values\", color=\"Season\"))\n    + geom_abline(slope=0, color=\"black\", size=1)\n    + geom_line(size=1)\n    + labs(\n        title=f\"Average annual temperature anomaly in \\n in the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})\",\n        y=\"Annual temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n    + geom_text(\n        x=min_year, y=0.1, label=\"1951—1980 average\", hjust=\"left\", color=\"black\"\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n              \n            \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                    1951—1980 average\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1880\n              \n            \n          \n          \n            \n            \n            \n              \n                1900\n              \n            \n          \n          \n            \n            \n            \n              \n                1920\n              \n            \n          \n          \n            \n            \n            \n              \n                1940\n              \n            \n          \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2020\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -1.0\n              \n            \n          \n          \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.5\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n          \n            \n              \n                1.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Average annual temperature anomaly in \n      \n      \n         in the northern hemisphere (1880—2024)\n      \n    \n    \n      \n        Annual temperature anomalies\n      \n    \n    \n      \n        Year\n      \n    \n    \n      \n      \n      \n        \n          \n            Season\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                MAM\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                JJA\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                SON\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                DJF\n              \n            \n          \n        \n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_co2 = pd.read_csv(\"data2.csv\")\ndf_co2.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n1\n1958\n4\n317.45\n317.45\n315.29\n\n\n2\n1958\n5\n317.50\n317.50\n314.71\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n4\n1958\n7\n315.86\n315.86\n314.98\n\n\n\n\n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 6]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n15\n1959\n6\n318.15\n318.15\n315.92\n\n\n27\n1960\n6\n319.59\n319.59\n317.36\n\n\n39\n1961\n6\n319.77\n319.77\n317.48\n\n\n51\n1962\n6\n320.55\n320.55\n318.27\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Jun\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nJun\nTrend\n\n\n\n\n0\n1958\n0.05\n314.85\n\n\n1\n1959\n0.14\n315.92\n\n\n2\n1960\n0.18\n317.36\n\n\n3\n1961\n0.18\n317.48\n\n\n4\n1962\n-0.13\n318.27\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Jun\", y=\"Trend\"))\n    + geom_point(color=\"black\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_temp_co2[[\"Jun\", \"Trend\"]].corr(method=\"pearson\")\n\n\n\n\n\n\n\n\nJun\nTrend\n\n\n\n\nJun\n1.000000\n0.914371\n\n\nTrend\n0.914371\n1.000000\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Year\", y=\"Jun\"))\n    + geom_line(size=1)\n    + labs(\n        title=\"June temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1970\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                1990\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2010\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.6\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n        \n      \n    \n    \n      \n        June temperature anomalies\n      \n    \n    \n      \n        Jun\n      \n    \n    \n      \n        Year\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\nbase_plot = ggplot(df_temp_co2) + scale_x_continuous(format=\"d\")\nplot_p = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Jun\"), size=1)\n    + labs(title=\"June temperature anomalies\")\n)\nplot_q = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Trend\"), size=1)\n    + labs(title=\"Carbon dioxide emissions\")\n)\ngggrid([plot_p, plot_q], ncol=2)\n\n\n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  0.0\n                \n              \n            \n            \n              \n                \n                  0.2\n                \n              \n            \n            \n              \n                \n                  0.4\n                \n              \n            \n            \n              \n                \n                  0.6\n                \n              \n            \n            \n              \n                \n                  0.8\n                \n              \n            \n            \n              \n                \n                  1.0\n                \n              \n            \n          \n        \n      \n      \n        \n          June temperature anomalies\n        \n      \n      \n        \n          Jun\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  320\n                \n              \n            \n            \n              \n                \n                  340\n                \n              \n            \n            \n              \n                \n                  360\n                \n              \n            \n            \n              \n                \n                  380\n                \n              \n            \n            \n              \n                \n                  400\n                \n              \n            \n          \n        \n      \n      \n        \n          Carbon dioxide emissions\n        \n      \n      \n        \n          Trend\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n\n\n\nSource: Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)"
  },
  {
    "objectID": "lab/01Chipotle-Exercises-with-solutions.html",
    "href": "lab/01Chipotle-Exercises-with-solutions.html",
    "title": "Ex2 - Getting and Knowing your Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet. Special thanks to: https://github.com/justmarkham for sharing the dataset and materials.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 2. Import the dataset from this address.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 3. Assign it to a variable called chipo.\n\nchipo = pd.read_csv('https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv', sep= '\\t')\n\n\n\nStep 4. See the first 10 entries\n\nchipo.head(10)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nchoice_description\nitem_price\n\n\n\n\n0\n1\n1\nChips and Fresh Tomato Salsa\nNaN\n$2.39\n\n\n1\n1\n1\nIzze\n[Clementine]\n$3.39\n\n\n2\n1\n1\nNantucket Nectar\n[Apple]\n$3.39\n\n\n3\n1\n1\nChips and Tomatillo-Green Chili Salsa\nNaN\n$2.39\n\n\n4\n2\n2\nChicken Bowl\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98\n\n\n5\n3\n1\nChicken Bowl\n[Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...\n$10.98\n\n\n6\n3\n1\nSide of Chips\nNaN\n$1.69\n\n\n7\n4\n1\nSteak Burrito\n[Tomatillo Red Chili Salsa, [Fajita Vegetables...\n$11.75\n\n\n8\n4\n1\nSteak Soft Tacos\n[Tomatillo Green Chili Salsa, [Pinto Beans, Ch...\n$9.25\n\n\n9\n5\n1\nSteak Burrito\n[Fresh Tomato Salsa, [Rice, Black Beans, Pinto...\n$9.25\n\n\n\n\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\n# Solution 1\n\nchipo.shape\n\n(4622, 5)\n\n\n\n# Solution 2\n\nchipo.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4622 entries, 0 to 4621\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   order_id            4622 non-null   int64 \n 1   quantity            4622 non-null   int64 \n 2   item_name           4622 non-null   object\n 3   choice_description  3376 non-null   object\n 4   item_price          4622 non-null   object\ndtypes: int64(2), object(3)\nmemory usage: 180.7+ KB\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nchipo.shape[1]\n\n5\n\n\n\n\nStep 7. Print the name of all the columns.\n\nchipo.columns\n\nIndex(['order_id', 'quantity', 'item_name', 'choice_description',\n       'item_price'],\n      dtype='object')\n\n\n\n\nStep 8. How is the dataset indexed?\n\nchipo.index\n\nRangeIndex(start=0, stop=4622, step=1)\n\n\n\n\nStep 9. Which was the most-ordered item?\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\nStep 10. For the most-ordered item, how many items were ordered?\n\nchipo.groupby(by=\"item_name\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nchoice_description\nitem_price\n\n\nitem_name\n\n\n\n\n\n\n\n\nChicken Bowl\n713926\n761\n[Tomatillo-Red Chili Salsa (Hot), [Black Beans...\n$16.98 $10.98 $11.25 $8.75 $8.49 $11.25 $8.75 ...\n\n\n\n\n\n\n\n\n\nStep 11. What was the most ordered item in the choice_description column?\n\nchipo.groupby(by=\"choice_description\").sum().sort_values('quantity',ascending=False).head(1)\n\n\n\n\n\n\n\n\norder_id\nquantity\nitem_name\nitem_price\n\n\nchoice_description\n\n\n\n\n\n\n\n\n[Diet Coke]\n123455\n159\nCanned SodaCanned SodaCanned Soda6 Pack Soft D...\n$2.18 $1.09 $1.09 $6.49 $2.18 $1.25 $1.09 $6.4...\n\n\n\n\n\n\n\n\n\nStep 12. How many items were orderd in total?\n\nchipo.item_name.count()\n\n4622\n\n\n\n\nStep 13. Turn the item price into a float\n\nStep 13.a. Check the item price type\n\nchipo.item_price.dtype\n\ndtype('O')\n\n\n\n\nStep 13.b. Create a lambda function and change the type of item price\n\ndollarizer = lambda x: float(x[1:-1])\nchipo.item_price = chipo.item_price.apply(dollarizer)\n\n\n\nStep 13.c. Check the item price type\n\nchipo.item_price.dtype\n\ndtype('float64')\n\n\n\n\n\nStep 14. How much was the revenue for the period in the dataset?\n\nrevenue =  (chipo.item_price * chipo.quantity).sum()\nprint('Revenue is : $ '+ str(revenue))\n\nRevenue is : $ 39237.02\n\n\n\n\nStep 15. How many orders were made in the period?\n\nchipo.order_id.value_counts().count()\n\n1834\n\n\n\n\nStep 16. What is the average revenue amount per order?\n\n# Solution 1\nchipo['revenue'] = chipo['quantity'] * chipo['item_price']\norder_grouped = chipo.groupby(by=['order_id']).sum()\norder_grouped['revenue'].mean()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[8], line 4\n      2 chipo['revenue'] = chipo['quantity'] * chipo['item_price']\n      3 order_grouped = chipo.groupby(by=['order_id']).sum()\n----&gt; 4 order_grouped['revenue'].mean()\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6549, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n   6541 @doc(make_doc(\"mean\", ndim=1))\n   6542 def mean(\n   6543     self,\n   (...)\n   6547     **kwargs,\n   6548 ):\n-&gt; 6549     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n  12413 def mean(\n  12414     self,\n  12415     axis: Axis | None = 0,\n   (...)\n  12418     **kwargs,\n  12419 ) -&gt; Series | float:\n&gt; 12420     return self._stat_function(\n  12421         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n  12422     )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n  12373 nv.validate_func(name, (), kwargs)\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n&gt; 12377 return self._reduce(\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n  12379 )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6457, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   6452     # GH#47500 - change to TypeError to match other methods\n   6453     raise TypeError(\n   6454         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n   6455         \"with non-numeric dtypes.\"\n   6456     )\n-&gt; 6457 return op(delegate, skipna=skipna, **kwds)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147, in bottleneck_switch.__call__.&lt;locals&gt;.f(values, axis, skipna, **kwds)\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n    146 else:\n--&gt; 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n    149 return result\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404, in _datetimelike_compat.&lt;locals&gt;.new_func(values, axis, skipna, mask, **kwargs)\n    401 if datetimelike and mask is None:\n    402     mask = isna(values)\n--&gt; 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n    406 if datetimelike:\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:720, in nanmean(values, axis, skipna, mask)\n    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    719 the_sum = values.sum(axis, dtype=dtype_sum)\n--&gt; 720 the_sum = _ensure_numeric(the_sum)\n    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n    723     count = cast(np.ndarray, count)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1701, in _ensure_numeric(x)\n   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n   1699     if isinstance(x, str):\n   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n-&gt; 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n   1702     try:\n   1703         x = float(x)\n\nTypeError: Could not convert string '$2.39 $3.39 $3.39 $2.39 $16.98 $16.98 $10.98 $1.69 $11.75 $9.25 $9.25 $4.45 $8.75 $8.75 $11.25 $4.45 $2.39 $8.49 $8.49 $2.18 $2.18 $8.75 $4.45 $8.99 $3.39 $10.98 $3.39 $2.39 $8.49 $8.99 $1.09 $8.49 $2.39 $8.99 $1.69 $8.99 $1.09 $8.75 $8.75 $4.45 $2.95 $11.75 $2.15 $4.45 $11.25 $11.75 $8.75 $10.98 $8.99 $3.39 $8.99 $3.99 $8.99 $2.18 $2.18 $10.98 $1.09 $8.99 $2.39 $9.25 $11.25 $11.75 $2.15 $4.45 $9.25 $11.25 $8.75 $8.99 $8.99 $3.39 $8.99 $10.98 $8.99 $1.69 $8.99 $3.99 $8.75 $4.45 $8.75 $8.75 $2.15 $8.75 $11.25 $2.15 $9.25 $8.75 $8.75 $9.25 $8.49 $8.99 $1.09 $9.25 $2.95 $11.75 $11.75 $9.25 $11.75 $4.45 $9.25 $4.45 $11.75 $8.75 $8.75 $4.45 $8.99 $8.99 $3.99 $8.49 $3.39 $8.99 $1.09 $9.25 $4.45 $8.75 $2.95 $4.45 $2.39 $8.49 $8.99 $8.49 $1.09 $8.99 $3.99 $8.75 $9.25 $4.45 $11.25 $4.45 $8.99 $1.09 $9.25 $2.95 $4.45 $11.75 $4.45 $8.49 $2.39 $10.98 $22.50 $22.50 $11.75 $4.45 $11.25 $4.45 $11.25 $4.45 $11.25 $11.25 $11.75 $9.25 $4.45 $11.48 $17.98 $17.98 $1.69 $17.50 $17.50 $4.45 $8.49 $2.39 $17.50 $17.50 $4.45 $4.45 $11.25 $11.75 $10.98 $8.49 $10.98 $2.18 $2.18 $11.48 $8.49 $2.39 $4.45 $11.25 $11.75 $8.75 $8.49 $2.18 $2.18 $8.49 $3.39 $8.49 $8.99 $10.98 $11.48 $8.49 $1.09 $1.09 $9.25 $8.75 $2.95 $9.25 $4.45 $11.25 $11.48 $8.49 $8.49 $8.99 $2.39 $11.25 $8.75 $2.95 $1.09 $8.99 $8.49 $2.39 $10.98 $1.09 $3.99 $11.25 $8.75 $8.49 $3.39 $8.75 $9.25 $2.15 $11.25 $11.25 $11.25 $4.45 $22.50 $22.50 $4.45 $11.75 $8.75 $17.50 $17.50 $8.75 $9.25 $8.75 $2.15 $9.25 $4.30 $4.30 $8.75 $11.25 $2.15 $8.99 $1.09 $8.99 $3.99 $8.75 $2.95 $2.95 $11.75 $5.90 $5.90 $9.25 $9.25 $11.75 $9.25 $2.95 $17.50 $17.50 $8.75 $9.25 $10.98 $8.99 $1.09 $1.09 $1.09 $8.99 $10.98 $1.09 $8.75 $8.75 $9.25 $9.25 $8.75 $8.75 $8.99 $8.99 $8.99 $1.09 $11.75 $1.25 $8.99 $2.39 $9.25 $2.95 $8.99 $3.99 $8.49 $2.39 $8.49 $8.49 $8.49 $1.69 $8.49 $3.99 $8.99 $1.69 $1.09 $23.78 $23.78 $17.50 $17.50 $2.15 $8.75 $9.25 $9.25 $8.75 $4.45 $8.75 $11.25 $11.25 $1.25 $9.25 $4.45 $11.25 $11.75 $11.75 $6.49 $8.99 $2.39 $8.49 $2.39 $11.25 $8.75 $2.15 $8.99 $1.69 $8.75 $11.25 $2.15 $4.45 $8.75 $8.49 $8.99 $17.50 $17.50 $8.49 $1.09 $1.09 $8.75 $1.25 $2.15 $11.08 $8.49 $8.49 $8.99 $2.39 $8.75 $2.15 $1.50 $11.25 $2.15 $8.49 $8.49 $11.75 $9.25 $11.75 $1.25 $11.25 $8.75 $4.45 $6.49 $9.25 $2.95 $11.25 $4.45 $1.25 $1.25 $8.49 $2.39 $2.18 $2.18 $8.49 $2.18 $2.18 $22.16 $22.16 $17.50 $17.50 $8.75 $2.95 $6.49 $8.99 $3.39 $3.39 $8.99 $8.49 $11.25 $2.15 $11.25 $2.95 $11.25 $1.25 $8.99 $1.09 $8.75 $8.75 $9.25 $2.95 $11.75 $1.50 $8.99 $1.09 $11.25 $1.25 $1.25 $11.25 $11.75 $2.15 $8.99 $1.69 $11.75 $6.49 $8.75 $9.25 $11.25 $4.45 $1.25 $11.25 $4.45 $8.49 $8.99 $8.49 $8.99 $11.25 $1.25 $11.75 $1.25 $11.75 $9.25 $4.45 $11.25 $2.15 $32.94 $32.94 $32.94 $1.25 $11.25 $11.48 $1.69 $1.09 $17.50 $17.50 $4.45 $6.49 $9.25 $8.75 $9.25 $9.25 $8.75 $8.75 $2.15 $2.95 $17.50 $17.50 $10.98 $11.48 $11.48 $3.39 $8.99 $1.69 $8.99 $1.09 $10.98 $3.39 $8.99 $1.09 $9.25 $8.75 $11.25 $4.45 $2.95 $9.25 $22.20 $22.20 $22.20 $8.49 $8.99 $8.75 $8.75 $11.75 $8.75 $11.25 $9.25 $11.25 $11.25 $8.75 $11.25 $2.95 $1.25 $8.49 $1.69 $11.75 $11.25 $8.75 $8.75 $4.45 $8.49 $3.99 $8.49 $3.99 $11.48 $1.69 $1.09 $11.25 $1.50 $10.58 $1.69 $9.25 $11.25 $8.75 $9.25 $11.25 $11.25 $8.75 $11.75 $8.75 $8.75 $8.75 $2.15 $11.25 $11.75 $2.50 $2.50 $4.45 $9.25 $4.45 $11.25 $8.49 $3.99 $9.25 $9.25 $11.25 $9.25 $11.75 $11.25 $1.25 $23.50 $23.50 $1.25 $8.99 $8.49 $7.40 $7.40 $8.75 $1.25 $4.45 $8.75 $2.15 $8.75 $4.45 $7.40 $7.40 $7.40 $8.99 $3.99 $8.99 $1.69 $8.75 $8.75 $8.75 $8.75 $11.25 $11.25 $2.95 $8.75 $18.50 $18.50 $8.49 $3.99 $2.95 $9.25 $9.25 $3.00 $3.00 $1.25 $8.75 $9.25 $4.45 $8.75 $11.25 $4.45 $10.98 $22.16 $22.16 $4.45 $8.75 $9.25 $6.49 $9.25 $11.25 $8.75 $9.25 $2.15 $9.25 $4.45 $9.25 $2.95 $9.25 $8.75 $9.25 $1.25 $1.25 $8.75 $8.75 $9.25 $4.45 $11.75 $11.75 $11.75 $9.25 $9.25 $16.98 $16.98 $2.39 $3.39 $3.39 $9.25 $11.75 $11.25 $2.15 $8.75 $9.25 $4.45 $10.98 $11.25 $9.25 $22.50 $22.50 $9.25 $2.95 $1.50 $11.48 $8.49 $1.69 $8.49 $8.49 $8.49 $6.78 $6.78 $11.75 $4.45 $8.75 $4.45 $11.89 $9.39 $8.75 $2.95 $1.25 $9.25 $8.75 $23.78 $23.78 $8.75 $9.25 $2.15 $2.15 $1.25 $8.49 $3.99 $10.98 $1.09 $8.75 $4.45 $8.75 $11.75 $2.95 $4.45 $9.25 $8.75 $8.49 $3.99 $22.50 $22.50 $11.25 $1.25 $8.75 $8.75 $18.50 $18.50 $6.49 $8.75 $8.75 $4.45 $8.49 $3.99 $8.99 $1.09 $8.49 $2.39 $11.48 $1.69 $2.50 $2.50 $9.25 $1.50 $17.50 $17.50 $2.95 $8.75 $4.45 $11.75 $8.75 $8.49 $1.69 $8.49 $3.99 $8.99 $8.99 $3.99 $8.99 $11.25 $4.45 $1.25 $3.99 $10.98 $7.40 $3.00 $7.40 $4.00 $8.49 $3.99 $9.25 $4.45 $11.25 $1.25 $11.75 $1.25 $11.25 $2.15 $11.25 $4.45 $3.75 $3.75 $3.75 $11.75 $8.99 $2.39 $8.75 $4.45 $1.25 $8.99 $8.49 $2.18 $2.18 $8.49 $2.18 $2.18 $1.09 $8.75 $2.95 $1.25 $1.50 $11.25 $9.25 $2.95 $1.25 $8.49 $3.99 $11.48 $3.99 $8.49 $11.25 $1.25 $8.99 $1.69 $11.25 $1.25 $6.49 $8.75 $9.25 $8.75 $2.95 $8.75 $11.75 $8.69 $8.69 $2.29 $3.99 $8.49 $8.75 $8.75 $1.25 $11.75 $11.25 $11.25 $11.25 $1.25 $9.25 $11.75 $6.49 $3.99 $8.49 $11.25 $2.15 $11.25 $11.89 $8.99 $1.69 $8.99 $8.99 $3.99 $8.99 $9.25 $9.25 $2.15 $7.40 $7.40 $8.75 $8.75 $9.25 $4.45 $11.25 $1.25 $11.75 $11.25 $1.25 $3.99 $8.49 $8.49 $8.49 $8.99 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $8.75 $2.95 $1.25 $9.25 $1.25 $2.15 $11.25 $1.25 $4.45 $8.75 $2.50 $2.50 $8.90 $8.90 $8.75 $8.75 $8.75 $11.25 $11.25 $10.98 $3.99 $10.98 $3.99 $1.69 $8.99 $9.25 $8.75 $8.99 $1.09 $9.25 $2.95 $8.75 $9.25 $3.99 $8.49 $8.75 $8.75 $22.50 $22.50 $10.98 $3.27 $3.27 $3.27 $3.99 $8.99 $1.09 $11.08 $8.75 $4.45 $11.08 $3.99 $8.49 $4.30 $4.30 $9.25 $8.75 $11.25 $11.25 $9.25 $8.49 $8.99 $8.49 $8.75 $2.95 $4.45 $9.25 $2.95 $9.25 $8.75 $11.25 $4.45 $16.98 $16.98 $8.49 $2.39 $11.25 $3.75 $3.75 $3.75 $9.25 $4.45 $9.25 $9.25 $4.45 $8.75 $9.25 $8.75 $9.25 $9.25 $9.25 $11.48 $8.99 $22.50 $22.50 $11.75 $11.25 $1.25 $8.75 $2.15 $1.25 $11.25 $8.75 $1.25 $11.25 $1.50 $11.25 $11.25 $9.25 $6.49 $8.90 $8.90 $8.75 $4.45 $11.25 $1.25 $17.50 $17.50 $9.25 $8.75 $11.75 $3.00 $3.00 $8.49 $8.49 $10.98 $8.99 $3.99 $8.75 $4.45 $8.99 $1.69 $11.75 $8.75 $11.25 $4.45 $11.75 $1.25 $11.75 $2.95 $8.99 $8.99 $2.18 $2.18 $17.98 $17.98 $8.99 $8.49 $1.69 $11.75 $11.25 $2.95 $3.75 $3.75 $3.75 $9.25 $11.75 $8.75 $2.15 $1.50 $8.49 $8.49 $3.39 $8.69 $3.89 $8.75 $4.45 $8.75 $11.25 $2.15 $8.75 $8.49 $1.69 $8.49 $8.49 $1.25 $8.75 $11.75 $11.75 $8.99 $1.09 $8.75 $4.45 $8.75 $2.95 $8.75 $2.15 $3.99 $8.49 $8.99 $3.99 $8.49 $1.69 $1.09 $8.99 $1.09 $9.25 $8.75 $8.99 $2.39 $1.25 $1.25 $11.25 $11.25 $9.25 $9.25 $11.25 $1.50 $3.99 $8.49 $11.25 $9.25 $11.25 $17.50 $17.50 $8.75 $8.90 $8.90 $8.75 $8.75 $8.99 $2.39 $11.25 $9.25 $2.15 $11.25 $1.25 $11.75 $1.25 $11.25 $11.75 $1.25 $11.25 $11.25 $8.49 $10.98 $8.75 $1.25 $8.75 $8.49 $8.49 $1.50 $1.50 $8.75 $4.45 $11.25 $1.25 $11.75 $8.49 $2.39 $9.25 $4.45 $9.25 $8.75 $8.99 $1.69 $17.50 $17.50 $2.39 $8.99 $8.99 $11.25 $4.45 $8.75 $4.45 $9.25 $6.49 $10.98 $8.49 $8.49 $1.09 $1.69 $9.25 $4.45 $8.75 $1.25 $2.95 $3.99 $8.49 $11.75 $11.75 $2.15 $11.48 $8.75 $2.15 $1.25 $11.25 $2.15 $1.25 $8.75 $8.75 $6.49 $1.69 $8.99 $8.75 $11.75 $10.98 $1.09 $8.49 $3.39 $8.75 $2.15 $1.25 $11.48 $10.98 $10.98 $8.49 $2.95 $9.25 $9.25 $11.75 $4.45 $11.48 $11.25 $8.75 $4.45 $1.69 $8.99 $8.75 $4.45 $1.50 $11.75 $2.15 $8.99 $2.39 $8.75 $2.95 $1.25 $8.75 $2.15 $1.25 $2.18 $2.18 $2.18 $2.18 $11.48 $8.75 $2.95 $11.75 $11.75 $1.25 $10.58 $8.99 $2.39 $11.75 $4.45 $11.25 $11.25 $17.50 $17.50 $8.75 $8.75 $8.75 $22.50 $22.50 $9.25 $8.75 $4.45 $11.75 $1.25 $11.25 $11.25 $2.95 $8.99 $1.69 $11.25 $4.45 $8.75 $6.49 $8.75 $4.45 $9.25 $4.45 $11.75 $11.75 $4.45 $11.89 $11.75 $11.25 $2.95 $1.50 $4.45 $8.75 $8.99 $1.09 $8.99 $1.09 $3.99 $11.48 $8.49 $9.25 $4.45 $11.48 $9.25 $2.95 $9.25 $8.49 $8.99 $8.99 $8.49 $8.75 $2.95 $4.45 $11.89 $10.58 $8.19 $1.69 $8.75 $2.15 $1.25 $17.50 $17.50 $6.49 $9.25 $2.15 $8.75 $4.45 $8.75 $1.25 $11.48 $11.48 $8.99 $2.18 $2.18 $8.49 $8.99 $2.39 $2.39 $2.18 $2.18 $8.75 $4.45 $11.25 $9.25 $9.25 $11.25 $11.25 $4.45 $2.95 $11.75 $8.49 $8.49 $8.99 $1.69 $9.25 $11.25 $11.75 $9.25 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $10.98 $11.25 $4.45 $10.98 $8.49 $8.99 $3.39 $3.99 $8.99 $1.09 $1.09 $2.39 $17.50 $17.50 $4.45 $11.25 $11.25 $4.45 $9.25 $4.45 $8.75 $2.15 $1.25 $11.89 $2.95 $11.75 $1.25 $11.25 $4.45 $11.48 $11.48 $2.95 $9.25 $8.75 $9.25 $2.95 $11.25 $1.25 $11.75 $1.25 $8.99 $2.39 $1.25 $11.25 $1.25 $11.25 $8.49 $3.99 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $8.75 $11.80 $11.80 $11.80 $11.80 $8.90 $8.90 $5.90 $5.90 $6.49 $10.98 $17.98 $17.98 $2.39 $9.25 $8.75 $2.15 $8.75 $4.45 $8.49 $1.69 $8.19 $8.69 $10.98 $3.99 $11.48 $11.48 $4.45 $8.75 $6.49 $8.75 $8.75 $9.25 $1.25 $4.45 $8.49 $1.69 $9.25 $4.45 $8.99 $1.09 $11.25 $2.95 $11.08 $11.08 $3.89 $10.98 $11.25 $8.75 $11.25 $9.25 $4.30 $4.30 $8.75 $8.49 $3.99 $1.69 $8.99 $8.49 $1.69 $11.75 $11.25 $11.89 $9.25 $2.95 $9.25 $2.95 $8.75 $4.45 $4.45 $8.75 $10.98 $11.48 $8.49 $9.25 $4.45 $11.75 $11.89 $8.99 $8.49 $8.75 $9.25 $8.75 $8.75 $11.75 $11.75 $4.45 $11.25 $11.75 $2.50 $2.50 $8.99 $1.69 $11.75 $2.15 $1.25 $9.25 $8.75 $8.90 $8.90 $9.25 $2.95 $8.75 $11.25 $8.90 $8.90 $11.25 $11.75 $11.48 $1.69 $3.39 $9.25 $2.95 $8.99 $1.69 $8.49 $10.98 $11.25 $2.95 $8.99 $1.69 $8.75 $2.15 $1.25 $8.75 $2.95 $9.25 $2.50 $2.50 $11.25 $1.25 $11.75 $2.50 $2.50 $11.25 $1.50 $8.75 $1.25 $2.95 $11.48 $11.48 $8.75 $8.75 $2.15 $11.75 $1.25 $9.25 $9.25 $6.49 $11.75 $8.49 $8.49 $1.09 $10.98 $8.75 $1.25 $2.15 $11.25 $1.50 $11.25 $11.25 $8.49 $8.49 $8.75 $1.50 $1.25 $1.50 $8.75 $2.50 $2.50 $2.15 $7.40 $7.40 $4.00 $9.25 $9.39 $9.25 $9.25 $9.39 $11.25 $8.90 $8.90 $11.25 $6.00 $6.00 $6.00 $6.00 $11.25 $11.25 $11.25 $22.50 $22.50 $11.48 $1.09 $8.49 $8.49 $17.50 $17.50 $11.25 $1.50 $9.25 $8.75 $3.99 $8.49 $8.75 $8.75 $8.75 $8.75 $8.75 $11.75 $1.50 $11.25 $11.25 $2.95 $8.99 $10.98 $9.25 $8.75 $4.45 $8.49 $1.09 $2.39 $8.75 $8.75 $11.48 $8.99 $8.49 $8.49 $2.39 $10.98 $8.49 $3.99 $11.75 $4.45 $8.75 $2.15 $1.25 $10.98 $8.99 $11.25 $1.50 $8.75 $2.15 $1.25 $8.75 $9.25 $8.75 $11.25 $1.50 $8.75 $1.25 $4.45 $10.98 $8.75 $2.95 $1.25 $8.75 $2.95 $1.25 $8.49 $8.49 $2.39 $11.25 $1.25 $8.75 $8.75 $9.25 $8.75 $11.89 $1.25 $8.75 $2.15 $1.25 $8.99 $1.09 $8.75 $4.45 $26.25 $26.25 $26.25 $8.75 $4.45 $11.75 $2.95 $8.75 $8.75 $11.75 $8.75 $11.25 $11.25 $11.25 $4.45 $1.25 $8.49 $8.49 $8.49 $8.99 $8.99 $2.39 $2.39 $3.99 $8.75 $4.45 $2.15 $9.25 $1.25 $11.25 $11.75 $8.75 $4.45 $11.25 $2.15 $8.75 $4.45 $8.75 $8.75 $1.25 $11.25 $2.15 $8.75 $5.90 $5.90 $11.75 $1.25 $9.25 $3.75 $3.75 $3.75 $8.75 $1.25 $4.45 $11.75 $4.45 $8.75 $23.50 $23.50 $8.75 $2.95 $8.75 $8.75 $11.89 $4.45 $2.95 $1.25 $8.75 $4.45 $2.95 $1.25 $8.75 $2.15 $1.25 $11.75 $2.95 $8.99 $3.39 $9.25 $9.25 $17.50 $17.50 $2.95 $11.89 $1.50 $11.25 $2.95 $9.25 $11.25 $11.25 $2.95 $8.75 $9.25 $4.30 $4.30 $8.75 $8.75 $11.25 $8.75 $4.30 $4.30 $8.75 $1.25 $2.15 $8.49 $8.49 $3.39 $3.39 $10.98 $10.98 $2.39 $11.25 $11.75 $11.75 $1.25 $5.90 $5.90 $8.75 $11.25 $9.25 $4.45 $1.50 $3.39 $8.99 $2.39 $11.25 $2.15 $11.25 $11.75 $11.75 $4.45 $11.75 $4.45 $9.25 $8.75 $8.49 $8.99 $8.49 $8.99 $11.75 $8.75 $8.49 $3.99 $3.89 $11.08 $8.49 $8.99 $8.49 $8.49 $8.49 $11.25 $2.15 $17.50 $17.50 $8.75 $2.95 $8.49 $8.49 $10.98 $1.09 $11.25 $2.15 $2.95 $1.25 $8.75 $9.25 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $8.99 $3.99 $11.75 $2.15 $8.99 $3.39 $9.25 $8.75 $11.25 $11.25 $4.45 $8.75 $2.15 $1.25 $11.75 $4.45 $9.25 $2.95 $8.49 $8.49 $11.25 $8.75 $4.45 $11.25 $11.25 $11.25 $11.25 $4.45 $8.49 $1.69 $8.49 $3.39 $8.75 $11.25 $9.25 $8.75 $11.25 $11.25 $11.75 $11.25 $11.75 $11.25 $11.75 $21.96 $21.96 $10.98 $1.69 $11.48 $8.99 $8.49 $1.69 $9.25 $2.15 $1.50 $11.25 $1.50 $8.75 $8.75 $2.95 $8.49 $1.69 $8.75 $2.95 $1.25 $11.25 $2.15 $11.08 $8.49 $8.49 $8.49 $11.75 $1.25 $11.75 $8.75 $8.75 $8.75 $4.45 $11.25 $1.50 $23.50 $23.50 $11.75 $6.49 $8.75 $4.45 $6.49 $8.75 $2.50 $2.50 $2.15 $8.49 $2.39 $8.75 $11.75 $4.45 $8.99 $10.98 $9.25 $2.95 $9.25 $9.25 $11.75 $8.75 $8.75 $8.75 $10.98 $11.25 $9.25 $8.75 $8.75 $2.15 $11.25 $2.15 $4.45 $11.75 $8.49 $2.39 $9.25 $1.25 $1.25 $1.25 $1.25 $8.75 $2.15 $8.49 $1.69 $11.25 $1.50 $8.75 $8.75 $8.49 $3.99 $8.99 $1.09 $11.25 $1.25 $8.49 $2.39 $8.49 $8.75 $9.25 $11.25 $4.45 $11.25 $11.89 $8.99 $8.49 $8.75 $4.45 $8.75 $11.75 $11.75 $8.90 $8.90 $9.39 $2.95 $8.49 $3.99 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $8.75 $4.45 $8.49 $8.99 $8.49 $3.99 $8.75 $8.75 $2.95 $8.75 $17.50 $17.50 $9.25 $2.95 $8.75 $6.49 $4.30 $4.30 $8.75 $8.75 $2.15 $1.50 $8.49 $8.49 $2.39 $9.25 $4.45 $6.49 $11.75 $4.45 $10.98 $1.69 $9.39 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $11.25 $9.25 $8.75 $11.25 $8.75 $11.25 $2.50 $2.50 $2.50 $2.50 $6.00 $6.00 $6.00 $6.00 $8.90 $8.90 $5.90 $5.90 $11.25 $11.25 $8.49 $10.98 $8.75 $2.15 $1.50 $9.25 $1.25 $1.50 $2.15 $1.25 $8.75 $2.95 $8.49 $3.99 $11.25 $4.30 $4.30 $11.75 $2.15 $18.50 $18.50 $8.49 $2.39 $8.75 $4.45 $11.75 $8.99 $3.99 $9.25 $9.25 $1.50 $8.75 $2.95 $6.49 $11.75 $8.49 $8.99 $8.75 $4.45 $6.49 $22.50 $22.50 $9.25 $2.95 $8.49 $1.69 $10.98 $8.75 $4.45 $11.25 $2.95 $8.99 $8.49 $2.39 $11.75 $6.49 $11.25 $11.75 $2.95 $8.99 $1.69 $8.99 $2.18 $2.18 $1.09 $8.99 $8.99 $1.09 $8.99 $8.99 $8.49 $10.98 $1.09 $11.75 $9.25 $11.25 $11.25 $2.15 $11.25 $8.75 $4.45 $2.95 $11.75 $1.50 $8.99 $10.98 $2.39 $8.75 $2.15 $9.25 $1.50 $8.75 $2.15 $3.99 $8.99 $6.49 $8.75 $8.90 $8.90 $8.99 $3.99 $17.50 $17.50 $11.25 $1.25 $10.98 $9.25 $4.45 $1.25 $3.00 $3.00 $11.25 $4.45 $4.45 $2.95 $9.25 $11.25 $2.15 $11.25 $11.25 $4.45 $2.95 $9.25 $11.25 $1.25 $8.75 $2.95 $1.25 $8.75 $4.45 $11.48 $11.48 $8.49 $2.39 $11.25 $11.75 $2.15 $1.50 $2.15 $8.75 $11.25 $8.90 $8.90 $11.25 $11.25 $1.25 $4.45 $9.25 $9.25 $8.75 $9.25 $8.75 $8.75 $9.25 $8.75 $11.75 $11.75 $8.75 $8.75 $8.90 $8.90 $2.95 $10.98 $8.49 $8.49 $10.98 $8.99 $8.99 $11.75 $17.50 $17.50 $11.75 $3.99 $8.49 $10.98 $1.69 $17.50 $17.50 $8.99 $2.39 $8.99 $2.39 $1.25 $8.75 $2.95 $11.75 $11.25 $17.50 $17.50 $8.49 $8.49 $2.39 $11.25 $1.50 $8.75 $3.00 $3.00 $1.25 $8.75 $4.45 $11.75 $11.75 $4.45 $21.96 $21.96 $8.75 $4.45 $8.75 $11.25 $9.25 $8.99 $2.39 $9.25 $8.75 $10.98 $8.49 $3.99 $3.39 $11.75 $1.50 $4.45 $9.25 $8.75 $1.25 $11.75 $8.75 $1.50 $8.75 $8.75 $2.15 $1.50 $8.75 $2.95 $8.75 $8.75 $17.50 $17.50 $8.75 $6.49 $4.45 $11.25 $11.25 $4.30 $4.30 $8.75 $11.25 $4.45 $8.99 $2.39 $9.25 $9.25 $9.25 $4.45 $11.75 $11.25 $2.95 $2.15 $11.25 $11.25 $8.75 $2.15 $1.50 $9.25 $4.45 $10.98 $8.99 $2.18 $2.18 $8.75 $4.45 $1.25 $8.99 $2.39 $4.45 $8.75 $10.98 $11.75 $1.50 $10.98 $8.99 $8.49 $3.99 $8.99 $8.49 $3.99 $8.49 $8.49 $8.99 $11.25 $11.25 $10.98 $10.98 $10.98 $2.39 $3.39 $8.75 $1.25 $2.95 $11.75 $1.50 $10.98 $1.69 $4.45 $8.75 $8.75 $8.75 $8.75 $4.45 $9.25 $8.75 $11.25 $8.75 $3.99 $8.99 $8.49 $11.25 $11.25 $8.75 $4.45 $8.75 $4.45 $1.25 $8.75 $8.75 $1.50 $2.15 $11.75 $11.75 $11.75 $11.75 $11.75 $1.50 $8.75 $9.25 $1.25 $8.75 $2.15 $8.99 $1.09 $4.45 $11.25 $11.75 $2.15 $8.75 $8.75 $1.25 $9.25 $2.15 $11.75 $11.25 $8.75 $11.25 $4.45 $8.49 $1.69 $8.75 $8.75 $8.99 $8.49 $9.25 $11.25 $2.95 $4.45 $11.75 $6.49 $11.48 $8.99 $4.36 $4.36 $4.36 $4.36 $11.48 $8.99 $8.49 $11.48 $8.75 $2.15 $1.50 $8.99 $1.69 $11.25 $1.25 $9.25 $9.25 $8.75 $9.25 $8.90 $8.90 $2.15 $9.25 $10.98 $8.49 $8.75 $9.25 $4.30 $4.30 $9.25 $8.75 $8.75 $2.15 $1.25 $8.75 $1.25 $8.75 $5.90 $5.90 $9.25 $8.75 $9.25 $4.45 $9.25 $11.75 $2.50 $2.50 $9.25 $2.15 $9.25 $1.50 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $10.98 $8.75 $8.75 $8.75 $2.15 $1.25 $10.98 $8.75 $2.15 $1.50 $8.75 $2.95 $1.25 $9.25 $9.25 $8.49 $2.39 $8.75 $4.45 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.99 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $11.25 $11.25 $8.75 $1.50 $2.15 $16.98 $16.98 $11.75 $1.50 $8.75 $4.30 $4.30 $1.50 $8.75 $2.95 $1.25 $1.25 $9.25 $4.45 $11.25 $8.75 $4.45 $8.75 $2.15 $1.25 $10.98 $1.69 $8.75 $1.25 $8.75 $1.25 $11.25 $8.75 $8.75 $8.49 $1.69 $9.25 $11.75 $8.49 $2.39 $9.25 $2.95 $6.49 $8.75 $8.75 $9.25 $8.75 $6.78 $6.78 $17.98 $17.98 $3.39 $11.75 $11.25 $8.75 $4.45 $11.75 $9.25 $8.75 $6.49 $8.99 $2.39 $8.75 $11.25 $11.75 $4.45 $8.75 $2.15 $9.25 $9.25 $9.25 $11.89 $11.75 $11.25 $9.25 $9.25 $8.75 $8.75 $8.49 $1.69 $1.09 $11.25 $1.50 $11.25 $11.25 $11.75 $1.50 $8.49 $8.99 $22.50 $22.50 $8.75 $4.30 $4.30 $8.75 $11.25 $2.15 $11.25 $2.95 $4.45 $11.25 $8.49 $3.39 $2.39 $11.75 $2.15 $11.75 $8.99 $2.39 $8.75 $11.75 $11.89 $1.25 $7.50 $7.50 $7.50 $7.50 $7.50 $11.89 $1.09 $8.49 $2.39 $8.75 $8.75 $8.75 $8.75 $9.25 $11.25 $8.75 $8.90 $8.90 $9.25 $8.75 $8.75 $11.75 $3.00 $3.00 $1.50 $11.25 $11.75 $8.99 $10.98 $4.45 $8.75 $2.15 $9.25 $11.25 $4.45 $1.69 $10.98 $9.25 $11.75 $9.25 $4.45 $10.98 $3.99 $8.49 $1.25 $9.25 $4.45 $10.98 $8.75 $8.75 $11.75 $11.25 $8.49 $11.48 $4.45 $1.25 $11.25 $8.99 $1.09 $2.39 $11.25 $2.15 $8.75 $4.45 $8.49 $1.69 $10.98 $1.69 $9.25 $4.45 $11.25 $8.75 $11.25 $11.75 $11.25 $22.50 $22.50 $8.49 $2.39 $2.50 $2.50 $8.75 $8.75 $9.25 $9.25 $11.25 $8.99 $1.09 $8.99 $1.69 $11.75 $1.25 $21.96 $21.96 $8.75 $2.15 $1.25 $8.75 $11.25 $9.25 $11.25 $8.75 $8.75 $11.25 $2.15 $8.99 $1.09 $1.69 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $11.48 $8.49 $8.49 $4.78 $4.78 $9.25 $1.25 $1.25 $1.25 $11.25 $11.25 $11.75 $4.45 $11.25 $4.45 $8.99 $1.09 $11.25 $2.15 $11.25 $9.25 $11.75 $11.25 $11.25 $9.25 $2.95 $11.25 $4.45 $8.75 $2.95 $2.95 $11.25 $1.50 $10.98 $16.98 $16.98 $18.50 $18.50 $10.98 $3.99 $1.09 $9.25 $9.25 $8.75 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $1.25 $4.45 $9.25 $8.75 $8.75 $17.50 $17.50 $4.45 $9.39 $1.25 $2.95 $11.25 $8.75 $8.75 $11.25 $2.15 $8.90 $8.90 $11.25 $11.89 $10.98 $11.25 $4.45 $11.25 $11.25 $8.49 $10.98 $8.49 $3.39 $9.25 $8.75 $2.95 $3.00 $3.00 $9.39 $11.75 $2.95 $1.50 $11.25 $11.75 $8.75 $2.15 $1.50 $8.49 $3.39 $11.75 $1.25 $17.50 $17.50 $11.25 $1.25 $8.75 $2.95 $1.25 $11.25 $11.75 $13.35 $13.35 $13.35 $11.25 $11.75 $11.25 $11.25 $4.45 $11.25 $8.49 $3.39 $9.25 $2.95 $4.78 $4.78 $2.39 $3.99 $8.99 $8.99 $11.25 $11.25 $8.75 $11.25 $2.95 $4.45 $9.25 $8.75 $4.45 $8.49 $8.49 $10.98 $10.98 $3.99 $11.75 $8.75 $11.75 $4.45 $1.50 $1.25 $8.49 $8.49 $8.75 $8.75 $8.75 $9.25 $8.75 $2.95 $1.25 $11.25 $1.50 $11.25 $4.45 $9.25 $8.75 $8.75 $9.25 $8.75 $4.45 $1.50 $8.75 $8.75 $8.49 $1.69 $8.75 $2.15 $9.25 $2.15 $1.50 $11.25 $11.75 $2.15 $6.49 $9.25 $9.25 $11.25 $11.25 $11.75 $11.75 $11.75 $11.25 $8.75 $2.15 $1.25 $11.75 $9.25 $11.25 $8.75 $5.90 $5.90 $8.75 $4.45 $9.25 $9.25 $4.45 $11.25 $4.45 $11.25 $8.75 $2.15 $11.89 $11.25 $8.75 $2.95 $1.50 $8.75 $4.30 $4.30 $8.75 $11.25 $11.75 $11.75 $2.15 $11.25 $8.99 $1.09 $8.49 $8.49 $8.49 $3.39 $8.99 $10.98 $3.99 $11.75 $2.15 $8.75 $4.45 $2.50 $2.50 $11.48 $1.09 $8.49 $8.49 $16.98 $16.98 $3.99 $10.98 $1.09 $8.75 $2.95 $8.75 $8.75 $2.95 $9.25 $11.25 $2.15 $9.25 $4.45 $4.45 $9.25 $11.75 $11.75 $2.15 $9.25 $8.75 $11.25 $6.49 $8.75 $11.25 $2.95 $10.98 $3.99 $1.50 $9.25 $2.15 $8.75 $11.25 $11.89 $4.45 $1.50 $1.25 $8.75 $8.75 $4.45 $11.25 $11.75 $8.49 $1.09 $1.09 $1.69 $8.99 $3.39 $8.99 $1.69 $8.49 $8.99 $3.27 $3.27 $3.27 $8.99 $8.99 $1.09 $10.98 $1.69 $3.99 $8.49 $1.09 $8.75 $8.75 $11.75 $8.75 $9.25 $8.75 $3.39 $8.99 $8.99 $2.39 $9.25 $8.75 $9.25 $9.25 $8.99 $3.99 $2.39 $8.49 $1.09 $8.49 $8.99 $3.39 $11.25 $1.25 $8.99 $3.99 $8.75 $8.90 $8.90 $6.49 $8.75 $9.25 $11.25 $11.25 $11.25 $1.25 $8.75 $9.25 $4.45 $1.25 $8.75 $1.25 $2.15 $17.98 $17.98 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $1.50 $8.75 $8.75 $11.08 $8.99 $1.69 $8.99 $1.69 $10.98 $3.99 $3.39 $11.75 $2.15 $11.75 $2.95 $8.75 $8.75 $11.75 $11.25 $11.75 $11.25 $4.45 $11.25 $1.25 $2.18 $2.18 $2.18 $2.18 $2.39 $8.49 $8.99 $2.39 $11.25 $8.75 $11.75 $11.75 $11.25 $4.45 $2.15 $8.19 $10.58 $4.45 $9.25 $1.09 $8.99 $11.25 $1.50 $8.99 $3.99 $4.45 $11.75 $2.15 $11.25 $8.75 $4.45 $8.75 $9.25 $6.45 $6.45 $6.45 $8.75 $11.25 $11.25 $8.75 $11.75 $21.96 $21.96 $8.99 $5.07 $5.07 $5.07 $8.49 $9.25 $11.25 $4.45 $3.39 $8.49 $8.99 $8.49 $17.50 $17.50 $22.96 $22.96 $8.75 $11.25 $11.89 $11.25 $8.49 $1.69 $1.09 $8.99 $8.99 $9.25 $8.75 $9.25 $2.95 $8.49 $3.99 $8.99 $8.49 $7.17 $7.17 $7.17 $8.49 $8.99 $17.50 $17.50 $9.25 $9.25 $11.25 $1.25 $8.99 $1.09 $8.75 $4.45 $11.25 $2.15 $11.75 $11.25 $11.25 $8.75 $8.75 $4.45 $1.25 $11.75 $11.75 $2.50 $2.50 $8.49 $8.99 $2.18 $2.18 $11.25 $4.45 $11.25 $11.75 $8.49 $8.99 $1.69 $1.09 $8.99 $8.99 $11.25 $6.49 $11.25 $8.75 $4.45 $8.99 $1.69 $11.48 $11.75 $2.50 $2.50 $8.49 $1.09 $1.09 $1.69 $8.49 $2.39 $11.75 $1.25 $8.49 $1.69 $8.49 $1.69 $11.75 $4.45 $8.75 $8.75 $4.45 $8.75 $11.25 $11.25 $8.75 $7.98 $7.98 $8.49 $1.09 $8.49 $3.99 $8.49 $3.99 $8.99 $3.99 $11.25 $4.45 $8.49 $2.39 $8.49 $2.39 $3.99 $8.49 $1.25 $11.25 $4.45 $9.25 $4.45 $1.09 $8.99 $3.99 $11.25 $8.90 $8.90 $9.25 $11.25 $8.75 $11.25 $11.25 $11.25 $11.25 $11.25 $8.99 $8.49 $8.75 $8.75 $4.45 $16.98 $16.98 $11.75 $11.25 $9.25 $4.45 $9.25 $2.95 $8.49 $1.69 $3.75 $3.75 $3.75 $4.45 $9.25 $1.50 $11.25 $11.48 $11.25 $2.15 $8.75 $9.39 $8.49 $3.99 $8.19 $2.29 $11.48 $1.69 $11.48 $3.99 $8.49 $1.69 $9.25 $2.95 $8.49 $1.69 $11.25 $4.45 $9.39 $9.25 $8.75 $8.75 $4.45 $11.89 $4.45 $4.45 $8.75 $8.75 $8.75 $2.15 $8.75 $3.75 $3.75 $3.75 $9.25 $11.25 $4.45 $6.49 $16.98 $16.98 $18.50 $18.50 $2.50 $2.50 $2.95 $3.99 $8.49 $8.19 $11.08 $6.49 $11.75 $2.39 $8.99 $1.09 $11.25 $4.45 $11.25 $8.99 $1.69 $21.96 $21.96 $2.18 $2.18 $8.99 $8.99 $2.39 $8.69 $1.69 $8.90 $8.90 $2.50 $2.50 $8.75 $8.99 $1.09 $8.49 $8.49 $8.75 $4.45 $17.50 $17.50 $8.75 $9.25 $8.49 $2.39 $8.75 $4.45 $11.25 $11.25 $11.75 $8.75 $8.49 $8.49 $8.49 $8.99 $8.75 $4.45 $11.48 $8.75 $1.25 $2.15 $9.25 $4.45 $11.75 $2.15 $11.25 $8.99 $2.39 $8.69 $8.69 $11.75 $2.95 $11.75 $1.50 $9.25 $4.45 $1.50 $11.48 $8.99 $2.39 $11.25 $11.89 $2.15 $1.25 $11.75 $4.45 $8.75 $8.75 $11.25 $4.45 $11.25 $2.15 $4.45 $8.49 $1.09 $3.99 $11.25 $11.25 $8.49 $2.39 $8.99 $2.39 $11.25 $2.15 $8.75 $2.95 $1.25 $8.75 $11.25 $17.50 $17.50 $11.75 $11.75 $11.25 $11.25 $4.45 $2.50 $2.50 $8.75 $8.99 $8.99 $1.69 $8.99 $1.69 $11.25 $1.25 $11.08 $8.69 $8.99 $1.09 $11.25 $11.25 $2.95 $1.25 $8.75 $1.25 $8.75 $8.75 $2.15 $1.25 $8.49 $3.99 $8.49 $2.39 $8.49 $7.17 $7.17 $7.17 $8.75 $4.45 $11.48 $8.75 $8.75 $11.48 $8.75 $9.25 $8.49 $3.99 $1.50 $11.25 $11.25 $8.75 $8.75 $4.45 $9.25 $4.45 $8.75 $8.75 $4.30 $4.30 $2.95 $8.75 $4.50 $4.50 $4.50 $9.25 $11.25 $4.45 $11.25 $11.25 $8.75 $9.25 $8.75 $2.15 $1.25 $8.75 $2.15 $1.25 $8.99 $8.49 $8.75 $8.75 $1.25 $11.75 $4.50 $4.50 $4.50 $8.75 $8.75 $3.99 $3.39 $8.49 $2.39 $8.99 $1.50 $11.25 $11.25 $8.75 $8.75 $2.15 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $26.07 $26.07 $26.07 $11.75 $1.50 $8.99 $8.99 $11.48 $9.25 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $8.75 $2.15 $1.25 $11.89 $8.75 $11.75 $4.45 $18.50 $18.50 $9.25 $9.39 $8.49 $2.39 $8.49 $1.69 $1.09 $8.99 $8.49 $2.18 $2.18 $11.25 $1.25 $8.49 $3.39 $8.49 $3.99 $11.25 $8.75 $8.49 $1.69 $16.98 $16.98 $9.25 $9.25 $11.75 $1.25 $11.25 $8.75 $8.49 $8.49 $8.75 $1.25 $1.25 $1.25 $11.25 $12.98 $12.98 $11.75 $11.75 $4.45 $11.25 $11.75 $10.98 $8.49 $8.49 $2.39 $9.25 $11.25 $8.75 $2.95 $1.50 $11.25 $2.95 $9.25 $2.95 $9.25 $8.75 $11.25 $8.75 $8.75 $4.45 $11.25 $1.25 $8.75 $2.95 $2.50 $2.50 $9.25 $9.25 $9.25 $6.49 $17.50 $17.50 $8.49 $1.69 $8.49 $3.99 $8.75 $2.95 $2.95 $8.49 $8.99 $8.99 $1.09 $8.75 $4.45 $1.25 $11.25 $11.75 $11.25 $9.25 $11.25 $1.50 $11.25 $2.15 $10.98 $8.75 $11.75 $2.95 $11.25 $11.25 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $1.25 $11.25 $1.50 $2.15 $8.75 $8.75 $11.25 $8.75 $8.75 $11.25 $4.45 $8.49 $8.49 $8.49 $8.49 $8.99 $1.69 $2.39 $1.09 $1.09 $11.25 $2.95 $35.25 $35.25 $35.25 $8.75 $2.95 $1.25 $11.25 $2.15 $9.25 $4.45 $8.75 $8.75 $8.75 $4.45 $1.25 $11.89 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $8.69 $8.69 $9.25 $2.95 $10.98 $2.39 $22.50 $22.50 $21.96 $21.96 $10.98 $8.49 $1.69 $11.75 $2.95 $8.75 $11.25 $8.75 $9.25 $8.75 $8.19 $10.58 $8.75 $2.95 $1.50 $2.50 $2.50 $9.25 $4.45 $9.25 $11.25 $8.69 $8.69 $3.89 $8.69 $1.69 $4.45 $9.25 $11.25 $4.45 $2.15 $8.19 $8.69 $4.45 $11.25 $1.25 $11.25 $8.75 $11.89 $11.75 $11.75 $9.25 $8.75 $2.15 $1.50 $11.75 $4.45 $3.99 $8.99 $10.98 $2.39 $1.25 $2.95 $8.75 $2.95 $9.25 $9.25 $9.25 $8.75 $2.15 $9.25 $9.25 $3.39 $8.49 $8.49 $2.39 $10.98 $1.09 $1.09 $8.49 $11.25 $1.25 $8.99 $1.09 $8.75 $2.15 $1.50 $3.99 $8.49 $8.75 $2.15 $1.50 $8.49 $10.98 $2.18 $2.18 $8.75 $2.15 $1.50 $4.45 $9.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $6.49 $33.75 $33.75 $33.75 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $3.00 $3.00 $11.25 $11.75 $10.98 $2.39 $2.50 $2.50 $8.75 $4.45 $16.98 $16.98 $8.75 $6.49 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $16.98 $16.98 $8.99 $8.99 $8.49 $9.25 $8.75 $2.95 $11.48 $2.39 $8.99 $2.39 $11.25 $4.45 $8.75 $9.25 $6.49 $26.25 $26.25 $26.25 $8.75 $26.25 $26.25 $26.25 $8.75 $8.75 $11.25 $11.25 $2.15 $1.25 $11.75 $8.75 $2.15 $1.50 $11.25 $2.15 $8.99 $2.39 $11.25 $11.25 $2.15 $11.25 $11.25 $8.75 $4.78 $4.78 $21.96 $21.96 $8.49 $2.39 $9.25 $2.95 $16.98 $16.98 $8.19 $3.89 $8.99 $1.09 $8.99 $3.39 $9.25 $4.45 $10.98 $10.98 $17.50 $17.50 $11.25 $1.25 $8.75 $11.75 $4.45 $11.25 $11.25 $8.99 $1.09 $10.98 $8.49 $1.69 $11.25 $9.25 $16.98 $16.98 $8.75 $4.45 $11.25 $6.49 $11.75 $9.25 $9.25 $8.75 $4.45 $2.50 $2.50 $8.75 $11.75 $8.75 $9.25 $11.75 $9.25 $8.75 $9.25 $8.75 $11.25 $11.75 $9.25 $8.75 $11.75 $8.49 $1.09 $1.09 $8.49 $1.09 $1.69 $11.25 $1.25 $8.75 $2.15 $1.50 $8.49 $1.69 $1.25 $8.75 $2.95 $8.49 $3.99 $8.49 $8.49 $8.75 $11.25 $2.15 $1.50 $11.75 $8.99 $1.09 $10.98 $10.98 $11.25 $1.25 $8.75 $4.45 $4.45 $1.25 $11.89 $8.99 $8.99 $11.25 $4.45 $23.50 $23.50 $8.49 $3.99 $9.25 $4.45 $4.45 $9.25 $8.75 $4.45 $8.75 $8.75 $11.75 $6.49 $17.50 $17.50 $4.45 $8.75 $2.95 $1.50 $8.75 $8.75 $8.75 $4.45 $11.25 $11.25 $11.75 $11.25 $2.95 $11.25 $4.45 $3.00 $3.00 $1.25 $2.95 $9.25 $8.99 $2.39 $6.49 $8.75 $8.90 $8.90 $11.48 $1.09 $10.98 $9.25 $9.25 $11.25 $8.75 $11.75 $11.25 $11.25 $1.25 $9.25 $4.45 $9.25 $6.49 $11.75 $11.75 $8.99 $2.39 $8.49 $8.49 $9.25 $9.25 $1.25 $8.75 $2.95 $11.75 $2.15 $8.49 $8.49 $8.69 $16.38 $16.38 $8.19 $3.89 $2.29 $11.75 $8.75 $8.75 $8.75 $4.45 $8.49 $8.49 $9.25 $8.75 $6.49 $2.95 $11.25 $11.25 $2.15 $9.25 $11.75 $21.96 $21.96 $8.49 $3.39 $1.69 $8.49 $8.75 $4.45 $8.49 $3.99 $11.25 $8.75 $11.25 $2.15 $11.75 $4.45 $11.25 $9.25 $8.75 $18.50 $18.50 $1.50 $8.75 $2.15 $11.48 $2.18 $2.18 $3.99 $11.25 $1.50 $8.99 $2.39 $11.75 $1.50 $11.25 $6.49 $4.45 $11.25 $8.49 $3.99 $2.50 $2.50 $8.75 $9.25 $3.99 $8.99 $8.75 $6.49 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $16.98 $16.98 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $8.75 $8.75 $11.25 $11.25 $8.49 $1.09 $1.69 $1.25 $9.25 $2.95 $8.69 $8.19 $8.49 $2.39 $8.49 $2.39 $10.98 $8.99 $8.99 $1.69 $8.49 $8.75 $8.75 $11.25 $4.45 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $2.15 $1.50 $1.50 $8.99 $1.09 $8.75 $4.45 $8.75 $8.75 $11.25 $4.30 $4.30 $8.49 $1.69 $1.09 $1.09 $8.75 $2.15 $1.50 $8.99 $8.49 $3.99 $8.75 $2.15 $1.25 $9.25 $2.95 $11.25 $4.45 $9.25 $2.95 $3.99 $8.99 $8.49 $8.75 $8.75 $11.25 $9.25 $8.75 $4.45 $11.25 $1.25 $9.25 $11.25 $4.45 $2.95 $10.98 $8.75 $8.75 $18.50 $18.50 $9.25 $9.25 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $16.98 $16.98 $11.25 $2.95 $8.75 $2.15 $1.50 $8.49 $2.39 $9.25 $2.15 $1.25 $8.19 $8.69 $8.19 $8.19 $8.75 $2.95 $1.25 $9.25 $2.95 $11.25 $8.75 $11.25 $11.25 $8.99 $1.09 $9.25 $9.25 $4.45 $8.49 $3.99 $2.39 $1.09 $8.99 $8.49 $8.75 $8.75 $11.25 $11.75 $4.45 $2.50 $2.50 $8.75 $8.49 $3.39 $8.75 $9.25 $4.45 $1.25 $11.25 $2.15 $4.45 $2.50 $2.50 $8.99 $3.99 $8.75 $2.15 $11.75 $11.75 $1.25 $8.75 $9.39 $11.25 $9.25 $9.25 $2.95 $9.25 $4.45 $1.25 $9.25 $8.75 $11.75 $1.50 $8.75 $4.45 $8.99 $1.09 $9.25 $2.95 $8.99 $1.69 $8.69 $1.69 $11.25 $4.45 $8.75 $8.75 $4.45 $11.25 $8.75 $2.95 $1.50 $8.19 $8.69 $1.09 $1.69 $8.49 $8.75 $2.95 $1.25 $8.49 $3.99 $10.98 $3.39 $11.25 $11.25 $2.15 $18.50 $18.50 $8.49 $8.49 $11.25 $1.50 $8.49 $2.39 $8.99 $2.39 $11.75 $4.45 $17.50 $17.50 $9.25 $9.25 $8.75 $4.45 $3.75 $3.75 $3.75 $8.75 $4.45 $11.75 $2.95 $1.25 $4.45 $8.75 $1.25 $1.50 $9.25 $11.25 $11.25 $11.25 $11.25 $9.25 $11.25 $4.45 $8.75 $9.25 $8.75 $8.75 $4.45 $8.75 $1.25 $2.15 $8.75 $8.75 $4.30 $4.30 $8.75 $1.25 $2.95 $9.25 $2.95 $4.45 $11.25 $11.25 $9.25 $9.25 $4.50 $4.50 $4.50 $11.75 $1.25 $11.75 $11.75 $1.25 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $2.15 $1.25 $11.25 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $11.25 $4.45 $4.45 $2.95 $11.25 $2.15 $1.25 $1.50 $8.75 $11.25 $2.95 $11.25 $1.25 $2.15 $11.25 $9.25 $6.49 $1.25 $8.75 $2.15 $8.75 $6.49 $11.25 $1.50 $8.75 $4.45 $8.75 $4.45 $9.25 $9.25 $1.25 $1.25 $8.75 $4.50 $4.50 $4.50 $11.25 $1.25 $1.50 $9.25 $2.15 $11.25 $4.45 $11.25 $4.45 $8.75 $4.45 $9.25 $4.45 $1.25 $11.25 $4.45 $8.75 $4.45 $8.75 $2.15 $8.75 $4.45 $8.75 $11.75 $1.50 $11.25 $4.45 $8.75 $2.15 $1.50 $8.75 $4.45 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $1.50 $8.75 $2.15 $11.75 $2.15 $9.25 $2.95 $18.50 $18.50 $1.25 $4.45 $8.50 $8.50 $8.50 $8.50 $11.25 $11.89 $1.25 $9.39 $4.45 $8.75 $2.15 $1.50 $11.75 $8.75 $9.25 $9.25 $4.45 $1.25 $11.25 $9.25 $2.95 $8.75 $8.75 $2.15 $8.75 $11.25 $11.25 $11.25 $11.75 $11.25 $2.15 $11.25 $2.15 $8.99 $8.99 $8.75 $9.25 $9.25 $11.25 $8.75 $4.45 $8.75 $1.25 $4.45 $11.25 $1.25 $8.75 $8.75 $11.25 $8.75 $1.25 $1.25 $1.25 $9.25 $11.75 $2.15 $8.75 $4.45 $8.75 $4.45 $11.25 $9.25 $8.75 $9.25 $4.45 $11.75 $4.45 $1.25 $4.45 $11.75 $9.25 $11.25 $2.15 $23.50 $23.50 $9.25 $2.15 $18.50 $18.50 $8.75 $5.90 $5.90 $11.89 $4.45 $8.75 $4.45 $9.25 $2.95 $11.25 $2.95 $11.25 $11.25 $11.25 $2.95 $11.75 $9.25 $9.25 $2.95 $11.25 $2.15 $9.25 $8.90 $8.90 $8.75 $11.25 $11.25 $11.25 $8.75 $2.15 $1.25 $11.25 $1.25 $8.75 $4.45 $1.25 $11.25 $11.25 $11.75 $1.25 $11.25 $11.25 $11.25 $8.75 $8.75 $18.50 $18.50 $11.75 $1.25 $4.45 $9.25 $6.49 $4.45 $8.75 $11.25 $6.49 $11.75 $8.75 $9.25 $11.25 $2.15 $8.75 $4.45 $11.25 $4.45 $8.75 $9.25 $4.45 $1.25 $1.25 $8.75 $11.25 $11.75 $2.15 $11.75 $4.45 $11.75 $1.25 $11.75 $11.75 $11.25 $4.30 $4.30 $9.39 $9.39 $8.75 $1.25 $9.25 $4.45 $11.25 $1.25 $8.75 $1.25 $2.95 $11.25 $4.45 $8.75 $1.50 $4.45 $4.45 $9.25 $8.75 $2.95 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $8.75 $4.30 $4.30 $9.25 $9.39 $4.45 $9.25 $1.25 $22.50 $22.50 $4.45 $2.95 $2.15 $23.50 $23.50 $11.75 $2.15 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $8.75 $11.75 $11.25 $8.75 $4.45 $11.75 $1.50 $8.75 $8.75 $11.75 $9.25 $11.25 $4.45 $11.75 $9.25 $4.45 $11.25 $8.75 $8.75 $2.15 $1.50 $8.75 $4.45 $9.25 $8.75 $1.50 $1.25 $1.25 $1.25 $8.75 $2.95 $11.25 $11.25 $1.50 $11.75 $11.25 $2.15 $9.25 $8.75 $11.75 $2.95 $1.50 $8.75 $1.50 $1.25 $8.75 $11.75 $11.25 $11.25 $11.75 $11.25 $11.75 $8.75 $17.80 $17.80 $17.80 $17.80 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $1.25 $11.25 $1.25 $2.15 $11.25 $2.50 $2.50 $9.25 $1.25 $11.25 $2.95 $11.75 $2.15 $11.25 $1.50 $8.99 $1.99 $11.49 $8.75 $4.45 $1.25 $8.75 $4.45 $1.25 $1.50 $11.75 $8.75 $8.75 $11.25 $6.49 $11.75 $8.75 $2.15 $1.25 $6.49 $8.75 $4.45 $8.75 $4.45 $8.75 $11.25 $4.45 $6.49 $9.25 $8.75 $1.25 $4.45 $11.25 $8.75 $1.50 $8.75 $1.50 $1.25 $9.25 $9.39 $4.45 $9.25 $8.75 $4.45 $1.25 $11.25 $11.75 $8.75 $11.25 $9.25 $8.75 $11.25 $2.50 $2.50 $17.50 $17.50 $9.25 $4.45 $11.25 $1.25 $8.75 $4.45 $1.50 $8.75 $1.50 $1.25 $9.39 $8.75 $8.75 $4.45 $11.25 $1.25 $9.25 $4.45 $11.25 $8.75 $3.00 $3.00 $8.75 $2.15 $1.25 $11.25 $11.25 $4.45 $11.25 $11.25 $8.75 $11.75 $11.75 $11.75 $8.75 $4.45 $1.25 $1.50 $8.75 $4.45 $1.25 $9.25 $9.25 $8.75 $4.45 $1.25 $11.75 $11.25 $1.25 $11.75 $11.25 $9.25 $2.15 $1.50 $8.75 $4.45 $11.75 $11.75 $11.25 $8.75 $8.75 ' to numeric\n\n\n\n\n# Solution 2\n\nchipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 # Solution 2\n----&gt; 3 chipo.groupby(by=['order_id']).sum()['revenue'].mean()\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6549, in Series.mean(self, axis, skipna, numeric_only, **kwargs)\n   6541 @doc(make_doc(\"mean\", ndim=1))\n   6542 def mean(\n   6543     self,\n   (...)\n   6547     **kwargs,\n   6548 ):\n-&gt; 6549     return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12420, in NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)\n  12413 def mean(\n  12414     self,\n  12415     axis: Axis | None = 0,\n   (...)\n  12418     **kwargs,\n  12419 ) -&gt; Series | float:\n&gt; 12420     return self._stat_function(\n  12421         \"mean\", nanops.nanmean, axis, skipna, numeric_only, **kwargs\n  12422     )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12377, in NDFrame._stat_function(self, name, func, axis, skipna, numeric_only, **kwargs)\n  12373 nv.validate_func(name, (), kwargs)\n  12375 validate_bool_kwarg(skipna, \"skipna\", none_allowed=False)\n&gt; 12377 return self._reduce(\n  12378     func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only\n  12379 )\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6457, in Series._reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\n   6452     # GH#47500 - change to TypeError to match other methods\n   6453     raise TypeError(\n   6454         f\"Series.{name} does not allow {kwd_name}={numeric_only} \"\n   6455         \"with non-numeric dtypes.\"\n   6456     )\n-&gt; 6457 return op(delegate, skipna=skipna, **kwds)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147, in bottleneck_switch.__call__.&lt;locals&gt;.f(values, axis, skipna, **kwds)\n    145         result = alt(values, axis=axis, skipna=skipna, **kwds)\n    146 else:\n--&gt; 147     result = alt(values, axis=axis, skipna=skipna, **kwds)\n    149 return result\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:404, in _datetimelike_compat.&lt;locals&gt;.new_func(values, axis, skipna, mask, **kwargs)\n    401 if datetimelike and mask is None:\n    402     mask = isna(values)\n--&gt; 404 result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)\n    406 if datetimelike:\n    407     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:720, in nanmean(values, axis, skipna, mask)\n    718 count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n    719 the_sum = values.sum(axis, dtype=dtype_sum)\n--&gt; 720 the_sum = _ensure_numeric(the_sum)\n    722 if axis is not None and getattr(the_sum, \"ndim\", False):\n    723     count = cast(np.ndarray, count)\n\nFile c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:1701, in _ensure_numeric(x)\n   1698 elif not (is_float(x) or is_integer(x) or is_complex(x)):\n   1699     if isinstance(x, str):\n   1700         # GH#44008, GH#36703 avoid casting e.g. strings to numeric\n-&gt; 1701         raise TypeError(f\"Could not convert string '{x}' to numeric\")\n   1702     try:\n   1703         x = float(x)\n\nTypeError: Could not convert string '$2.39 $3.39 $3.39 $2.39 $16.98 $16.98 $10.98 $1.69 $11.75 $9.25 $9.25 $4.45 $8.75 $8.75 $11.25 $4.45 $2.39 $8.49 $8.49 $2.18 $2.18 $8.75 $4.45 $8.99 $3.39 $10.98 $3.39 $2.39 $8.49 $8.99 $1.09 $8.49 $2.39 $8.99 $1.69 $8.99 $1.09 $8.75 $8.75 $4.45 $2.95 $11.75 $2.15 $4.45 $11.25 $11.75 $8.75 $10.98 $8.99 $3.39 $8.99 $3.99 $8.99 $2.18 $2.18 $10.98 $1.09 $8.99 $2.39 $9.25 $11.25 $11.75 $2.15 $4.45 $9.25 $11.25 $8.75 $8.99 $8.99 $3.39 $8.99 $10.98 $8.99 $1.69 $8.99 $3.99 $8.75 $4.45 $8.75 $8.75 $2.15 $8.75 $11.25 $2.15 $9.25 $8.75 $8.75 $9.25 $8.49 $8.99 $1.09 $9.25 $2.95 $11.75 $11.75 $9.25 $11.75 $4.45 $9.25 $4.45 $11.75 $8.75 $8.75 $4.45 $8.99 $8.99 $3.99 $8.49 $3.39 $8.99 $1.09 $9.25 $4.45 $8.75 $2.95 $4.45 $2.39 $8.49 $8.99 $8.49 $1.09 $8.99 $3.99 $8.75 $9.25 $4.45 $11.25 $4.45 $8.99 $1.09 $9.25 $2.95 $4.45 $11.75 $4.45 $8.49 $2.39 $10.98 $22.50 $22.50 $11.75 $4.45 $11.25 $4.45 $11.25 $4.45 $11.25 $11.25 $11.75 $9.25 $4.45 $11.48 $17.98 $17.98 $1.69 $17.50 $17.50 $4.45 $8.49 $2.39 $17.50 $17.50 $4.45 $4.45 $11.25 $11.75 $10.98 $8.49 $10.98 $2.18 $2.18 $11.48 $8.49 $2.39 $4.45 $11.25 $11.75 $8.75 $8.49 $2.18 $2.18 $8.49 $3.39 $8.49 $8.99 $10.98 $11.48 $8.49 $1.09 $1.09 $9.25 $8.75 $2.95 $9.25 $4.45 $11.25 $11.48 $8.49 $8.49 $8.99 $2.39 $11.25 $8.75 $2.95 $1.09 $8.99 $8.49 $2.39 $10.98 $1.09 $3.99 $11.25 $8.75 $8.49 $3.39 $8.75 $9.25 $2.15 $11.25 $11.25 $11.25 $4.45 $22.50 $22.50 $4.45 $11.75 $8.75 $17.50 $17.50 $8.75 $9.25 $8.75 $2.15 $9.25 $4.30 $4.30 $8.75 $11.25 $2.15 $8.99 $1.09 $8.99 $3.99 $8.75 $2.95 $2.95 $11.75 $5.90 $5.90 $9.25 $9.25 $11.75 $9.25 $2.95 $17.50 $17.50 $8.75 $9.25 $10.98 $8.99 $1.09 $1.09 $1.09 $8.99 $10.98 $1.09 $8.75 $8.75 $9.25 $9.25 $8.75 $8.75 $8.99 $8.99 $8.99 $1.09 $11.75 $1.25 $8.99 $2.39 $9.25 $2.95 $8.99 $3.99 $8.49 $2.39 $8.49 $8.49 $8.49 $1.69 $8.49 $3.99 $8.99 $1.69 $1.09 $23.78 $23.78 $17.50 $17.50 $2.15 $8.75 $9.25 $9.25 $8.75 $4.45 $8.75 $11.25 $11.25 $1.25 $9.25 $4.45 $11.25 $11.75 $11.75 $6.49 $8.99 $2.39 $8.49 $2.39 $11.25 $8.75 $2.15 $8.99 $1.69 $8.75 $11.25 $2.15 $4.45 $8.75 $8.49 $8.99 $17.50 $17.50 $8.49 $1.09 $1.09 $8.75 $1.25 $2.15 $11.08 $8.49 $8.49 $8.99 $2.39 $8.75 $2.15 $1.50 $11.25 $2.15 $8.49 $8.49 $11.75 $9.25 $11.75 $1.25 $11.25 $8.75 $4.45 $6.49 $9.25 $2.95 $11.25 $4.45 $1.25 $1.25 $8.49 $2.39 $2.18 $2.18 $8.49 $2.18 $2.18 $22.16 $22.16 $17.50 $17.50 $8.75 $2.95 $6.49 $8.99 $3.39 $3.39 $8.99 $8.49 $11.25 $2.15 $11.25 $2.95 $11.25 $1.25 $8.99 $1.09 $8.75 $8.75 $9.25 $2.95 $11.75 $1.50 $8.99 $1.09 $11.25 $1.25 $1.25 $11.25 $11.75 $2.15 $8.99 $1.69 $11.75 $6.49 $8.75 $9.25 $11.25 $4.45 $1.25 $11.25 $4.45 $8.49 $8.99 $8.49 $8.99 $11.25 $1.25 $11.75 $1.25 $11.75 $9.25 $4.45 $11.25 $2.15 $32.94 $32.94 $32.94 $1.25 $11.25 $11.48 $1.69 $1.09 $17.50 $17.50 $4.45 $6.49 $9.25 $8.75 $9.25 $9.25 $8.75 $8.75 $2.15 $2.95 $17.50 $17.50 $10.98 $11.48 $11.48 $3.39 $8.99 $1.69 $8.99 $1.09 $10.98 $3.39 $8.99 $1.09 $9.25 $8.75 $11.25 $4.45 $2.95 $9.25 $22.20 $22.20 $22.20 $8.49 $8.99 $8.75 $8.75 $11.75 $8.75 $11.25 $9.25 $11.25 $11.25 $8.75 $11.25 $2.95 $1.25 $8.49 $1.69 $11.75 $11.25 $8.75 $8.75 $4.45 $8.49 $3.99 $8.49 $3.99 $11.48 $1.69 $1.09 $11.25 $1.50 $10.58 $1.69 $9.25 $11.25 $8.75 $9.25 $11.25 $11.25 $8.75 $11.75 $8.75 $8.75 $8.75 $2.15 $11.25 $11.75 $2.50 $2.50 $4.45 $9.25 $4.45 $11.25 $8.49 $3.99 $9.25 $9.25 $11.25 $9.25 $11.75 $11.25 $1.25 $23.50 $23.50 $1.25 $8.99 $8.49 $7.40 $7.40 $8.75 $1.25 $4.45 $8.75 $2.15 $8.75 $4.45 $7.40 $7.40 $7.40 $8.99 $3.99 $8.99 $1.69 $8.75 $8.75 $8.75 $8.75 $11.25 $11.25 $2.95 $8.75 $18.50 $18.50 $8.49 $3.99 $2.95 $9.25 $9.25 $3.00 $3.00 $1.25 $8.75 $9.25 $4.45 $8.75 $11.25 $4.45 $10.98 $22.16 $22.16 $4.45 $8.75 $9.25 $6.49 $9.25 $11.25 $8.75 $9.25 $2.15 $9.25 $4.45 $9.25 $2.95 $9.25 $8.75 $9.25 $1.25 $1.25 $8.75 $8.75 $9.25 $4.45 $11.75 $11.75 $11.75 $9.25 $9.25 $16.98 $16.98 $2.39 $3.39 $3.39 $9.25 $11.75 $11.25 $2.15 $8.75 $9.25 $4.45 $10.98 $11.25 $9.25 $22.50 $22.50 $9.25 $2.95 $1.50 $11.48 $8.49 $1.69 $8.49 $8.49 $8.49 $6.78 $6.78 $11.75 $4.45 $8.75 $4.45 $11.89 $9.39 $8.75 $2.95 $1.25 $9.25 $8.75 $23.78 $23.78 $8.75 $9.25 $2.15 $2.15 $1.25 $8.49 $3.99 $10.98 $1.09 $8.75 $4.45 $8.75 $11.75 $2.95 $4.45 $9.25 $8.75 $8.49 $3.99 $22.50 $22.50 $11.25 $1.25 $8.75 $8.75 $18.50 $18.50 $6.49 $8.75 $8.75 $4.45 $8.49 $3.99 $8.99 $1.09 $8.49 $2.39 $11.48 $1.69 $2.50 $2.50 $9.25 $1.50 $17.50 $17.50 $2.95 $8.75 $4.45 $11.75 $8.75 $8.49 $1.69 $8.49 $3.99 $8.99 $8.99 $3.99 $8.99 $11.25 $4.45 $1.25 $3.99 $10.98 $7.40 $3.00 $7.40 $4.00 $8.49 $3.99 $9.25 $4.45 $11.25 $1.25 $11.75 $1.25 $11.25 $2.15 $11.25 $4.45 $3.75 $3.75 $3.75 $11.75 $8.99 $2.39 $8.75 $4.45 $1.25 $8.99 $8.49 $2.18 $2.18 $8.49 $2.18 $2.18 $1.09 $8.75 $2.95 $1.25 $1.50 $11.25 $9.25 $2.95 $1.25 $8.49 $3.99 $11.48 $3.99 $8.49 $11.25 $1.25 $8.99 $1.69 $11.25 $1.25 $6.49 $8.75 $9.25 $8.75 $2.95 $8.75 $11.75 $8.69 $8.69 $2.29 $3.99 $8.49 $8.75 $8.75 $1.25 $11.75 $11.25 $11.25 $11.25 $1.25 $9.25 $11.75 $6.49 $3.99 $8.49 $11.25 $2.15 $11.25 $11.89 $8.99 $1.69 $8.99 $8.99 $3.99 $8.99 $9.25 $9.25 $2.15 $7.40 $7.40 $8.75 $8.75 $9.25 $4.45 $11.25 $1.25 $11.75 $11.25 $1.25 $3.99 $8.49 $8.49 $8.49 $8.99 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $8.75 $2.95 $1.25 $9.25 $1.25 $2.15 $11.25 $1.25 $4.45 $8.75 $2.50 $2.50 $8.90 $8.90 $8.75 $8.75 $8.75 $11.25 $11.25 $10.98 $3.99 $10.98 $3.99 $1.69 $8.99 $9.25 $8.75 $8.99 $1.09 $9.25 $2.95 $8.75 $9.25 $3.99 $8.49 $8.75 $8.75 $22.50 $22.50 $10.98 $3.27 $3.27 $3.27 $3.99 $8.99 $1.09 $11.08 $8.75 $4.45 $11.08 $3.99 $8.49 $4.30 $4.30 $9.25 $8.75 $11.25 $11.25 $9.25 $8.49 $8.99 $8.49 $8.75 $2.95 $4.45 $9.25 $2.95 $9.25 $8.75 $11.25 $4.45 $16.98 $16.98 $8.49 $2.39 $11.25 $3.75 $3.75 $3.75 $9.25 $4.45 $9.25 $9.25 $4.45 $8.75 $9.25 $8.75 $9.25 $9.25 $9.25 $11.48 $8.99 $22.50 $22.50 $11.75 $11.25 $1.25 $8.75 $2.15 $1.25 $11.25 $8.75 $1.25 $11.25 $1.50 $11.25 $11.25 $9.25 $6.49 $8.90 $8.90 $8.75 $4.45 $11.25 $1.25 $17.50 $17.50 $9.25 $8.75 $11.75 $3.00 $3.00 $8.49 $8.49 $10.98 $8.99 $3.99 $8.75 $4.45 $8.99 $1.69 $11.75 $8.75 $11.25 $4.45 $11.75 $1.25 $11.75 $2.95 $8.99 $8.99 $2.18 $2.18 $17.98 $17.98 $8.99 $8.49 $1.69 $11.75 $11.25 $2.95 $3.75 $3.75 $3.75 $9.25 $11.75 $8.75 $2.15 $1.50 $8.49 $8.49 $3.39 $8.69 $3.89 $8.75 $4.45 $8.75 $11.25 $2.15 $8.75 $8.49 $1.69 $8.49 $8.49 $1.25 $8.75 $11.75 $11.75 $8.99 $1.09 $8.75 $4.45 $8.75 $2.95 $8.75 $2.15 $3.99 $8.49 $8.99 $3.99 $8.49 $1.69 $1.09 $8.99 $1.09 $9.25 $8.75 $8.99 $2.39 $1.25 $1.25 $11.25 $11.25 $9.25 $9.25 $11.25 $1.50 $3.99 $8.49 $11.25 $9.25 $11.25 $17.50 $17.50 $8.75 $8.90 $8.90 $8.75 $8.75 $8.99 $2.39 $11.25 $9.25 $2.15 $11.25 $1.25 $11.75 $1.25 $11.25 $11.75 $1.25 $11.25 $11.25 $8.49 $10.98 $8.75 $1.25 $8.75 $8.49 $8.49 $1.50 $1.50 $8.75 $4.45 $11.25 $1.25 $11.75 $8.49 $2.39 $9.25 $4.45 $9.25 $8.75 $8.99 $1.69 $17.50 $17.50 $2.39 $8.99 $8.99 $11.25 $4.45 $8.75 $4.45 $9.25 $6.49 $10.98 $8.49 $8.49 $1.09 $1.69 $9.25 $4.45 $8.75 $1.25 $2.95 $3.99 $8.49 $11.75 $11.75 $2.15 $11.48 $8.75 $2.15 $1.25 $11.25 $2.15 $1.25 $8.75 $8.75 $6.49 $1.69 $8.99 $8.75 $11.75 $10.98 $1.09 $8.49 $3.39 $8.75 $2.15 $1.25 $11.48 $10.98 $10.98 $8.49 $2.95 $9.25 $9.25 $11.75 $4.45 $11.48 $11.25 $8.75 $4.45 $1.69 $8.99 $8.75 $4.45 $1.50 $11.75 $2.15 $8.99 $2.39 $8.75 $2.95 $1.25 $8.75 $2.15 $1.25 $2.18 $2.18 $2.18 $2.18 $11.48 $8.75 $2.95 $11.75 $11.75 $1.25 $10.58 $8.99 $2.39 $11.75 $4.45 $11.25 $11.25 $17.50 $17.50 $8.75 $8.75 $8.75 $22.50 $22.50 $9.25 $8.75 $4.45 $11.75 $1.25 $11.25 $11.25 $2.95 $8.99 $1.69 $11.25 $4.45 $8.75 $6.49 $8.75 $4.45 $9.25 $4.45 $11.75 $11.75 $4.45 $11.89 $11.75 $11.25 $2.95 $1.50 $4.45 $8.75 $8.99 $1.09 $8.99 $1.09 $3.99 $11.48 $8.49 $9.25 $4.45 $11.48 $9.25 $2.95 $9.25 $8.49 $8.99 $8.99 $8.49 $8.75 $2.95 $4.45 $11.89 $10.58 $8.19 $1.69 $8.75 $2.15 $1.25 $17.50 $17.50 $6.49 $9.25 $2.15 $8.75 $4.45 $8.75 $1.25 $11.48 $11.48 $8.99 $2.18 $2.18 $8.49 $8.99 $2.39 $2.39 $2.18 $2.18 $8.75 $4.45 $11.25 $9.25 $9.25 $11.25 $11.25 $4.45 $2.95 $11.75 $8.49 $8.49 $8.99 $1.69 $9.25 $11.25 $11.75 $9.25 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $10.98 $11.25 $4.45 $10.98 $8.49 $8.99 $3.39 $3.99 $8.99 $1.09 $1.09 $2.39 $17.50 $17.50 $4.45 $11.25 $11.25 $4.45 $9.25 $4.45 $8.75 $2.15 $1.25 $11.89 $2.95 $11.75 $1.25 $11.25 $4.45 $11.48 $11.48 $2.95 $9.25 $8.75 $9.25 $2.95 $11.25 $1.25 $11.75 $1.25 $8.99 $2.39 $1.25 $11.25 $1.25 $11.25 $8.49 $3.99 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $8.75 $11.80 $11.80 $11.80 $11.80 $8.90 $8.90 $5.90 $5.90 $6.49 $10.98 $17.98 $17.98 $2.39 $9.25 $8.75 $2.15 $8.75 $4.45 $8.49 $1.69 $8.19 $8.69 $10.98 $3.99 $11.48 $11.48 $4.45 $8.75 $6.49 $8.75 $8.75 $9.25 $1.25 $4.45 $8.49 $1.69 $9.25 $4.45 $8.99 $1.09 $11.25 $2.95 $11.08 $11.08 $3.89 $10.98 $11.25 $8.75 $11.25 $9.25 $4.30 $4.30 $8.75 $8.49 $3.99 $1.69 $8.99 $8.49 $1.69 $11.75 $11.25 $11.89 $9.25 $2.95 $9.25 $2.95 $8.75 $4.45 $4.45 $8.75 $10.98 $11.48 $8.49 $9.25 $4.45 $11.75 $11.89 $8.99 $8.49 $8.75 $9.25 $8.75 $8.75 $11.75 $11.75 $4.45 $11.25 $11.75 $2.50 $2.50 $8.99 $1.69 $11.75 $2.15 $1.25 $9.25 $8.75 $8.90 $8.90 $9.25 $2.95 $8.75 $11.25 $8.90 $8.90 $11.25 $11.75 $11.48 $1.69 $3.39 $9.25 $2.95 $8.99 $1.69 $8.49 $10.98 $11.25 $2.95 $8.99 $1.69 $8.75 $2.15 $1.25 $8.75 $2.95 $9.25 $2.50 $2.50 $11.25 $1.25 $11.75 $2.50 $2.50 $11.25 $1.50 $8.75 $1.25 $2.95 $11.48 $11.48 $8.75 $8.75 $2.15 $11.75 $1.25 $9.25 $9.25 $6.49 $11.75 $8.49 $8.49 $1.09 $10.98 $8.75 $1.25 $2.15 $11.25 $1.50 $11.25 $11.25 $8.49 $8.49 $8.75 $1.50 $1.25 $1.50 $8.75 $2.50 $2.50 $2.15 $7.40 $7.40 $4.00 $9.25 $9.39 $9.25 $9.25 $9.39 $11.25 $8.90 $8.90 $11.25 $6.00 $6.00 $6.00 $6.00 $11.25 $11.25 $11.25 $22.50 $22.50 $11.48 $1.09 $8.49 $8.49 $17.50 $17.50 $11.25 $1.50 $9.25 $8.75 $3.99 $8.49 $8.75 $8.75 $8.75 $8.75 $8.75 $11.75 $1.50 $11.25 $11.25 $2.95 $8.99 $10.98 $9.25 $8.75 $4.45 $8.49 $1.09 $2.39 $8.75 $8.75 $11.48 $8.99 $8.49 $8.49 $2.39 $10.98 $8.49 $3.99 $11.75 $4.45 $8.75 $2.15 $1.25 $10.98 $8.99 $11.25 $1.50 $8.75 $2.15 $1.25 $8.75 $9.25 $8.75 $11.25 $1.50 $8.75 $1.25 $4.45 $10.98 $8.75 $2.95 $1.25 $8.75 $2.95 $1.25 $8.49 $8.49 $2.39 $11.25 $1.25 $8.75 $8.75 $9.25 $8.75 $11.89 $1.25 $8.75 $2.15 $1.25 $8.99 $1.09 $8.75 $4.45 $26.25 $26.25 $26.25 $8.75 $4.45 $11.75 $2.95 $8.75 $8.75 $11.75 $8.75 $11.25 $11.25 $11.25 $4.45 $1.25 $8.49 $8.49 $8.49 $8.99 $8.99 $2.39 $2.39 $3.99 $8.75 $4.45 $2.15 $9.25 $1.25 $11.25 $11.75 $8.75 $4.45 $11.25 $2.15 $8.75 $4.45 $8.75 $8.75 $1.25 $11.25 $2.15 $8.75 $5.90 $5.90 $11.75 $1.25 $9.25 $3.75 $3.75 $3.75 $8.75 $1.25 $4.45 $11.75 $4.45 $8.75 $23.50 $23.50 $8.75 $2.95 $8.75 $8.75 $11.89 $4.45 $2.95 $1.25 $8.75 $4.45 $2.95 $1.25 $8.75 $2.15 $1.25 $11.75 $2.95 $8.99 $3.39 $9.25 $9.25 $17.50 $17.50 $2.95 $11.89 $1.50 $11.25 $2.95 $9.25 $11.25 $11.25 $2.95 $8.75 $9.25 $4.30 $4.30 $8.75 $8.75 $11.25 $8.75 $4.30 $4.30 $8.75 $1.25 $2.15 $8.49 $8.49 $3.39 $3.39 $10.98 $10.98 $2.39 $11.25 $11.75 $11.75 $1.25 $5.90 $5.90 $8.75 $11.25 $9.25 $4.45 $1.50 $3.39 $8.99 $2.39 $11.25 $2.15 $11.25 $11.75 $11.75 $4.45 $11.75 $4.45 $9.25 $8.75 $8.49 $8.99 $8.49 $8.99 $11.75 $8.75 $8.49 $3.99 $3.89 $11.08 $8.49 $8.99 $8.49 $8.49 $8.49 $11.25 $2.15 $17.50 $17.50 $8.75 $2.95 $8.49 $8.49 $10.98 $1.09 $11.25 $2.15 $2.95 $1.25 $8.75 $9.25 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $8.99 $3.99 $11.75 $2.15 $8.99 $3.39 $9.25 $8.75 $11.25 $11.25 $4.45 $8.75 $2.15 $1.25 $11.75 $4.45 $9.25 $2.95 $8.49 $8.49 $11.25 $8.75 $4.45 $11.25 $11.25 $11.25 $11.25 $4.45 $8.49 $1.69 $8.49 $3.39 $8.75 $11.25 $9.25 $8.75 $11.25 $11.25 $11.75 $11.25 $11.75 $11.25 $11.75 $21.96 $21.96 $10.98 $1.69 $11.48 $8.99 $8.49 $1.69 $9.25 $2.15 $1.50 $11.25 $1.50 $8.75 $8.75 $2.95 $8.49 $1.69 $8.75 $2.95 $1.25 $11.25 $2.15 $11.08 $8.49 $8.49 $8.49 $11.75 $1.25 $11.75 $8.75 $8.75 $8.75 $4.45 $11.25 $1.50 $23.50 $23.50 $11.75 $6.49 $8.75 $4.45 $6.49 $8.75 $2.50 $2.50 $2.15 $8.49 $2.39 $8.75 $11.75 $4.45 $8.99 $10.98 $9.25 $2.95 $9.25 $9.25 $11.75 $8.75 $8.75 $8.75 $10.98 $11.25 $9.25 $8.75 $8.75 $2.15 $11.25 $2.15 $4.45 $11.75 $8.49 $2.39 $9.25 $1.25 $1.25 $1.25 $1.25 $8.75 $2.15 $8.49 $1.69 $11.25 $1.50 $8.75 $8.75 $8.49 $3.99 $8.99 $1.09 $11.25 $1.25 $8.49 $2.39 $8.49 $8.75 $9.25 $11.25 $4.45 $11.25 $11.89 $8.99 $8.49 $8.75 $4.45 $8.75 $11.75 $11.75 $8.90 $8.90 $9.39 $2.95 $8.49 $3.99 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $8.75 $4.45 $8.49 $8.99 $8.49 $3.99 $8.75 $8.75 $2.95 $8.75 $17.50 $17.50 $9.25 $2.95 $8.75 $6.49 $4.30 $4.30 $8.75 $8.75 $2.15 $1.50 $8.49 $8.49 $2.39 $9.25 $4.45 $6.49 $11.75 $4.45 $10.98 $1.69 $9.39 $9.25 $9.25 $2.95 $8.75 $2.15 $1.25 $11.25 $9.25 $8.75 $11.25 $8.75 $11.25 $2.50 $2.50 $2.50 $2.50 $6.00 $6.00 $6.00 $6.00 $8.90 $8.90 $5.90 $5.90 $11.25 $11.25 $8.49 $10.98 $8.75 $2.15 $1.50 $9.25 $1.25 $1.50 $2.15 $1.25 $8.75 $2.95 $8.49 $3.99 $11.25 $4.30 $4.30 $11.75 $2.15 $18.50 $18.50 $8.49 $2.39 $8.75 $4.45 $11.75 $8.99 $3.99 $9.25 $9.25 $1.50 $8.75 $2.95 $6.49 $11.75 $8.49 $8.99 $8.75 $4.45 $6.49 $22.50 $22.50 $9.25 $2.95 $8.49 $1.69 $10.98 $8.75 $4.45 $11.25 $2.95 $8.99 $8.49 $2.39 $11.75 $6.49 $11.25 $11.75 $2.95 $8.99 $1.69 $8.99 $2.18 $2.18 $1.09 $8.99 $8.99 $1.09 $8.99 $8.99 $8.49 $10.98 $1.09 $11.75 $9.25 $11.25 $11.25 $2.15 $11.25 $8.75 $4.45 $2.95 $11.75 $1.50 $8.99 $10.98 $2.39 $8.75 $2.15 $9.25 $1.50 $8.75 $2.15 $3.99 $8.99 $6.49 $8.75 $8.90 $8.90 $8.99 $3.99 $17.50 $17.50 $11.25 $1.25 $10.98 $9.25 $4.45 $1.25 $3.00 $3.00 $11.25 $4.45 $4.45 $2.95 $9.25 $11.25 $2.15 $11.25 $11.25 $4.45 $2.95 $9.25 $11.25 $1.25 $8.75 $2.95 $1.25 $8.75 $4.45 $11.48 $11.48 $8.49 $2.39 $11.25 $11.75 $2.15 $1.50 $2.15 $8.75 $11.25 $8.90 $8.90 $11.25 $11.25 $1.25 $4.45 $9.25 $9.25 $8.75 $9.25 $8.75 $8.75 $9.25 $8.75 $11.75 $11.75 $8.75 $8.75 $8.90 $8.90 $2.95 $10.98 $8.49 $8.49 $10.98 $8.99 $8.99 $11.75 $17.50 $17.50 $11.75 $3.99 $8.49 $10.98 $1.69 $17.50 $17.50 $8.99 $2.39 $8.99 $2.39 $1.25 $8.75 $2.95 $11.75 $11.25 $17.50 $17.50 $8.49 $8.49 $2.39 $11.25 $1.50 $8.75 $3.00 $3.00 $1.25 $8.75 $4.45 $11.75 $11.75 $4.45 $21.96 $21.96 $8.75 $4.45 $8.75 $11.25 $9.25 $8.99 $2.39 $9.25 $8.75 $10.98 $8.49 $3.99 $3.39 $11.75 $1.50 $4.45 $9.25 $8.75 $1.25 $11.75 $8.75 $1.50 $8.75 $8.75 $2.15 $1.50 $8.75 $2.95 $8.75 $8.75 $17.50 $17.50 $8.75 $6.49 $4.45 $11.25 $11.25 $4.30 $4.30 $8.75 $11.25 $4.45 $8.99 $2.39 $9.25 $9.25 $9.25 $4.45 $11.75 $11.25 $2.95 $2.15 $11.25 $11.25 $8.75 $2.15 $1.50 $9.25 $4.45 $10.98 $8.99 $2.18 $2.18 $8.75 $4.45 $1.25 $8.99 $2.39 $4.45 $8.75 $10.98 $11.75 $1.50 $10.98 $8.99 $8.49 $3.99 $8.99 $8.49 $3.99 $8.49 $8.49 $8.99 $11.25 $11.25 $10.98 $10.98 $10.98 $2.39 $3.39 $8.75 $1.25 $2.95 $11.75 $1.50 $10.98 $1.69 $4.45 $8.75 $8.75 $8.75 $8.75 $4.45 $9.25 $8.75 $11.25 $8.75 $3.99 $8.99 $8.49 $11.25 $11.25 $8.75 $4.45 $8.75 $4.45 $1.25 $8.75 $8.75 $1.50 $2.15 $11.75 $11.75 $11.75 $11.75 $11.75 $1.50 $8.75 $9.25 $1.25 $8.75 $2.15 $8.99 $1.09 $4.45 $11.25 $11.75 $2.15 $8.75 $8.75 $1.25 $9.25 $2.15 $11.75 $11.25 $8.75 $11.25 $4.45 $8.49 $1.69 $8.75 $8.75 $8.99 $8.49 $9.25 $11.25 $2.95 $4.45 $11.75 $6.49 $11.48 $8.99 $4.36 $4.36 $4.36 $4.36 $11.48 $8.99 $8.49 $11.48 $8.75 $2.15 $1.50 $8.99 $1.69 $11.25 $1.25 $9.25 $9.25 $8.75 $9.25 $8.90 $8.90 $2.15 $9.25 $10.98 $8.49 $8.75 $9.25 $4.30 $4.30 $9.25 $8.75 $8.75 $2.15 $1.25 $8.75 $1.25 $8.75 $5.90 $5.90 $9.25 $8.75 $9.25 $4.45 $9.25 $11.75 $2.50 $2.50 $9.25 $2.15 $9.25 $1.50 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $10.98 $8.75 $8.75 $8.75 $2.15 $1.25 $10.98 $8.75 $2.15 $1.50 $8.75 $2.95 $1.25 $9.25 $9.25 $8.49 $2.39 $8.75 $4.45 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $9.25 $8.75 $8.99 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $11.25 $11.25 $8.75 $1.50 $2.15 $16.98 $16.98 $11.75 $1.50 $8.75 $4.30 $4.30 $1.50 $8.75 $2.95 $1.25 $1.25 $9.25 $4.45 $11.25 $8.75 $4.45 $8.75 $2.15 $1.25 $10.98 $1.69 $8.75 $1.25 $8.75 $1.25 $11.25 $8.75 $8.75 $8.49 $1.69 $9.25 $11.75 $8.49 $2.39 $9.25 $2.95 $6.49 $8.75 $8.75 $9.25 $8.75 $6.78 $6.78 $17.98 $17.98 $3.39 $11.75 $11.25 $8.75 $4.45 $11.75 $9.25 $8.75 $6.49 $8.99 $2.39 $8.75 $11.25 $11.75 $4.45 $8.75 $2.15 $9.25 $9.25 $9.25 $11.89 $11.75 $11.25 $9.25 $9.25 $8.75 $8.75 $8.49 $1.69 $1.09 $11.25 $1.50 $11.25 $11.25 $11.75 $1.50 $8.49 $8.99 $22.50 $22.50 $8.75 $4.30 $4.30 $8.75 $11.25 $2.15 $11.25 $2.95 $4.45 $11.25 $8.49 $3.39 $2.39 $11.75 $2.15 $11.75 $8.99 $2.39 $8.75 $11.75 $11.89 $1.25 $7.50 $7.50 $7.50 $7.50 $7.50 $11.89 $1.09 $8.49 $2.39 $8.75 $8.75 $8.75 $8.75 $9.25 $11.25 $8.75 $8.90 $8.90 $9.25 $8.75 $8.75 $11.75 $3.00 $3.00 $1.50 $11.25 $11.75 $8.99 $10.98 $4.45 $8.75 $2.15 $9.25 $11.25 $4.45 $1.69 $10.98 $9.25 $11.75 $9.25 $4.45 $10.98 $3.99 $8.49 $1.25 $9.25 $4.45 $10.98 $8.75 $8.75 $11.75 $11.25 $8.49 $11.48 $4.45 $1.25 $11.25 $8.99 $1.09 $2.39 $11.25 $2.15 $8.75 $4.45 $8.49 $1.69 $10.98 $1.69 $9.25 $4.45 $11.25 $8.75 $11.25 $11.75 $11.25 $22.50 $22.50 $8.49 $2.39 $2.50 $2.50 $8.75 $8.75 $9.25 $9.25 $11.25 $8.99 $1.09 $8.99 $1.69 $11.75 $1.25 $21.96 $21.96 $8.75 $2.15 $1.25 $8.75 $11.25 $9.25 $11.25 $8.75 $8.75 $11.25 $2.15 $8.99 $1.09 $1.69 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $11.48 $8.49 $8.49 $4.78 $4.78 $9.25 $1.25 $1.25 $1.25 $11.25 $11.25 $11.75 $4.45 $11.25 $4.45 $8.99 $1.09 $11.25 $2.15 $11.25 $9.25 $11.75 $11.25 $11.25 $9.25 $2.95 $11.25 $4.45 $8.75 $2.95 $2.95 $11.25 $1.50 $10.98 $16.98 $16.98 $18.50 $18.50 $10.98 $3.99 $1.09 $9.25 $9.25 $8.75 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $1.25 $4.45 $9.25 $8.75 $8.75 $17.50 $17.50 $4.45 $9.39 $1.25 $2.95 $11.25 $8.75 $8.75 $11.25 $2.15 $8.90 $8.90 $11.25 $11.89 $10.98 $11.25 $4.45 $11.25 $11.25 $8.49 $10.98 $8.49 $3.39 $9.25 $8.75 $2.95 $3.00 $3.00 $9.39 $11.75 $2.95 $1.50 $11.25 $11.75 $8.75 $2.15 $1.50 $8.49 $3.39 $11.75 $1.25 $17.50 $17.50 $11.25 $1.25 $8.75 $2.95 $1.25 $11.25 $11.75 $13.35 $13.35 $13.35 $11.25 $11.75 $11.25 $11.25 $4.45 $11.25 $8.49 $3.39 $9.25 $2.95 $4.78 $4.78 $2.39 $3.99 $8.99 $8.99 $11.25 $11.25 $8.75 $11.25 $2.95 $4.45 $9.25 $8.75 $4.45 $8.49 $8.49 $10.98 $10.98 $3.99 $11.75 $8.75 $11.75 $4.45 $1.50 $1.25 $8.49 $8.49 $8.75 $8.75 $8.75 $9.25 $8.75 $2.95 $1.25 $11.25 $1.50 $11.25 $4.45 $9.25 $8.75 $8.75 $9.25 $8.75 $4.45 $1.50 $8.75 $8.75 $8.49 $1.69 $8.75 $2.15 $9.25 $2.15 $1.50 $11.25 $11.75 $2.15 $6.49 $9.25 $9.25 $11.25 $11.25 $11.75 $11.75 $11.75 $11.25 $8.75 $2.15 $1.25 $11.75 $9.25 $11.25 $8.75 $5.90 $5.90 $8.75 $4.45 $9.25 $9.25 $4.45 $11.25 $4.45 $11.25 $8.75 $2.15 $11.89 $11.25 $8.75 $2.95 $1.50 $8.75 $4.30 $4.30 $8.75 $11.25 $11.75 $11.75 $2.15 $11.25 $8.99 $1.09 $8.49 $8.49 $8.49 $3.39 $8.99 $10.98 $3.99 $11.75 $2.15 $8.75 $4.45 $2.50 $2.50 $11.48 $1.09 $8.49 $8.49 $16.98 $16.98 $3.99 $10.98 $1.09 $8.75 $2.95 $8.75 $8.75 $2.95 $9.25 $11.25 $2.15 $9.25 $4.45 $4.45 $9.25 $11.75 $11.75 $2.15 $9.25 $8.75 $11.25 $6.49 $8.75 $11.25 $2.95 $10.98 $3.99 $1.50 $9.25 $2.15 $8.75 $11.25 $11.89 $4.45 $1.50 $1.25 $8.75 $8.75 $4.45 $11.25 $11.75 $8.49 $1.09 $1.09 $1.69 $8.99 $3.39 $8.99 $1.69 $8.49 $8.99 $3.27 $3.27 $3.27 $8.99 $8.99 $1.09 $10.98 $1.69 $3.99 $8.49 $1.09 $8.75 $8.75 $11.75 $8.75 $9.25 $8.75 $3.39 $8.99 $8.99 $2.39 $9.25 $8.75 $9.25 $9.25 $8.99 $3.99 $2.39 $8.49 $1.09 $8.49 $8.99 $3.39 $11.25 $1.25 $8.99 $3.99 $8.75 $8.90 $8.90 $6.49 $8.75 $9.25 $11.25 $11.25 $11.25 $1.25 $8.75 $9.25 $4.45 $1.25 $8.75 $1.25 $2.15 $17.98 $17.98 $8.99 $8.75 $2.95 $1.25 $11.75 $1.50 $1.50 $8.75 $8.75 $11.08 $8.99 $1.69 $8.99 $1.69 $10.98 $3.99 $3.39 $11.75 $2.15 $11.75 $2.95 $8.75 $8.75 $11.75 $11.25 $11.75 $11.25 $4.45 $11.25 $1.25 $2.18 $2.18 $2.18 $2.18 $2.39 $8.49 $8.99 $2.39 $11.25 $8.75 $11.75 $11.75 $11.25 $4.45 $2.15 $8.19 $10.58 $4.45 $9.25 $1.09 $8.99 $11.25 $1.50 $8.99 $3.99 $4.45 $11.75 $2.15 $11.25 $8.75 $4.45 $8.75 $9.25 $6.45 $6.45 $6.45 $8.75 $11.25 $11.25 $8.75 $11.75 $21.96 $21.96 $8.99 $5.07 $5.07 $5.07 $8.49 $9.25 $11.25 $4.45 $3.39 $8.49 $8.99 $8.49 $17.50 $17.50 $22.96 $22.96 $8.75 $11.25 $11.89 $11.25 $8.49 $1.69 $1.09 $8.99 $8.99 $9.25 $8.75 $9.25 $2.95 $8.49 $3.99 $8.99 $8.49 $7.17 $7.17 $7.17 $8.49 $8.99 $17.50 $17.50 $9.25 $9.25 $11.25 $1.25 $8.99 $1.09 $8.75 $4.45 $11.25 $2.15 $11.75 $11.25 $11.25 $8.75 $8.75 $4.45 $1.25 $11.75 $11.75 $2.50 $2.50 $8.49 $8.99 $2.18 $2.18 $11.25 $4.45 $11.25 $11.75 $8.49 $8.99 $1.69 $1.09 $8.99 $8.99 $11.25 $6.49 $11.25 $8.75 $4.45 $8.99 $1.69 $11.48 $11.75 $2.50 $2.50 $8.49 $1.09 $1.09 $1.69 $8.49 $2.39 $11.75 $1.25 $8.49 $1.69 $8.49 $1.69 $11.75 $4.45 $8.75 $8.75 $4.45 $8.75 $11.25 $11.25 $8.75 $7.98 $7.98 $8.49 $1.09 $8.49 $3.99 $8.49 $3.99 $8.99 $3.99 $11.25 $4.45 $8.49 $2.39 $8.49 $2.39 $3.99 $8.49 $1.25 $11.25 $4.45 $9.25 $4.45 $1.09 $8.99 $3.99 $11.25 $8.90 $8.90 $9.25 $11.25 $8.75 $11.25 $11.25 $11.25 $11.25 $11.25 $8.99 $8.49 $8.75 $8.75 $4.45 $16.98 $16.98 $11.75 $11.25 $9.25 $4.45 $9.25 $2.95 $8.49 $1.69 $3.75 $3.75 $3.75 $4.45 $9.25 $1.50 $11.25 $11.48 $11.25 $2.15 $8.75 $9.39 $8.49 $3.99 $8.19 $2.29 $11.48 $1.69 $11.48 $3.99 $8.49 $1.69 $9.25 $2.95 $8.49 $1.69 $11.25 $4.45 $9.39 $9.25 $8.75 $8.75 $4.45 $11.89 $4.45 $4.45 $8.75 $8.75 $8.75 $2.15 $8.75 $3.75 $3.75 $3.75 $9.25 $11.25 $4.45 $6.49 $16.98 $16.98 $18.50 $18.50 $2.50 $2.50 $2.95 $3.99 $8.49 $8.19 $11.08 $6.49 $11.75 $2.39 $8.99 $1.09 $11.25 $4.45 $11.25 $8.99 $1.69 $21.96 $21.96 $2.18 $2.18 $8.99 $8.99 $2.39 $8.69 $1.69 $8.90 $8.90 $2.50 $2.50 $8.75 $8.99 $1.09 $8.49 $8.49 $8.75 $4.45 $17.50 $17.50 $8.75 $9.25 $8.49 $2.39 $8.75 $4.45 $11.25 $11.25 $11.75 $8.75 $8.49 $8.49 $8.49 $8.99 $8.75 $4.45 $11.48 $8.75 $1.25 $2.15 $9.25 $4.45 $11.75 $2.15 $11.25 $8.99 $2.39 $8.69 $8.69 $11.75 $2.95 $11.75 $1.50 $9.25 $4.45 $1.50 $11.48 $8.99 $2.39 $11.25 $11.89 $2.15 $1.25 $11.75 $4.45 $8.75 $8.75 $11.25 $4.45 $11.25 $2.15 $4.45 $8.49 $1.09 $3.99 $11.25 $11.25 $8.49 $2.39 $8.99 $2.39 $11.25 $2.15 $8.75 $2.95 $1.25 $8.75 $11.25 $17.50 $17.50 $11.75 $11.75 $11.25 $11.25 $4.45 $2.50 $2.50 $8.75 $8.99 $8.99 $1.69 $8.99 $1.69 $11.25 $1.25 $11.08 $8.69 $8.99 $1.09 $11.25 $11.25 $2.95 $1.25 $8.75 $1.25 $8.75 $8.75 $2.15 $1.25 $8.49 $3.99 $8.49 $2.39 $8.49 $7.17 $7.17 $7.17 $8.75 $4.45 $11.48 $8.75 $8.75 $11.48 $8.75 $9.25 $8.49 $3.99 $1.50 $11.25 $11.25 $8.75 $8.75 $4.45 $9.25 $4.45 $8.75 $8.75 $4.30 $4.30 $2.95 $8.75 $4.50 $4.50 $4.50 $9.25 $11.25 $4.45 $11.25 $11.25 $8.75 $9.25 $8.75 $2.15 $1.25 $8.75 $2.15 $1.25 $8.99 $8.49 $8.75 $8.75 $1.25 $11.75 $4.50 $4.50 $4.50 $8.75 $8.75 $3.99 $3.39 $8.49 $2.39 $8.99 $1.50 $11.25 $11.25 $8.75 $8.75 $2.15 $8.75 $2.15 $1.25 $21.96 $21.96 $8.49 $1.69 $26.07 $26.07 $26.07 $11.75 $1.50 $8.99 $8.99 $11.48 $9.25 $9.25 $8.75 $8.75 $8.75 $9.25 $8.75 $8.75 $2.15 $1.25 $11.89 $8.75 $11.75 $4.45 $18.50 $18.50 $9.25 $9.39 $8.49 $2.39 $8.49 $1.69 $1.09 $8.99 $8.49 $2.18 $2.18 $11.25 $1.25 $8.49 $3.39 $8.49 $3.99 $11.25 $8.75 $8.49 $1.69 $16.98 $16.98 $9.25 $9.25 $11.75 $1.25 $11.25 $8.75 $8.49 $8.49 $8.75 $1.25 $1.25 $1.25 $11.25 $12.98 $12.98 $11.75 $11.75 $4.45 $11.25 $11.75 $10.98 $8.49 $8.49 $2.39 $9.25 $11.25 $8.75 $2.95 $1.50 $11.25 $2.95 $9.25 $2.95 $9.25 $8.75 $11.25 $8.75 $8.75 $4.45 $11.25 $1.25 $8.75 $2.95 $2.50 $2.50 $9.25 $9.25 $9.25 $6.49 $17.50 $17.50 $8.49 $1.69 $8.49 $3.99 $8.75 $2.95 $2.95 $8.49 $8.99 $8.99 $1.09 $8.75 $4.45 $1.25 $11.25 $11.75 $11.25 $9.25 $11.25 $1.50 $11.25 $2.15 $10.98 $8.75 $11.75 $2.95 $11.25 $11.25 $9.25 $8.75 $9.25 $8.75 $8.75 $8.75 $1.25 $11.25 $1.50 $2.15 $8.75 $8.75 $11.25 $8.75 $8.75 $11.25 $4.45 $8.49 $8.49 $8.49 $8.49 $8.99 $1.69 $2.39 $1.09 $1.09 $11.25 $2.95 $35.25 $35.25 $35.25 $8.75 $2.95 $1.25 $11.25 $2.15 $9.25 $4.45 $8.75 $8.75 $8.75 $4.45 $1.25 $11.89 $8.75 $2.15 $1.25 $8.49 $1.09 $1.09 $1.69 $8.69 $8.69 $9.25 $2.95 $10.98 $2.39 $22.50 $22.50 $21.96 $21.96 $10.98 $8.49 $1.69 $11.75 $2.95 $8.75 $11.25 $8.75 $9.25 $8.75 $8.19 $10.58 $8.75 $2.95 $1.50 $2.50 $2.50 $9.25 $4.45 $9.25 $11.25 $8.69 $8.69 $3.89 $8.69 $1.69 $4.45 $9.25 $11.25 $4.45 $2.15 $8.19 $8.69 $4.45 $11.25 $1.25 $11.25 $8.75 $11.89 $11.75 $11.75 $9.25 $8.75 $2.15 $1.50 $11.75 $4.45 $3.99 $8.99 $10.98 $2.39 $1.25 $2.95 $8.75 $2.95 $9.25 $9.25 $9.25 $8.75 $2.15 $9.25 $9.25 $3.39 $8.49 $8.49 $2.39 $10.98 $1.09 $1.09 $8.49 $11.25 $1.25 $8.99 $1.09 $8.75 $2.15 $1.50 $3.99 $8.49 $8.75 $2.15 $1.50 $8.49 $10.98 $2.18 $2.18 $8.75 $2.15 $1.50 $4.45 $9.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $44.25 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $10.50 $6.49 $33.75 $33.75 $33.75 $35.00 $35.00 $35.00 $35.00 $27.75 $27.75 $27.75 $3.00 $3.00 $11.25 $11.75 $10.98 $2.39 $2.50 $2.50 $8.75 $4.45 $16.98 $16.98 $8.75 $6.49 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $16.98 $16.98 $8.99 $8.99 $8.49 $9.25 $8.75 $2.95 $11.48 $2.39 $8.99 $2.39 $11.25 $4.45 $8.75 $9.25 $6.49 $26.25 $26.25 $26.25 $8.75 $26.25 $26.25 $26.25 $8.75 $8.75 $11.25 $11.25 $2.15 $1.25 $11.75 $8.75 $2.15 $1.50 $11.25 $2.15 $8.99 $2.39 $11.25 $11.25 $2.15 $11.25 $11.25 $8.75 $4.78 $4.78 $21.96 $21.96 $8.49 $2.39 $9.25 $2.95 $16.98 $16.98 $8.19 $3.89 $8.99 $1.09 $8.99 $3.39 $9.25 $4.45 $10.98 $10.98 $17.50 $17.50 $11.25 $1.25 $8.75 $11.75 $4.45 $11.25 $11.25 $8.99 $1.09 $10.98 $8.49 $1.69 $11.25 $9.25 $16.98 $16.98 $8.75 $4.45 $11.25 $6.49 $11.75 $9.25 $9.25 $8.75 $4.45 $2.50 $2.50 $8.75 $11.75 $8.75 $9.25 $11.75 $9.25 $8.75 $9.25 $8.75 $11.25 $11.75 $9.25 $8.75 $11.75 $8.49 $1.09 $1.09 $8.49 $1.09 $1.69 $11.25 $1.25 $8.75 $2.15 $1.50 $8.49 $1.69 $1.25 $8.75 $2.95 $8.49 $3.99 $8.49 $8.49 $8.75 $11.25 $2.15 $1.50 $11.75 $8.99 $1.09 $10.98 $10.98 $11.25 $1.25 $8.75 $4.45 $4.45 $1.25 $11.89 $8.99 $8.99 $11.25 $4.45 $23.50 $23.50 $8.49 $3.99 $9.25 $4.45 $4.45 $9.25 $8.75 $4.45 $8.75 $8.75 $11.75 $6.49 $17.50 $17.50 $4.45 $8.75 $2.95 $1.50 $8.75 $8.75 $8.75 $4.45 $11.25 $11.25 $11.75 $11.25 $2.95 $11.25 $4.45 $3.00 $3.00 $1.25 $2.95 $9.25 $8.99 $2.39 $6.49 $8.75 $8.90 $8.90 $11.48 $1.09 $10.98 $9.25 $9.25 $11.25 $8.75 $11.75 $11.25 $11.25 $1.25 $9.25 $4.45 $9.25 $6.49 $11.75 $11.75 $8.99 $2.39 $8.49 $8.49 $9.25 $9.25 $1.25 $8.75 $2.95 $11.75 $2.15 $8.49 $8.49 $8.69 $16.38 $16.38 $8.19 $3.89 $2.29 $11.75 $8.75 $8.75 $8.75 $4.45 $8.49 $8.49 $9.25 $8.75 $6.49 $2.95 $11.25 $11.25 $2.15 $9.25 $11.75 $21.96 $21.96 $8.49 $3.39 $1.69 $8.49 $8.75 $4.45 $8.49 $3.99 $11.25 $8.75 $11.25 $2.15 $11.75 $4.45 $11.25 $9.25 $8.75 $18.50 $18.50 $1.50 $8.75 $2.15 $11.48 $2.18 $2.18 $3.99 $11.25 $1.50 $8.99 $2.39 $11.75 $1.50 $11.25 $6.49 $4.45 $11.25 $8.49 $3.99 $2.50 $2.50 $8.75 $9.25 $3.99 $8.99 $8.75 $6.49 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $13.52 $16.98 $16.98 $16.98 $16.98 $17.98 $17.98 $16.98 $16.98 $8.75 $8.75 $11.25 $11.25 $8.49 $1.09 $1.69 $1.25 $9.25 $2.95 $8.69 $8.19 $8.49 $2.39 $8.49 $2.39 $10.98 $8.99 $8.99 $1.69 $8.49 $8.75 $8.75 $11.25 $4.45 $4.45 $17.50 $17.50 $8.75 $4.45 $8.75 $2.15 $1.50 $1.50 $8.99 $1.09 $8.75 $4.45 $8.75 $8.75 $11.25 $4.30 $4.30 $8.49 $1.69 $1.09 $1.09 $8.75 $2.15 $1.50 $8.99 $8.49 $3.99 $8.75 $2.15 $1.25 $9.25 $2.95 $11.25 $4.45 $9.25 $2.95 $3.99 $8.99 $8.49 $8.75 $8.75 $11.25 $9.25 $8.75 $4.45 $11.25 $1.25 $9.25 $11.25 $4.45 $2.95 $10.98 $8.75 $8.75 $18.50 $18.50 $9.25 $9.25 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $16.98 $16.98 $11.25 $2.95 $8.75 $2.15 $1.50 $8.49 $2.39 $9.25 $2.15 $1.25 $8.19 $8.69 $8.19 $8.19 $8.75 $2.95 $1.25 $9.25 $2.95 $11.25 $8.75 $11.25 $11.25 $8.99 $1.09 $9.25 $9.25 $4.45 $8.49 $3.99 $2.39 $1.09 $8.99 $8.49 $8.75 $8.75 $11.25 $11.75 $4.45 $2.50 $2.50 $8.75 $8.49 $3.39 $8.75 $9.25 $4.45 $1.25 $11.25 $2.15 $4.45 $2.50 $2.50 $8.99 $3.99 $8.75 $2.15 $11.75 $11.75 $1.25 $8.75 $9.39 $11.25 $9.25 $9.25 $2.95 $9.25 $4.45 $1.25 $9.25 $8.75 $11.75 $1.50 $8.75 $4.45 $8.99 $1.09 $9.25 $2.95 $8.99 $1.69 $8.69 $1.69 $11.25 $4.45 $8.75 $8.75 $4.45 $11.25 $8.75 $2.95 $1.50 $8.19 $8.69 $1.09 $1.69 $8.49 $8.75 $2.95 $1.25 $8.49 $3.99 $10.98 $3.39 $11.25 $11.25 $2.15 $18.50 $18.50 $8.49 $8.49 $11.25 $1.50 $8.49 $2.39 $8.99 $2.39 $11.75 $4.45 $17.50 $17.50 $9.25 $9.25 $8.75 $4.45 $3.75 $3.75 $3.75 $8.75 $4.45 $11.75 $2.95 $1.25 $4.45 $8.75 $1.25 $1.50 $9.25 $11.25 $11.25 $11.25 $11.25 $9.25 $11.25 $4.45 $8.75 $9.25 $8.75 $8.75 $4.45 $8.75 $1.25 $2.15 $8.75 $8.75 $4.30 $4.30 $8.75 $1.25 $2.95 $9.25 $2.95 $4.45 $11.25 $11.25 $9.25 $9.25 $4.50 $4.50 $4.50 $11.75 $1.25 $11.75 $11.75 $1.25 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $2.15 $1.25 $11.25 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $15.00 $11.25 $4.45 $4.45 $2.95 $11.25 $2.15 $1.25 $1.50 $8.75 $11.25 $2.95 $11.25 $1.25 $2.15 $11.25 $9.25 $6.49 $1.25 $8.75 $2.15 $8.75 $6.49 $11.25 $1.50 $8.75 $4.45 $8.75 $4.45 $9.25 $9.25 $1.25 $1.25 $8.75 $4.50 $4.50 $4.50 $11.25 $1.25 $1.50 $9.25 $2.15 $11.25 $4.45 $11.25 $4.45 $8.75 $4.45 $9.25 $4.45 $1.25 $11.25 $4.45 $8.75 $4.45 $8.75 $2.15 $8.75 $4.45 $8.75 $11.75 $1.50 $11.25 $4.45 $8.75 $2.15 $1.50 $8.75 $4.45 $8.75 $11.75 $8.75 $8.75 $11.25 $11.25 $1.50 $8.75 $2.15 $11.75 $2.15 $9.25 $2.95 $18.50 $18.50 $1.25 $4.45 $8.50 $8.50 $8.50 $8.50 $11.25 $11.89 $1.25 $9.39 $4.45 $8.75 $2.15 $1.50 $11.75 $8.75 $9.25 $9.25 $4.45 $1.25 $11.25 $9.25 $2.95 $8.75 $8.75 $2.15 $8.75 $11.25 $11.25 $11.25 $11.75 $11.25 $2.15 $11.25 $2.15 $8.99 $8.99 $8.75 $9.25 $9.25 $11.25 $8.75 $4.45 $8.75 $1.25 $4.45 $11.25 $1.25 $8.75 $8.75 $11.25 $8.75 $1.25 $1.25 $1.25 $9.25 $11.75 $2.15 $8.75 $4.45 $8.75 $4.45 $11.25 $9.25 $8.75 $9.25 $4.45 $11.75 $4.45 $1.25 $4.45 $11.75 $9.25 $11.25 $2.15 $23.50 $23.50 $9.25 $2.15 $18.50 $18.50 $8.75 $5.90 $5.90 $11.89 $4.45 $8.75 $4.45 $9.25 $2.95 $11.25 $2.95 $11.25 $11.25 $11.25 $2.95 $11.75 $9.25 $9.25 $2.95 $11.25 $2.15 $9.25 $8.90 $8.90 $8.75 $11.25 $11.25 $11.25 $8.75 $2.15 $1.25 $11.25 $1.25 $8.75 $4.45 $1.25 $11.25 $11.25 $11.75 $1.25 $11.25 $11.25 $11.25 $8.75 $8.75 $18.50 $18.50 $11.75 $1.25 $4.45 $9.25 $6.49 $4.45 $8.75 $11.25 $6.49 $11.75 $8.75 $9.25 $11.25 $2.15 $8.75 $4.45 $11.25 $4.45 $8.75 $9.25 $4.45 $1.25 $1.25 $8.75 $11.25 $11.75 $2.15 $11.75 $4.45 $11.75 $1.25 $11.75 $11.75 $11.25 $4.30 $4.30 $9.39 $9.39 $8.75 $1.25 $9.25 $4.45 $11.25 $1.25 $8.75 $1.25 $2.95 $11.25 $4.45 $8.75 $1.50 $4.45 $4.45 $9.25 $8.75 $2.95 $1.25 $11.25 $2.95 $8.75 $8.75 $8.75 $8.75 $4.30 $4.30 $9.25 $9.39 $4.45 $9.25 $1.25 $22.50 $22.50 $4.45 $2.95 $2.15 $23.50 $23.50 $11.75 $2.15 $1.25 $9.25 $4.45 $11.25 $11.75 $17.50 $17.50 $8.75 $11.75 $11.25 $8.75 $4.45 $11.75 $1.50 $8.75 $8.75 $11.75 $9.25 $11.25 $4.45 $11.75 $9.25 $4.45 $11.25 $8.75 $8.75 $2.15 $1.50 $8.75 $4.45 $9.25 $8.75 $1.50 $1.25 $1.25 $1.25 $8.75 $2.95 $11.25 $11.25 $1.50 $11.75 $11.25 $2.15 $9.25 $8.75 $11.75 $2.95 $1.50 $8.75 $1.50 $1.25 $8.75 $11.75 $11.25 $11.25 $11.75 $11.25 $11.75 $8.75 $17.80 $17.80 $17.80 $17.80 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $5.00 $8.75 $2.95 $1.25 $11.25 $1.25 $2.15 $11.25 $2.50 $2.50 $9.25 $1.25 $11.25 $2.95 $11.75 $2.15 $11.25 $1.50 $8.99 $1.99 $11.49 $8.75 $4.45 $1.25 $8.75 $4.45 $1.25 $1.50 $11.75 $8.75 $8.75 $11.25 $6.49 $11.75 $8.75 $2.15 $1.25 $6.49 $8.75 $4.45 $8.75 $4.45 $8.75 $11.25 $4.45 $6.49 $9.25 $8.75 $1.25 $4.45 $11.25 $8.75 $1.50 $8.75 $1.50 $1.25 $9.25 $9.39 $4.45 $9.25 $8.75 $4.45 $1.25 $11.25 $11.75 $8.75 $11.25 $9.25 $8.75 $11.25 $2.50 $2.50 $17.50 $17.50 $9.25 $4.45 $11.25 $1.25 $8.75 $4.45 $1.50 $8.75 $1.50 $1.25 $9.39 $8.75 $8.75 $4.45 $11.25 $1.25 $9.25 $4.45 $11.25 $8.75 $3.00 $3.00 $8.75 $2.15 $1.25 $11.25 $11.25 $4.45 $11.25 $11.25 $8.75 $11.75 $11.75 $11.75 $8.75 $4.45 $1.25 $1.50 $8.75 $4.45 $1.25 $9.25 $9.25 $8.75 $4.45 $1.25 $11.75 $11.25 $1.25 $11.75 $11.25 $9.25 $2.15 $1.50 $8.75 $4.45 $11.75 $11.75 $11.25 $8.75 $8.75 ' to numeric\n\n\n\n\n\nStep 17. How many different items are sold?\n\nchipo.item_name.value_counts().count()\n\n50"
  },
  {
    "objectID": "lab/01World-Food-Facts-Exercises-with-solutions.html",
    "href": "lab/01World-Food-Facts-Exercises-with-solutions.html",
    "title": "Exercise 1",
    "section": "",
    "text": "Step 1. Go to https://www.kaggle.com/openfoodfacts/world-food-facts/data\n\n\nStep 2. Download the dataset to your computer and unzip it.\n\nimport pandas as pd\nimport numpy as np\n\n\n\nStep 3. Use the tsv file and assign it to a dataframe called food\n\nfood = pd.read_csv('E:\\Yang Fan\\Lab 1\\en.openfoodfacts.org.products.tsv', sep='\\t')\n\n\n\nStep 4. See the first 5 entries\n\nfood.head()\n\n\n\n\n\n\n\n\ncode\nurl\ncreator\ncreated_t\ncreated_datetime\nlast_modified_t\nlast_modified_datetime\nproduct_name\ngeneric_name\nquantity\n...\nfruits-vegetables-nuts_100g\nfruits-vegetables-nuts-estimate_100g\ncollagen-meat-protein-ratio_100g\ncocoa_100g\nchlorophyl_100g\ncarbon-footprint_100g\nnutrition-score-fr_100g\nnutrition-score-uk_100g\nglycemic-index_100g\nwater-hardness_100g\n\n\n\n\n0\n3087\nhttp://world-en.openfoodfacts.org/product/0000...\nopenfoodfacts-contributors\n1474103866\n2016-09-17T09:17:46Z\n1474103893\n2016-09-17T09:18:13Z\nFarine de blé noir\nNaN\n1kg\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n4530\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nBanana Chips Sweetened (Whole)\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n14.0\n14.0\nNaN\nNaN\n\n\n2\n4559\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489069957\n2017-03-09T14:32:37Z\n1489069957\n2017-03-09T14:32:37Z\nPeanuts\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\n\n\n3\n16087\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055731\n2017-03-09T10:35:31Z\n1489055731\n2017-03-09T10:35:31Z\nOrganic Salted Nut Mix\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n12.0\n12.0\nNaN\nNaN\n\n\n4\n16094\nhttp://world-en.openfoodfacts.org/product/0000...\nusda-ndb-import\n1489055653\n2017-03-09T10:34:13Z\n1489055653\n2017-03-09T10:34:13Z\nOrganic Polenta\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 163 columns\n\n\n\n\n\nStep 5. What is the number of observations in the dataset?\n\nfood.shape\n\n(356027, 163)\n\n\n\nfood.shape[0]\n\n356027\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\nprint(food.shape) \nprint(food.shape[1]) \n\n#OR\n\nfood.info() \n\n(356027, 163)\n163\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 356027 entries, 0 to 356026\nColumns: 163 entries, code to water-hardness_100g\ndtypes: float64(107), object(56)\nmemory usage: 442.8+ MB\n\n\n\n\nStep 7. Print the name of all the columns.\n\nfood.columns\n\nIndex(['code', 'url', 'creator', 'created_t', 'created_datetime',\n       'last_modified_t', 'last_modified_datetime', 'product_name',\n       'generic_name', 'quantity',\n       ...\n       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g',\n       'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g',\n       'carbon-footprint_100g', 'nutrition-score-fr_100g',\n       'nutrition-score-uk_100g', 'glycemic-index_100g',\n       'water-hardness_100g'],\n      dtype='object', length=163)\n\n\n\n\nStep 8. What is the name of 105th column?\n\nfood.columns[104]\n\n'-glucose_100g'\n\n\n\n\nStep 9. What is the type of the observations of the 105th column?\n\nfood.dtypes['-glucose_100g']\n\ndtype('float64')\n\n\n\n\nStep 10. How is the dataset indexed?\n\nfood.index\n\nRangeIndex(start=0, stop=356027, step=1)\n\n\n\n\nStep 11. What is the product name of the 19th observation?\n\nfood.values[18][7]\n\n'Lotus Organic Brown Jasmine Rice'"
  },
  {
    "objectID": "lab/02Euro12-Exercises-with-solutions.html",
    "href": "lab/02Euro12-Exercises-with-solutions.html",
    "title": "Ex2 - Filtering and Sorting Data",
    "section": "",
    "text": "This time we are going to pull data directly from the internet.\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\n\n\n\nStep 2. Import the dataset from this address.\n\n\nStep 3. Assign it to a variable called euro12.\n\neuro12 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv', sep=',')\neuro12\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n13\n81.3%\n41\n62\n2\n9\n0\n9\n9\n16\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n9\n60.1%\n53\n73\n8\n7\n0\n11\n11\n19\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n10\n66.7%\n25\n38\n8\n4\n0\n7\n7\n15\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n22\n88.1%\n43\n45\n6\n5\n0\n11\n11\n16\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n6\n54.6%\n36\n51\n5\n6\n0\n11\n11\n19\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n20\n74.1%\n101\n89\n16\n16\n0\n18\n18\n19\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n12\n70.6%\n35\n30\n3\n5\n0\n7\n7\n15\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n6\n66.7%\n48\n56\n3\n7\n1\n7\n7\n17\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n10\n71.5%\n73\n90\n10\n12\n0\n14\n14\n16\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n17\n65.4%\n43\n51\n11\n6\n1\n10\n10\n17\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n10\n77.0%\n34\n43\n4\n6\n0\n7\n7\n16\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n8\n61.6%\n35\n51\n7\n7\n0\n9\n9\n18\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n13\n76.5%\n48\n31\n4\n5\n0\n9\n9\n18\n\n\n\n\n16 rows × 35 columns\n\n\n\n\n\nStep 4. Select only the Goal column.\n\neuro12.Goals\n\n0      4\n1      4\n2      4\n3      5\n4      3\n5     10\n6      5\n7      6\n8      2\n9      2\n10     6\n11     1\n12     5\n13    12\n14     5\n15     2\nName: Goals, dtype: int64\n\n\n\n\nStep 5. How many team participated in the Euro2012?\n\neuro12.shape[0]\n\n16\n\n\n\n\nStep 6. What is the number of columns in the dataset?\n\neuro12.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 16 entries, 0 to 15\nData columns (total 35 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Team                        16 non-null     object \n 1   Goals                       16 non-null     int64  \n 2   Shots on target             16 non-null     int64  \n 3   Shots off target            16 non-null     int64  \n 4   Shooting Accuracy           16 non-null     object \n 5   % Goals-to-shots            16 non-null     object \n 6   Total shots (inc. Blocked)  16 non-null     int64  \n 7   Hit Woodwork                16 non-null     int64  \n 8   Penalty goals               16 non-null     int64  \n 9   Penalties not scored        16 non-null     int64  \n 10  Headed goals                16 non-null     int64  \n 11  Passes                      16 non-null     int64  \n 12  Passes completed            16 non-null     int64  \n 13  Passing Accuracy            16 non-null     object \n 14  Touches                     16 non-null     int64  \n 15  Crosses                     16 non-null     int64  \n 16  Dribbles                    16 non-null     int64  \n 17  Corners Taken               16 non-null     int64  \n 18  Tackles                     16 non-null     int64  \n 19  Clearances                  16 non-null     int64  \n 20  Interceptions               16 non-null     int64  \n 21  Clearances off line         15 non-null     float64\n 22  Clean Sheets                16 non-null     int64  \n 23  Blocks                      16 non-null     int64  \n 24  Goals conceded              16 non-null     int64  \n 25  Saves made                  16 non-null     int64  \n 26  Saves-to-shots ratio        16 non-null     object \n 27  Fouls Won                   16 non-null     int64  \n 28  Fouls Conceded              16 non-null     int64  \n 29  Offsides                    16 non-null     int64  \n 30  Yellow Cards                16 non-null     int64  \n 31  Red Cards                   16 non-null     int64  \n 32  Subs on                     16 non-null     int64  \n 33  Subs off                    16 non-null     int64  \n 34  Players Used                16 non-null     int64  \ndtypes: float64(1), int64(29), object(5)\nmemory usage: 4.5+ KB\n\n\n\n\nStep 7. View only the columns Team, Yellow Cards and Red Cards and assign them to a dataframe called discipline\n\ndiscipline = euro12[['Team', 'Yellow Cards', 'Red Cards']]\ndiscipline\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n2\nDenmark\n4\n0\n\n\n3\nEngland\n5\n0\n\n\n4\nFrance\n6\n0\n\n\n5\nGermany\n4\n0\n\n\n6\nGreece\n9\n1\n\n\n7\nItaly\n16\n0\n\n\n8\nNetherlands\n5\n0\n\n\n9\nPoland\n7\n1\n\n\n10\nPortugal\n12\n0\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n12\nRussia\n6\n0\n\n\n13\nSpain\n11\n0\n\n\n14\nSweden\n7\n0\n\n\n15\nUkraine\n5\n0\n\n\n\n\n\n\n\n\n\nStep 8. Sort the teams by Red Cards, then to Yellow Cards\n\ndiscipline.sort_values(['Red Cards', 'Yellow Cards'], ascending = False)\n\n\n\n\n\n\n\n\nTeam\nYellow Cards\nRed Cards\n\n\n\n\n6\nGreece\n9\n1\n\n\n9\nPoland\n7\n1\n\n\n11\nRepublic of Ireland\n6\n1\n\n\n7\nItaly\n16\n0\n\n\n10\nPortugal\n12\n0\n\n\n13\nSpain\n11\n0\n\n\n0\nCroatia\n9\n0\n\n\n1\nCzech Republic\n7\n0\n\n\n14\nSweden\n7\n0\n\n\n4\nFrance\n6\n0\n\n\n12\nRussia\n6\n0\n\n\n3\nEngland\n5\n0\n\n\n8\nNetherlands\n5\n0\n\n\n15\nUkraine\n5\n0\n\n\n2\nDenmark\n4\n0\n\n\n5\nGermany\n4\n0\n\n\n\n\n\n\n\n\n\nStep 9. Calculate the mean Yellow Cards given per Team\n\n\nround(discipline['Yellow Cards'].mean())\n\n7\n\n\n\n\nStep 10. Filter teams that scored more than 6 goals\n\neuro12[euro12.Goals &gt; 6]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n15\n93.8%\n102\n83\n19\n11\n0\n17\n17\n18\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 11. Select the teams that start with G\n\neuro12[euro12.Team.str.startswith('G')]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\nSubs on\nSubs off\nPlayers Used\n\n\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n10\n62.6%\n63\n49\n12\n4\n0\n15\n15\n17\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n13\n65.1%\n67\n48\n12\n9\n1\n12\n12\n20\n\n\n\n\n2 rows × 35 columns\n\n\n\n\n\nStep 12. Select the first 7 columns\n\neuro12.iloc[: , 0:7]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n\n\n\n\n\n\n\n\n\nStep 13. Select all columns except the last 3.\n\neuro12.iloc[: , :-3]\n\n\n\n\n\n\n\n\nTeam\nGoals\nShots on target\nShots off target\nShooting Accuracy\n% Goals-to-shots\nTotal shots (inc. Blocked)\nHit Woodwork\nPenalty goals\nPenalties not scored\n...\nClean Sheets\nBlocks\nGoals conceded\nSaves made\nSaves-to-shots ratio\nFouls Won\nFouls Conceded\nOffsides\nYellow Cards\nRed Cards\n\n\n\n\n0\nCroatia\n4\n13\n12\n51.9%\n16.0%\n32\n0\n0\n0\n...\n0\n10\n3\n13\n81.3%\n41\n62\n2\n9\n0\n\n\n1\nCzech Republic\n4\n13\n18\n41.9%\n12.9%\n39\n0\n0\n0\n...\n1\n10\n6\n9\n60.1%\n53\n73\n8\n7\n0\n\n\n2\nDenmark\n4\n10\n10\n50.0%\n20.0%\n27\n1\n0\n0\n...\n1\n10\n5\n10\n66.7%\n25\n38\n8\n4\n0\n\n\n3\nEngland\n5\n11\n18\n50.0%\n17.2%\n40\n0\n0\n0\n...\n2\n29\n3\n22\n88.1%\n43\n45\n6\n5\n0\n\n\n4\nFrance\n3\n22\n24\n37.9%\n6.5%\n65\n1\n0\n0\n...\n1\n7\n5\n6\n54.6%\n36\n51\n5\n6\n0\n\n\n5\nGermany\n10\n32\n32\n47.8%\n15.6%\n80\n2\n1\n0\n...\n1\n11\n6\n10\n62.6%\n63\n49\n12\n4\n0\n\n\n6\nGreece\n5\n8\n18\n30.7%\n19.2%\n32\n1\n1\n1\n...\n1\n23\n7\n13\n65.1%\n67\n48\n12\n9\n1\n\n\n7\nItaly\n6\n34\n45\n43.0%\n7.5%\n110\n2\n0\n0\n...\n2\n18\n7\n20\n74.1%\n101\n89\n16\n16\n0\n\n\n8\nNetherlands\n2\n12\n36\n25.0%\n4.1%\n60\n2\n0\n0\n...\n0\n9\n5\n12\n70.6%\n35\n30\n3\n5\n0\n\n\n9\nPoland\n2\n15\n23\n39.4%\n5.2%\n48\n0\n0\n0\n...\n0\n8\n3\n6\n66.7%\n48\n56\n3\n7\n1\n\n\n10\nPortugal\n6\n22\n42\n34.3%\n9.3%\n82\n6\n0\n0\n...\n2\n11\n4\n10\n71.5%\n73\n90\n10\n12\n0\n\n\n11\nRepublic of Ireland\n1\n7\n12\n36.8%\n5.2%\n28\n0\n0\n0\n...\n0\n23\n9\n17\n65.4%\n43\n51\n11\n6\n1\n\n\n12\nRussia\n5\n9\n31\n22.5%\n12.5%\n59\n2\n0\n0\n...\n0\n8\n3\n10\n77.0%\n34\n43\n4\n6\n0\n\n\n13\nSpain\n12\n42\n33\n55.9%\n16.0%\n100\n0\n1\n0\n...\n5\n8\n1\n15\n93.8%\n102\n83\n19\n11\n0\n\n\n14\nSweden\n5\n17\n19\n47.2%\n13.8%\n39\n3\n0\n0\n...\n1\n12\n5\n8\n61.6%\n35\n51\n7\n7\n0\n\n\n15\nUkraine\n2\n7\n26\n21.2%\n6.0%\n38\n0\n0\n0\n...\n0\n4\n4\n13\n76.5%\n48\n31\n4\n5\n0\n\n\n\n\n16 rows × 32 columns\n\n\n\n\n\nStep 14. Present only the Shooting Accuracy from England, Italy and Russia\n\neuro12.loc[euro12.Team.isin(['England', 'Italy', 'Russia']), ['Team','Shooting Accuracy']]\n\n\n\n\n\n\n\n\nTeam\nShooting Accuracy\n\n\n\n\n3\nEngland\n50.0%\n\n\n7\nItaly\n43.0%\n\n\n12\nRussia\n22.5%"
  },
  {
    "objectID": "lab/03Scores-Exercises-with-solutions.html",
    "href": "lab/03Scores-Exercises-with-solutions.html",
    "title": "Scores",
    "section": "",
    "text": "Introduction:\nThis time you will create the data.\nExercise based on Chris Albon work, the credits belong to him.\n\n\nStep 1. Import the necessary libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib inline\n\n\n\nStep 2. Create the DataFrame that should look like the one below.\n\nraw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n            'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'], \n            'female': [0, 1, 1, 0, 1],\n            'age': [42, 52, 36, 24, 73], \n            'preTestScore': [4, 24, 31, 2, 3],\n            'postTestScore': [25, 94, 57, 62, 70]}\n\ndf = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'female', 'preTestScore', 'postTestScore'])\n\ndf\n\n\n\n\n\n\n\n\nfirst_name\nlast_name\nage\nfemale\npreTestScore\npostTestScore\n\n\n\n\n0\nJason\nMiller\n42\n0\n4\n25\n\n\n1\nMolly\nJacobson\n52\n1\n24\n94\n\n\n2\nTina\nAli\n36\n1\n31\n57\n\n\n3\nJake\nMilner\n24\n0\n2\n62\n\n\n4\nAmy\nCooze\n73\n1\n3\n70\n\n\n\n\n\n\n\n\n\nStep 3. Create a Scatterplot of preTestScore and postTestScore, with the size of each point determined by age\n\nHint: Don’t forget to place the labels\n\nplt.scatter(df.preTestScore, df.postTestScore, s=df.age)\n\n#set labels and titles\nplt.title(\"preTestScore x postTestScore\")\nplt.xlabel('preTestScore')\nplt.ylabel('preTestScore')\n\nText(0, 0.5, 'preTestScore')\n\n\n\n\n\nStep 4. Create a Scatterplot of preTestScore and postTestScore.\n\n\nThis time the size should be 4.5 times the postTestScore and the color determined by sex\n\nplt.scatter(df.preTestScore, df.postTestScore, s= df.postTestScore * 4.5, c = df.female)\n\n#set labels and titles\nplt.title(\"preTestScore x postTestScore\")\nplt.xlabel('preTestScore')\nplt.ylabel('preTestScore')\n\nText(46.972222222222214, 0.5, 'preTestScore')\n\n\n\n\nBONUS: Create your own question and answer it."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "practice/practice 1.html",
    "href": "practice/practice 1.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport pingouin as pg\nfrom lets_plot import *\nLetsPlot.setup_html(no_js=True)\nplt.style.use(\n    \"https://raw.githubusercontent.com/aeturrell/core_python/main/plot_style.txt\"\n)\n\n\ndf = pd.read_csv(\n    \"https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv\",\n    skiprows=1,\n    na_values=\"***\",\n)\ndf\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n140\n2020\n1.58\n1.69\n1.66\n1.39\n1.26\n1.14\n1.10\n1.12\n1.19\n1.20\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.32\n\n\n141\n2021\n1.25\n0.95\n1.20\n1.12\n1.04\n1.20\n1.07\n1.02\n1.04\n1.29\n1.29\n1.16\n1.14\n1.14\n1.13\n1.12\n1.10\n1.21\n\n\n142\n2022\n1.24\n1.16\n1.41\n1.08\n1.02\n1.12\n1.06\n1.16\n1.14\n1.31\n1.09\n1.06\n1.15\n1.16\n1.19\n1.17\n1.11\n1.18\n\n\n143\n2023\n1.29\n1.29\n1.63\n1.01\n1.12\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.49\n1.43\n1.21\n1.26\n1.40\n1.84\n\n\n144\n2024\n1.66\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.57\nNaN\nNaN\nNaN\nNaN\nNaN\n1.81\n1.67\n1.46\nNaN\n\n\n\n\n145 rows × 19 columns\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\n\n\n0\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n2\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n3\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n4\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n\n\n\n\n\n\ndf.info()\nna_values=\"***\"\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 145 entries, 0 to 144\nData columns (total 19 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Year    145 non-null    int64  \n 1   Jan     145 non-null    float64\n 2   Feb     145 non-null    float64\n 3   Mar     145 non-null    float64\n 4   Apr     145 non-null    float64\n 5   May     145 non-null    float64\n 6   Jun     145 non-null    float64\n 7   Jul     145 non-null    float64\n 8   Aug     145 non-null    float64\n 9   Sep     145 non-null    float64\n 10  Oct     144 non-null    float64\n 11  Nov     144 non-null    float64\n 12  Dec     144 non-null    float64\n 13  J-D     144 non-null    float64\n 14  D-N     143 non-null    float64\n 15  DJF     144 non-null    float64\n 16  MAM     145 non-null    float64\n 17  JJA     145 non-null    float64\n 18  SON     144 non-null    float64\ndtypes: float64(18), int64(1)\nmemory usage: 21.7 KB\n\n\n\ndf = df.set_index(\"Year\")\ndf.head()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1880\n-0.39\n-0.54\n-0.24\n-0.31\n-0.06\n-0.18\n-0.22\n-0.26\n-0.25\n-0.31\n-0.44\n-0.43\n-0.30\nNaN\nNaN\n-0.20\n-0.22\n-0.33\n\n\n1881\n-0.31\n-0.25\n-0.06\n-0.02\n0.05\n-0.34\n0.09\n-0.06\n-0.29\n-0.45\n-0.37\n-0.23\n-0.19\n-0.20\n-0.33\n-0.01\n-0.10\n-0.37\n\n\n1882\n0.26\n0.21\n0.02\n-0.30\n-0.23\n-0.29\n-0.28\n-0.15\n-0.25\n-0.52\n-0.34\n-0.69\n-0.21\n-0.18\n0.08\n-0.17\n-0.24\n-0.37\n\n\n1883\n-0.58\n-0.66\n-0.15\n-0.30\n-0.26\n-0.12\n-0.06\n-0.23\n-0.34\n-0.17\n-0.45\n-0.15\n-0.29\n-0.33\n-0.64\n-0.24\n-0.14\n-0.32\n\n\n1884\n-0.16\n-0.11\n-0.64\n-0.59\n-0.36\n-0.42\n-0.41\n-0.52\n-0.45\n-0.45\n-0.58\n-0.47\n-0.43\n-0.40\n-0.14\n-0.53\n-0.45\n-0.50\n\n\n\n\n\n\n\n\ndf.tail()\n\n\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nJ-D\nD-N\nDJF\nMAM\nJJA\nSON\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020\n1.58\n1.69\n1.66\n1.39\n1.26\n1.14\n1.10\n1.12\n1.19\n1.20\n1.58\n1.18\n1.34\n1.36\n1.56\n1.44\n1.12\n1.32\n\n\n2021\n1.25\n0.95\n1.20\n1.12\n1.04\n1.20\n1.07\n1.02\n1.04\n1.29\n1.29\n1.16\n1.14\n1.14\n1.13\n1.12\n1.10\n1.21\n\n\n2022\n1.24\n1.16\n1.41\n1.08\n1.02\n1.12\n1.06\n1.16\n1.14\n1.31\n1.09\n1.06\n1.15\n1.16\n1.19\n1.17\n1.11\n1.18\n\n\n2023\n1.29\n1.29\n1.63\n1.01\n1.12\n1.19\n1.44\n1.57\n1.67\n1.88\n1.97\n1.85\n1.49\n1.43\n1.21\n1.26\n1.40\n1.84\n\n\n2024\n1.66\n1.93\n1.77\n1.79\n1.44\n1.54\n1.42\n1.42\n1.57\nNaN\nNaN\nNaN\nNaN\nNaN\n1.81\n1.67\n1.46\nNaN\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\ndf[\"Jan\"].plot(ax=ax)\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.plot(df.index, df[\"Jan\"])\nax.set_ylabel(\"y label\")\nax.set_xlabel(\"x label\")\nax.set_title(\"title\")\nplt.show()\n\n\n\n\n\n\n\n\n\nmonth = \"Jan\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.66, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average temperature anomaly in {month} \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\nmonth = \"J-D\"\nfig, ax = plt.subplots()\nax.axhline(0, color=\"orange\")\nax.annotate(\"1951—1980 average\", xy=(0.68, -0.2), xycoords=(\"figure fraction\", \"data\"))\ndf[month].plot(ax=ax)\nax.set_title(\n    f\"Average annual temperature anomaly in \\n in the northern hemisphere (1880—{df.index.max()})\"\n)\nax.set_ylabel(\"Annual temperature anomalies\");\n\n\n\n\n\n\n\n\n\ndf[\"Period\"] = pd.cut(\n    df.index,\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\ndf[\"Period\"].tail(20)\n\nYear\n2005    1981—2010\n2006    1981—2010\n2007    1981—2010\n2008    1981—2010\n2009    1981—2010\n2010    1981—2010\n2011          NaN\n2012          NaN\n2013          NaN\n2014          NaN\n2015          NaN\n2016          NaN\n2017          NaN\n2018          NaN\n2019          NaN\n2020          NaN\n2021          NaN\n2022          NaN\n2023          NaN\n2024          NaN\nName: Period, dtype: category\nCategories (3, object): ['1921—1950' &lt; '1951—1980' &lt; '1981—2010']\n\n\n\nlist_of_months = [\"Jun\", \"Jul\", \"Aug\"]\ndf[list_of_months].stack().head()\n\nYear     \n1880  Jun   -0.18\n      Jul   -0.22\n      Aug   -0.26\n1881  Jun   -0.34\n      Jul    0.09\ndtype: float64\n\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(9, 4), sharex=True, sharey=True)\nfor ax, period in zip(axes, df[\"Period\"].dropna().unique()):\n    df.loc[df[\"Period\"] == period, list_of_months].stack().hist(ax=ax)\n    ax.set_title(period)\nplt.suptitle(\"Histogram of temperature anomalies\")\naxes[1].set_xlabel(\"Summer temperature distribution\")\nplt.tight_layout();\n\n\n\n\n\n\n\n\n\n# Create a variable that has years 1951 to 1980, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1951) & (df.index &lt;= 1980), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at this data:\ntemp_all_months\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1951\nJan\n-0.36\n\n\n1\n1951\nFeb\n-0.51\n\n\n2\n1951\nMar\n-0.19\n\n\n3\n1951\nApr\n0.07\n\n\n4\n1951\nMay\n0.17\n\n\n...\n...\n...\n...\n\n\n355\n1980\nAug\n0.09\n\n\n356\n1980\nSep\n0.10\n\n\n357\n1980\nOct\n0.12\n\n\n358\n1980\nNov\n0.20\n\n\n359\n1980\nDec\n0.09\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nquantiles = [0.3, 0.7]\nlist_of_percentiles = np.quantile(temp_all_months[\"values\"], q=quantiles)\n\nprint(f\"The cold threshold of {quantiles[0]*100}% is {list_of_percentiles[0]}\")\nprint(f\"The hot threshold of {quantiles[1]*100}% is {list_of_percentiles[1]}\")\n\nThe cold threshold of 30.0% is -0.1\nThe hot threshold of 70.0% is 0.1\n\n\n\n# Create a variable that has years 1981 to 2010, and months Jan to Dec (inclusive)\ntemp_all_months = df.loc[(df.index &gt;= 1981) & (df.index &lt;= 2010), \"Jan\":\"Dec\"]\n# Put all the data in stacked format and give the new columns sensible names\ntemp_all_months = (\n    temp_all_months.stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"month\", 0: \"values\"})\n)\n# Take a look at the start of this data data:\ntemp_all_months.head()\n\n\n\n\n\n\n\n\nYear\nmonth\nvalues\n\n\n\n\n0\n1981\nJan\n0.79\n\n\n1\n1981\nFeb\n0.62\n\n\n2\n1981\nMar\n0.68\n\n\n3\n1981\nApr\n0.39\n\n\n4\n1981\nMay\n0.18\n\n\n\n\n\n\n\n\nentries_less_than_q30 = temp_all_months[\"values\"] &lt; list_of_percentiles[0]\nproportion_under_q30 = entries_less_than_q30.mean()\nprint(\n    f\"The proportion under {list_of_percentiles[0]} is {proportion_under_q30*100:.2f}%\"\n)\n\nThe proportion under -0.1 is 1.94%\n\n\n\nproportion_over_q70 = (temp_all_months[\"values\"] &gt; list_of_percentiles[1]).mean()\nprint(f\"The proportion over {list_of_percentiles[1]} is {proportion_over_q70*100:.2f}%\")\n\nThe proportion over 0.1 is 84.72%\n\n\n\ntemp_all_months = (\n    df.loc[:, \"DJF\":\"SON\"]\n    .stack()\n    .reset_index()\n    .rename(columns={\"level_1\": \"Season\", 0: \"Values\"})\n)\ntemp_all_months[\"Period\"] = pd.cut(\n    temp_all_months[\"Year\"],\n    bins=[1921, 1950, 1980, 2010],\n    labels=[\"1921—1950\", \"1951—1980\", \"1981—2010\"],\n    ordered=True,\n)\n# Take a look at a cut of the data using `.iloc`, which provides position\ntemp_all_months.iloc[-135:-125]\n\n\n\n\n\n\n\n\nYear\nSeason\nValues\nPeriod\n\n\n\n\n443\n1991\nDJF\n0.51\n1981—2010\n\n\n444\n1991\nMAM\n0.45\n1981—2010\n\n\n445\n1991\nJJA\n0.42\n1981—2010\n\n\n446\n1991\nSON\n0.32\n1981—2010\n\n\n447\n1992\nDJF\n0.43\n1981—2010\n\n\n448\n1992\nMAM\n0.30\n1981—2010\n\n\n449\n1992\nJJA\n-0.04\n1981—2010\n\n\n450\n1992\nSON\n-0.15\n1981—2010\n\n\n451\n1993\nDJF\n0.37\n1981—2010\n\n\n452\n1993\nMAM\n0.31\n1981—2010\n\n\n\n\n\n\n\n\ngrp_mean_var = temp_all_months.groupby([\"Season\", \"Period\"])[\"Values\"].agg(\n    [np.mean, np.var]\n)\ngrp_mean_var\n\n\n\n\n\n\n\n\n\nmean\nvar\n\n\nSeason\nPeriod\n\n\n\n\n\n\nDJF\n1921—1950\n-0.027931\n0.057703\n\n\n1951—1980\n-0.003333\n0.050375\n\n\n1981—2010\n0.522000\n0.078644\n\n\nJJA\n1921—1950\n-0.054483\n0.021611\n\n\n1951—1980\n0.001333\n0.014640\n\n\n1981—2010\n0.399000\n0.067775\n\n\nMAM\n1921—1950\n-0.041724\n0.031136\n\n\n1951—1980\n0.000333\n0.025272\n\n\n1981—2010\n0.507667\n0.075812\n\n\nSON\n1921—1950\n0.081379\n0.027798\n\n\n1951—1980\n-0.001333\n0.026384\n\n\n1981—2010\n0.427000\n0.110739\n\n\n\n\n\n\n\n\nmin_year = 1880\n(\n    ggplot(temp_all_months, aes(x=\"Year\", y=\"Values\", color=\"Season\"))\n    + geom_abline(slope=0, color=\"black\", size=1)\n    + geom_line(size=1)\n    + labs(\n        title=f\"Average annual temperature anomaly in \\n in the northern hemisphere ({min_year}—{temp_all_months['Year'].max()})\",\n        y=\"Annual temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n    + geom_text(\n        x=min_year, y=0.1, label=\"1951—1980 average\", hjust=\"left\", color=\"black\"\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n              \n            \n            \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n              \n                \n                \n              \n            \n            \n              \n                \n                  \n                    1951—1980 average\n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1880\n              \n            \n          \n          \n            \n            \n            \n              \n                1900\n              \n            \n          \n          \n            \n            \n            \n              \n                1920\n              \n            \n          \n          \n            \n            \n            \n              \n                1940\n              \n            \n          \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2020\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -1.0\n              \n            \n          \n          \n            \n              \n                -0.5\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.5\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n          \n            \n              \n                1.5\n              \n            \n          \n        \n      \n    \n    \n      \n        Average annual temperature anomaly in \n      \n      \n         in the northern hemisphere (1880—2024)\n      \n    \n    \n      \n        Annual temperature anomalies\n      \n    \n    \n      \n        Year\n      \n    \n    \n      \n      \n      \n        \n          \n            Season\n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                MAM\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                JJA\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                SON\n              \n            \n          \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n            \n              \n                DJF\n              \n            \n          \n        \n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_co2 = pd.read_csv(\"data2.csv\")\ndf_co2.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n0\n1958\n3\n315.71\n315.71\n314.62\n\n\n1\n1958\n4\n317.45\n317.45\n315.29\n\n\n2\n1958\n5\n317.50\n317.50\n314.71\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n4\n1958\n7\n315.86\n315.86\n314.98\n\n\n\n\n\n\n\n\ndf_co2_june = df_co2.loc[df_co2[\"Month\"] == 6]\ndf_co2_june.head()\n\n\n\n\n\n\n\n\nYear\nMonth\nMonthly average\nInterpolated\nTrend\n\n\n\n\n3\n1958\n6\n-99.99\n317.10\n314.85\n\n\n15\n1959\n6\n318.15\n318.15\n315.92\n\n\n27\n1960\n6\n319.59\n319.59\n317.36\n\n\n39\n1961\n6\n319.77\n319.77\n317.48\n\n\n51\n1962\n6\n320.55\n320.55\n318.27\n\n\n\n\n\n\n\n\ndf_temp_co2 = pd.merge(df_co2_june, df, on=\"Year\")\ndf_temp_co2[[\"Year\", \"Jun\", \"Trend\"]].head()\n\n\n\n\n\n\n\n\nYear\nJun\nTrend\n\n\n\n\n0\n1958\n0.05\n314.85\n\n\n1\n1959\n0.14\n315.92\n\n\n2\n1960\n0.18\n317.36\n\n\n3\n1961\n0.18\n317.48\n\n\n4\n1962\n-0.13\n318.27\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Jun\", y=\"Trend\"))\n    + geom_point(color=\"black\", size=3)\n    + labs(\n        title=\"Scatterplot of temperature anomalies vs carbon dioxide emissions\",\n        y=\"Carbon dioxide levels (trend, mole fraction)\",\n        x=\"Temperature anomaly (degrees Celsius)\",\n    )\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.0\n              \n            \n          \n          \n            \n            \n            \n              \n                0.2\n              \n            \n          \n          \n            \n            \n            \n              \n                0.4\n              \n            \n          \n          \n            \n            \n            \n              \n                0.6\n              \n            \n          \n          \n            \n            \n            \n              \n                0.8\n              \n            \n          \n          \n            \n            \n            \n              \n                1.0\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                320\n              \n            \n          \n          \n            \n              \n                340\n              \n            \n          \n          \n            \n              \n                360\n              \n            \n          \n          \n            \n              \n                380\n              \n            \n          \n          \n            \n              \n                400\n              \n            \n          \n        \n      \n    \n    \n      \n        Scatterplot of temperature anomalies vs carbon dioxide emissions\n      \n    \n    \n      \n        Carbon dioxide levels (trend, mole fraction)\n      \n    \n    \n      \n        Temperature anomaly (degrees Celsius)\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\ndf_temp_co2[[\"Jun\", \"Trend\"]].corr(method=\"pearson\")\n\n\n\n\n\n\n\n\nJun\nTrend\n\n\n\n\nJun\n1.000000\n0.914371\n\n\nTrend\n0.914371\n1.000000\n\n\n\n\n\n\n\n\n(\n    ggplot(df_temp_co2, aes(x=\"Year\", y=\"Jun\"))\n    + geom_line(size=1)\n    + labs(\n        title=\"June temperature anomalies\",\n    )\n    + scale_x_continuous(format=\"d\")\n)\n\n\n  \n  \n    \n    \n    \n      \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n        \n          \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n            \n          \n        \n      \n      \n        \n          \n            \n              \n                \n                \n              \n            \n          \n        \n        \n          \n            \n            \n          \n        \n      \n      \n        \n          \n            \n            \n            \n              \n                1960\n              \n            \n          \n          \n            \n            \n            \n              \n                1970\n              \n            \n          \n          \n            \n            \n            \n              \n                1980\n              \n            \n          \n          \n            \n            \n            \n              \n                1990\n              \n            \n          \n          \n            \n            \n            \n              \n                2000\n              \n            \n          \n          \n            \n            \n            \n              \n                2010\n              \n            \n          \n          \n          \n        \n        \n          \n            \n              \n                -0.2\n              \n            \n          \n          \n            \n              \n                0.0\n              \n            \n          \n          \n            \n              \n                0.2\n              \n            \n          \n          \n            \n              \n                0.4\n              \n            \n          \n          \n            \n              \n                0.6\n              \n            \n          \n          \n            \n              \n                0.8\n              \n            \n          \n          \n            \n              \n                1.0\n              \n            \n          \n        \n      \n    \n    \n      \n        June temperature anomalies\n      \n    \n    \n      \n        Jun\n      \n    \n    \n      \n        Year\n      \n    \n    \n    \n  \n  \n  \n\n\n\n\nbase_plot = ggplot(df_temp_co2) + scale_x_continuous(format=\"d\")\nplot_p = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Jun\"), size=1)\n    + labs(title=\"June temperature anomalies\")\n)\nplot_q = (\n    base_plot\n    + geom_line(aes(x=\"Year\", y=\"Trend\"), size=1)\n    + labs(title=\"Carbon dioxide emissions\")\n)\ngggrid([plot_p, plot_q], ncol=2)\n\n\n  \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  -0.2\n                \n              \n            \n            \n              \n                \n                  0.0\n                \n              \n            \n            \n              \n                \n                  0.2\n                \n              \n            \n            \n              \n                \n                  0.4\n                \n              \n            \n            \n              \n                \n                  0.6\n                \n              \n            \n            \n              \n                \n                  0.8\n                \n              \n            \n            \n              \n                \n                  1.0\n                \n              \n            \n          \n        \n      \n      \n        \n          June temperature anomalies\n        \n      \n      \n        \n          Jun\n        \n      \n      \n        \n          Year\n        \n      \n      \n      \n    \n    \n    \n  \n  \n    \n    \n      \n      \n      \n        \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n          \n            \n              \n              \n              \n              \n              \n              \n              \n              \n              \n              \n            \n          \n        \n        \n          \n            \n              \n                \n                  \n                  \n                \n              \n            \n          \n          \n            \n              \n              \n            \n          \n        \n        \n          \n            \n              \n              \n              \n                \n                  1960\n                \n              \n            \n            \n              \n              \n              \n                \n                  1970\n                \n              \n            \n            \n              \n              \n              \n                \n                  1980\n                \n              \n            \n            \n              \n              \n              \n                \n                  1990\n                \n              \n            \n            \n              \n              \n              \n                \n                  2000\n                \n              \n            \n            \n              \n              \n              \n                \n                  2010\n                \n              \n            \n            \n            \n          \n          \n            \n              \n                \n                  320\n                \n              \n            \n            \n              \n                \n                  340\n                \n              \n            \n            \n              \n                \n                  360\n                \n              \n            \n            \n              \n                \n                  380\n                \n              \n            \n            \n              \n                \n                  400\n                \n              \n            \n          \n        \n      \n      \n        \n          Carbon dioxide emissions\n        \n      \n      \n        \n          Trend\n        \n      \n      \n        \n          Year"
  },
  {
    "objectID": "practice/practice 2_2.html",
    "href": "practice/practice 2_2.html",
    "title": "",
    "section": "",
    "text": "%pip install lets-plot\n\nRequirement already satisfied: lets-plot in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.5.1)\nRequirement already satisfied: pypng in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lets-plot) (0.20220715.0)\nRequirement already satisfied: palettable in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lets-plot) (3.3.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom lets_plot import *\n\n\nLetsPlot.setup_html()\n\n\n            \n            \n            \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom lets_plot import *\n\ndata = {\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 4, 1, 3, 5]\n}\ndf = pd.DataFrame(data)\n\nggplot(df, aes(x='x', y='y')) + geom_line()\n\n   \n   \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.scatter(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nfrom lets_plot import *\n\ndata = {\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 4, 1, 3, 5]\n}\ndf = pd.DataFrame(data)\n\nggplot(df, aes(x='x', y='y')) + geom_bar(stat='identity')\n\n   \n   \n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 1, 3, 5]\nplt.bar(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\nplt.hist(x, bins=5)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nx = [1, 2, 3, 4, 5]\nplt.boxplot(x)\nplt.show()"
  },
  {
    "objectID": "practice/practice 3_2.html",
    "href": "practice/practice 3_2.html",
    "title": "",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport textwrap\n\n\npd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/dplyr/storms.csv\", nrows=10\n)\n\n\n\n\n\n\n\n\nrownames\nname\nyear\nmonth\nday\nhour\nlat\nlong\nstatus\ncategory\nwind\npressure\ntropicalstorm_force_diameter\nhurricane_force_diameter\n\n\n\n\n0\n1\nAmy\n1975\n6\n27\n0\n27.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n1\n2\nAmy\n1975\n6\n27\n6\n28.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n2\n3\nAmy\n1975\n6\n27\n12\n29.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n3\n4\nAmy\n1975\n6\n27\n18\n30.5\n-79.0\ntropical depression\nNaN\n25\n1013\nNaN\nNaN\n\n\n4\n5\nAmy\n1975\n6\n28\n0\n31.5\n-78.8\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n5\n6\nAmy\n1975\n6\n28\n6\n32.4\n-78.7\ntropical depression\nNaN\n25\n1012\nNaN\nNaN\n\n\n6\n7\nAmy\n1975\n6\n28\n12\n33.3\n-78.0\ntropical depression\nNaN\n25\n1011\nNaN\nNaN\n\n\n7\n8\nAmy\n1975\n6\n28\n18\n34.0\n-77.0\ntropical depression\nNaN\n30\n1006\nNaN\nNaN\n\n\n8\n9\nAmy\n1975\n6\n29\n0\n34.4\n-75.8\ntropical storm\nNaN\n35\n1004\nNaN\nNaN\n\n\n9\n10\nAmy\n1975\n6\n29\n6\n34.0\n-74.8\ntropical storm\nNaN\n40\n1002\nNaN\nNaN\n\n\n\n\n\n\n\n\nimport requests\n\nurl = \"https://api.ons.gov.uk/timeseries/JP9Z/dataset/UNEM/data\"\n\n# Get the data from the ONS API:\njson_data = requests.get(url).json()\n\n# Prep the data for a quick plot\ntitle = json_data[\"description\"][\"title\"]\ndf = (\n    pd.DataFrame(pd.json_normalize(json_data[\"months\"]))\n    .assign(\n        date=lambda x: pd.to_datetime(x[\"date\"]),\n        value=lambda x: pd.to_numeric(x[\"value\"]),\n    )\n    .set_index(\"date\")\n)\n\ndf[\"value\"].plot(title=title, ylim=(0, df[\"value\"].max() * 1.2), lw=3.0);\n\n\n---------------------------------------------------------------------------\nJSONDecodeError                           Traceback (most recent call last)\nFile d:\\anaconda3\\Lib\\site-packages\\requests\\models.py:974, in Response.json(self, **kwargs)\n    973 try:\n--&gt; 974     return complexjson.loads(self.text, **kwargs)\n    975 except JSONDecodeError as e:\n    976     # Catch JSON-related errors and raise as requests.JSONDecodeError\n    977     # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n\nFile d:\\anaconda3\\Lib\\json\\__init__.py:346, in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n    343 if (cls is None and object_hook is None and\n    344         parse_int is None and parse_float is None and\n    345         parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 346     return _default_decoder.decode(s)\n    347 if cls is None:\n\nFile d:\\anaconda3\\Lib\\json\\decoder.py:337, in JSONDecoder.decode(self, s, _w)\n    333 \"\"\"Return the Python representation of ``s`` (a ``str`` instance\n    334 containing a JSON document).\n    335 \n    336 \"\"\"\n--&gt; 337 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    338 end = _w(s, end).end()\n\nFile d:\\anaconda3\\Lib\\json\\decoder.py:355, in JSONDecoder.raw_decode(self, s, idx)\n    354 except StopIteration as err:\n--&gt; 355     raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n    356 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nDuring handling of the above exception, another exception occurred:\n\nJSONDecodeError                           Traceback (most recent call last)\nCell In[1], line 6\n      3 url = \"https://api.ons.gov.uk/timeseries/JP9Z/dataset/UNEM/data\"\n      5 # Get the data from the ONS API:\n----&gt; 6 json_data = requests.get(url).json()\n      8 # Prep the data for a quick plot\n      9 title = json_data[\"description\"][\"title\"]\n\nFile d:\\anaconda3\\Lib\\site-packages\\requests\\models.py:978, in Response.json(self, **kwargs)\n    974     return complexjson.loads(self.text, **kwargs)\n    975 except JSONDecodeError as e:\n    976     # Catch JSON-related errors and raise as requests.JSONDecodeError\n    977     # This aliases json.JSONDecodeError and simplejson.JSONDecodeError\n--&gt; 978     raise RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n\n\n\nurl = \"http://aeturrell.com/research\"\npage = requests.get(url)\npage.text[:300]\n\n'&lt;!DOCTYPE html&gt;\\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\" xml:lang=\"en\"&gt;&lt;head&gt;\\n\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"generator\" content=\"quarto-1.5.56\"&gt;\\n\\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=yes\"&gt;\\n\\n&lt;meta name=\"author\" content=\"Arthur Turrell\"&gt;\\n'\n\n\n\nsoup = BeautifulSoup(page.text, \"html.parser\")\nprint(soup.prettify()[60000:60500])\n\n       &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=gender pay gap\"&gt;\n            gender pay gap\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=labour\"&gt;\n            labour\n           &lt;/a&gt;\n          &lt;/div&gt;\n          &lt;div class=\"project-category\"&gt;\n           &lt;a href=\"#category=text analysis\"&gt;\n            text analysis\n           &lt;/a&gt;\n          &lt;/div&gt;\n         &lt;/div&gt;\n         &lt;div class=\"project-details-listing\n\n\n\n# Get all paragraphs\nall_paras = soup.find_all(\"p\")\n# Just show one of the paras\nall_paras[1]\n\n&lt;p&gt;Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" &lt;i&gt;Environment and Planning B: Urban Analytics and City Science&lt;/i&gt; (2024): 23998083241267331. doi: &lt;a href=\"https://doi.org/10.1177/23998083241267331\"&gt;&lt;code&gt;10.1177/23998083241267331&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;\n\n\n\nall_paras[1].text\n\n'Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331'\n\n\n\nprojects = soup.find_all(\"div\", class_=\"project-content listing-pub-info\")\nprojects = [x.text.strip() for x in projects]\nprojects[:4]\n\n['Botta, Federico, Robin Lovelace, Laura Gilbert, and Arthur Turrell. \"Packaging code and data for reproducible research: A case study of journey time statistics.\" Environment and Planning B: Urban Analytics and City Science (2024): 23998083241267331. doi: 10.1177/23998083241267331',\n 'Kalamara, Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. \"Making text count: economic forecasting using newspaper text.\" Journal of Applied Econometrics 37, no. 5 (2022): 896-919. doi: 10.1002/jae.2907',\n 'Turrell, A., Speigner, B., Copple, D., Djumalieva, J. and Thurgood, J., 2021. Is the UK’s productivity puzzle mostly driven by occupational mismatch? An analysis using big data on job vacancies. Labour Economics, 71, p.102013. doi: 10.1016/j.labeco.2021.102013',\n 'Haldane, Andrew G., and Arthur E. Turrell. \"Drawing on different disciplines: macroeconomic agent-based models.\" Journal of Evolutionary Economics 29 (2019): 39-66. doi: 10.1007/s00191-018-0557-5']\n\n\n\nstart, stop = 0, 50\nroot_url = \"www.codingforeconomists.com/page=\"\ninfo_on_pages = [scraper(root_url + str(i)) for i in range(start, stop)]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 3\n      1 start, stop = 0, 50\n      2 root_url = \"www.codingforeconomists.com/page=\"\n----&gt; 3 info_on_pages = [scraper(root_url + str(i)) for i in range(start, stop)]\n\nNameError: name 'scraper' is not defined\n\n\n\n\ndf_list = pd.read_html(\n    \"https://simple.wikipedia.org/wiki/FIFA_World_Cup\", match=\"Sweden\"\n)\n# Retrieve first and only entry from list of dataframes\ndf = df_list[0]\ndf.head()\n\n\n\n\n\n\n\n\nYears\nHosts\nWinners\nScore\nRunner's-up\nThird place\nScore.1\nFourth place\n\n\n\n\n0\n1930 Details\nUruguay\nUruguay\n4 - 2\nArgentina\nUnited States\n[note 1]\nYugoslavia\n\n\n1\n1934 Details\nItaly\nItaly\n2 - 1\nCzechoslovakia\nGermany\n3 - 2\nAustria\n\n\n2\n1938 Details\nFrance\nItaly\n4 - 2\nHungary\nBrazil\n4 - 2\nSweden\n\n\n3\n1950 Details\nBrazil\nUruguay\n2 - 1\nBrazil\nSweden\n[note 2]\nSpain\n\n\n4\n1954 Details\nSwitzerland\nWest Germany\n3 - 2\nHungary\nAustria\n3 - 1\nUruguay\n\n\n\n\n\n\n\n\nimport pdftotext\nfrom pathlib import Path\n\n# Download the pdf_with_table.pdf file from\n# https://github.com/aeturrell/coding-for-economists/blob/main/data/pdf_with_table.pdf\n# and put it in a subfolder called data before running the next line\n\n# Load the PDF\nwith open(Path(\"data/pdf_with_table.pdf\"), \"rb\") as f:\n    pdf = pdftotext.PDF(f)\n\n# Read all the text into one string; print a chunk of the string\nprint(\"\\n\\n\".join(pdf)[:220])\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 import pdftotext\n      2 from pathlib import Path\n      4 # Download the pdf_with_table.pdf file from\n      5 # https://github.com/aeturrell/coding-for-economists/blob/main/data/pdf_with_table.pdf\n      6 # and put it in a subfolder called data before running the next line\n      7 \n      8 # Load the PDF\n\nModuleNotFoundError: No module named 'pdftotext'\n\n\n\n\n%pip install pdftotext\n\nCollecting pdftotextNote: you may need to restart the kernel to use updated packages.\n\n  Downloading pdftotext-2.2.2.tar.gz (113 kB)\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nBuilding wheels for collected packages: pdftotext\n  Building wheel for pdftotext (setup.py): started\n  Building wheel for pdftotext (setup.py): finished with status 'error'\n  Running setup.py clean for pdftotext\nFailed to build pdftotext"
  },
  {
    "objectID": "practice/practice 4.html",
    "href": "practice/practice 4.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimdb_data = pd.read_csv('IMDB_Top250.csv')  # Replace with actual file path\ndouban_data = pd.read_csv('douban_top250.csv')  # Replace with actual file path\n\n\nfrom bs4 import BeautifulSoup\nimport re\nimport urllib.request, urllib.error  # for URL requests\nimport csv  # for saving as CSV\n\n\n# Regular expressions to extract information\nfindLink = re.compile(r'&lt;a href=\"(.*?)\"&gt;')  # detail link\nfindImgSrc = re.compile(r'&lt;img.*src=\"(.*?)\"', re.S)  # image link\nfindTitle = re.compile(r'&lt;span class=\"title\"&gt;(.*)&lt;/span&gt;')  # movie title\nfindRating = re.compile(r'&lt;span class=\"rating_num\" property=\"v:average\"&gt;(.*)&lt;/span&gt;')  # rating\nfindJudge = re.compile(r'&lt;span&gt;(\\d*)人评价&lt;/span&gt;')  # number of reviews\nfindInq = re.compile(r'&lt;span class=\"inq\"&gt;(.*)&lt;/span&gt;')  # summary\nfindBd = re.compile(r'&lt;p class=\"\"&gt;(.*?)&lt;/p&gt;', re.S)  # additional info\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load datasets\ndouban_file_path = 'douban_top250.csv'  \nimdb_file_path = 'IMDB_Top250.csv'      \n\ndouban_data = pd.read_csv(douban_file_path, encoding='utf-8', on_bad_lines='skip')\nimdb_data = pd.read_csv(imdb_file_path, encoding='utf-8', on_bad_lines='skip')\n\n# Renaming columns for clarity and merging compatibility\ndouban_data.rename(columns={\n    '影片中文名': 'Title',\n    '评分': 'Douban_Score',\n    '评价数': 'Douban_Reviews',\n    '相关信息': 'Douban_Info'\n}, inplace=True)\n\n\nimdb_data.rename(columns={\n    'Name': 'Title',\n    'Year': 'Release_Year',\n    'IMDB Ranking': 'IMDB_Score',\n    'Genre': 'IMDB_Genre',\n    'Director': 'IMDB_Director'\n}, inplace=True)\n\n\n# Calculate average scores for both platforms\ndouban_avg_score = douban_data['Douban_Score'].mean()\nimdb_avg_score = imdb_data['IMDB_Score'].mean()\n\n# Find overlapping movies by title\noverlap_movies = pd.merge(douban_data, imdb_data, on='Title')\n\n# Visualize average scores\nplt.figure(figsize=(8, 5))\nplt.bar(['Douban', 'IMDb'], [douban_avg_score, imdb_avg_score], alpha=0.7)\nplt.title('Average Scores: Douban vs IMDb')\nplt.ylabel('Average Score')\nplt.show()\n\n# Analyze release year distribution\nplt.figure(figsize=(10, 5))\ndouban_data['Douban_Info'] = douban_data['Douban_Info'].astype(str)\ndouban_years = douban_data['Douban_Info'].str.extract(r'(\\d{4})').dropna()\ndouban_years = douban_years[0].astype(int).value_counts().sort_index()\n\nimdb_years = imdb_data['Release_Year'].value_counts().sort_index()\n\ndouban_years.plot(kind='bar', alpha=0.7, label='Douban', figsize=(10, 5))\nimdb_years.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Release Year Distribution')\nplt.xlabel('Year')\nplt.ylabel('Number of Movies')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Analyze genre distribution\nimdb_genres = imdb_data['IMDB_Genre'].str.split(',').explode().str.strip().value_counts()\nplt.figure(figsize=(10, 5))\nimdb_genres.head(10).plot(kind='bar', alpha=0.7, color='orange')\nplt.title('Top 10 IMDb Genres')\nplt.xlabel('Genre')\nplt.ylabel('Count')\nplt.show()\n\n# Top directors by movie count\ndouban_directors = douban_data['Douban_Info'].str.extract(r'导演: (.+?) ').dropna()\ndouban_top_directors = douban_directors[0].value_counts().head(10)\n\nimdb_top_directors = imdb_data['IMDB_Director'].value_counts().head(10)\n\nplt.figure(figsize=(10, 5))\ndouban_top_directors.plot(kind='bar', alpha=0.7, label='Douban', color='blue')\nplt.title('Top 10 Douban Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\nplt.figure(figsize=(10, 5))\nimdb_top_directors.plot(kind='bar', alpha=0.7, label='IMDb', color='orange')\nplt.title('Top 10 IMDb Directors')\nplt.xlabel('Director')\nplt.ylabel('Movie Count')\nplt.show()\n\n# Save overlapping movies to a CSV file\noverlap_movies.to_csv('overlap_movies.csv', index=False)\n\n# Print results\nprint(f\"豆瓣平均评分: {douban_avg_score}\")\nprint(f\"IMDb平均评分: {imdb_avg_score}\")\nprint(f\"重叠电影数量: {len(overlap_movies)}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n豆瓣平均评分: 8.9396\nIMDb平均评分: 8.254\n重叠电影数量: 0"
  }
]